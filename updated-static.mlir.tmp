module attributes {llvm.data_layout = "e-m:e-p270:32:32-p271:32:32-p272:64:64-i64:64-i128:128-f80:128-n8:16:32:64-S128", llvm.target_triple = "x86_64-unknown-linux-gnu", "onnx-mlir.symbol-postfix" = "test_skip_layer_norm_all_outputs_fixed.mlir"} {
  func.func @main_graph(%arg0: tensor<1x128xi64> {onnx.name = "input_ids"}, %arg1: tensor<1x128xi64> {onnx.name = "attention_mask"}, %arg2: tensor<1x128xi64> {onnx.name = "position_ids"}, %arg3: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.0.key"}, %arg4: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.0.value"}, %arg5: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.1.key"}, %arg6: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.1.value"}, %arg7: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.2.key"}, %arg8: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.2.value"}, %arg9: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.3.key"}, %arg10: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.3.value"}, %arg11: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.4.key"}, %arg12: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.4.value"}, %arg13: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.5.key"}, %arg14: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.5.value"}, %arg15: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.6.key"}, %arg16: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.6.value"}, %arg17: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.7.key"}, %arg18: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.7.value"}, %arg19: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.8.key"}, %arg20: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.8.value"}, %arg21: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.9.key"}, %arg22: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.9.value"}, %arg23: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.10.key"}, %arg24: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.10.value"}, %arg25: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.11.key"}, %arg26: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.11.value"}, %arg27: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.12.key"}, %arg28: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.12.value"}, %arg29: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.13.key"}, %arg30: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.13.value"}, %arg31: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.14.key"}, %arg32: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.14.value"}, %arg33: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.15.key"}, %arg34: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.15.value"}, %arg35: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.16.key"}, %arg36: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.16.value"}, %arg37: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.17.key"}, %arg38: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.17.value"}, %arg39: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.18.key"}, %arg40: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.18.value"}, %arg41: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.19.key"}, %arg42: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.19.value"}, %arg43: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.20.key"}, %arg44: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.20.value"}, %arg45: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.21.key"}, %arg46: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.21.value"}, %arg47: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.22.key"}, %arg48: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.22.value"}, %arg49: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.23.key"}, %arg50: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.23.value"}, %arg51: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.24.key"}, %arg52: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.24.value"}, %arg53: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.25.key"}, %arg54: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.25.value"}, %arg55: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.26.key"}, %arg56: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.26.value"}, %arg57: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.27.key"}, %arg58: tensor<1x8x1x128xf32> {onnx.name = "past_key_values.27.value"}) -> (tensor<1x128x151936xf32> {onnx.name = "logits"}, tensor<1x8x128x128xf32> {onnx.name = "present.0.key"}, tensor<1x8x128x128xf32> {onnx.name = "present.0.value"}, tensor<1x8x128x128xf32> {onnx.name = "present.1.key"}, tensor<1x8x128x128xf32> {onnx.name = "present.1.value"}, tensor<1x8x128x128xf32> {onnx.name = "present.2.key"}, tensor<1x8x128x128xf32> {onnx.name = "present.2.value"}, tensor<1x8x128x128xf32> {onnx.name = "present.3.key"}, tensor<1x8x128x128xf32> {onnx.name = "present.3.value"}, tensor<1x8x128x128xf32> {onnx.name = "present.4.key"}, tensor<1x8x128x128xf32> {onnx.name = "present.4.value"}, tensor<1x8x128x128xf32> {onnx.name = "present.5.key"}, tensor<1x8x128x128xf32> {onnx.name = "present.5.value"}, tensor<1x8x128x128xf32> {onnx.name = "present.6.key"}, tensor<1x8x128x128xf32> {onnx.name = "present.6.value"}, tensor<1x8x128x128xf32> {onnx.name = "present.7.key"}, tensor<1x8x128x128xf32> {onnx.name = "present.7.value"}, tensor<1x8x128x128xf32> {onnx.name = "present.8.key"}, tensor<1x8x128x128xf32> {onnx.name = "present.8.value"}, tensor<1x8x128x128xf32> {onnx.name = "present.9.key"}, tensor<1x8x128x128xf32> {onnx.name = "present.9.value"}, tensor<1x8x128x128xf32> {onnx.name = "present.10.key"}, tensor<1x8x128x128xf32> {onnx.name = "present.10.value"}, tensor<1x8x128x128xf32> {onnx.name = "present.11.key"}, tensor<1x8x128x128xf32> {onnx.name = "present.11.value"}, tensor<1x8x128x128xf32> {onnx.name = "present.12.key"}, tensor<1x8x128x128xf32> {onnx.name = "present.12.value"}, tensor<1x8x128x128xf32> {onnx.name = "present.13.key"}, tensor<1x8x128x128xf32> {onnx.name = "present.13.value"}, tensor<1x8x128x128xf32> {onnx.name = "present.14.key"}, tensor<1x8x128x128xf32> {onnx.name = "present.14.value"}, tensor<1x8x128x128xf32> {onnx.name = "present.15.key"}, tensor<1x8x128x128xf32> {onnx.name = "present.15.value"}, tensor<1x8x128x128xf32> {onnx.name = "present.16.key"}, tensor<1x8x128x128xf32> {onnx.name = "present.16.value"}, tensor<1x8x128x128xf32> {onnx.name = "present.17.key"}, tensor<1x8x128x128xf32> {onnx.name = "present.17.value"}, tensor<1x8x128x128xf32> {onnx.name = "present.18.key"}, tensor<1x8x128x128xf32> {onnx.name = "present.18.value"}, tensor<1x8x128x128xf32> {onnx.name = "present.19.key"}, tensor<1x8x128x128xf32> {onnx.name = "present.19.value"}, tensor<1x8x128x128xf32> {onnx.name = "present.20.key"}, tensor<1x8x128x128xf32> {onnx.name = "present.20.value"}, tensor<1x8x128x128xf32> {onnx.name = "present.21.key"}, tensor<1x8x128x128xf32> {onnx.name = "present.21.value"}, tensor<1x8x128x128xf32> {onnx.name = "present.22.key"}, tensor<1x8x128x128xf32> {onnx.name = "present.22.value"}, tensor<1x8x128x128xf32> {onnx.name = "present.23.key"}, tensor<1x8x128x128xf32> {onnx.name = "present.23.value"}, tensor<1x8x128x128xf32> {onnx.name = "present.24.key"}, tensor<1x8x128x128xf32> {onnx.name = "present.24.value"}, tensor<1x8x128x128xf32> {onnx.name = "present.25.key"}, tensor<1x8x128x128xf32> {onnx.name = "present.25.value"}, tensor<1x8x128x128xf32> {onnx.name = "present.26.key"}, tensor<1x8x128x128xf32> {onnx.name = "present.26.value"}, tensor<1x8x128x128xf32> {onnx.name = "present.27.key"}, tensor<1x8x128x128xf32> {onnx.name = "present.27.value"}) {
    %0 = onnx.Constant dense<128> : tensor<i32>
    %1 = onnx.Constant dense<-1> : tensor<1xi64>
    %2 = onnx.Constant dense_resource<__elided__> : tensor<1024x151936xui8>
    %3 = "onnx.NoValue"() {onnx_node_name = "onnx.NoValue_0", value} : () -> none
    %4 = onnx.Constant dense<[0, -1, 1024]> : tensor<3xi64>
    %5 = onnx.Constant dense<[0, -1, 2048]> : tensor<3xi64>
    %6 = onnx.Constant dense<[0, -1, 128]> : tensor<3xi64>
    %7 = onnx.Constant dense<1> : tensor<1xi64>
    %8 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %9 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %10 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %11 = onnx.Constant dense_resource<__elided__> : tensor<40960x64xf32>
    %12 = onnx.Constant dense_resource<__elided__> : tensor<40960x64xf32>
    %13 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %14 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %15 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %16 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %17 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %18 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %19 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %20 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %21 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %22 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %23 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %24 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %25 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %26 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %27 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %28 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %29 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %30 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %31 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %32 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %33 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %34 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %35 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %36 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %37 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %38 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %39 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %40 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %41 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %42 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %43 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %44 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %45 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %46 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %47 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %48 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %49 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %50 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %51 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %52 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %53 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %54 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %55 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %56 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %57 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %58 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %59 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %60 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %61 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %62 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %63 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %64 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %65 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %66 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %67 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %68 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %69 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %70 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %71 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %72 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %73 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %74 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %75 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %76 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %77 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %78 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %79 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %80 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %81 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %82 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %83 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %84 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %85 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %86 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %87 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %88 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %89 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %90 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %91 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %92 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %93 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %94 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %95 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %96 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %97 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %98 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %99 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %100 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %101 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %102 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %103 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %104 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %105 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %106 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %107 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %108 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %109 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %110 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %111 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %112 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %113 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %114 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %115 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %116 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %117 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %118 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %119 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %120 = onnx.Constant dense_resource<__elided__> : tensor<128xf32>
    %121 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %122 = onnx.Constant dense_resource<__elided__> : tensor<1024xf32>
    %123 = onnx.Constant dense<0.00213311892> : tensor<f32>
    %124 = onnx.Constant dense<149> : tensor<ui8>
    %125 = onnx.Constant dense_resource<__elided__> : tensor<151936x1024xui8>
    %126 = onnx.Constant dense<0.00507504912> : tensor<f32>
    %127 = onnx.Constant dense<0> : tensor<i8>
    %128 = onnx.Constant dense_resource<__elided__> : tensor<1024x2048xi8>
    %129 = onnx.Constant dense<0.00341412402> : tensor<f32>
    %130 = onnx.Constant dense<0> : tensor<i8>
    %131 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %132 = onnx.Constant dense<0.00136103597> : tensor<f32>
    %133 = onnx.Constant dense<0> : tensor<i8>
    %134 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %135 = onnx.Constant dense<0.00332185044> : tensor<f32>
    %136 = onnx.Constant dense<0> : tensor<i8>
    %137 = onnx.Constant dense_resource<__elided__> : tensor<2048x1024xi8>
    %138 = onnx.Constant dense<0.00307578733> : tensor<f32>
    %139 = onnx.Constant dense<0> : tensor<i8>
    %140 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %141 = onnx.Constant dense<0.00295275589> : tensor<f32>
    %142 = onnx.Constant dense<0> : tensor<i8>
    %143 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %144 = onnx.Constant dense<0.00329109258> : tensor<f32>
    %145 = onnx.Constant dense<0> : tensor<i8>
    %146 = onnx.Constant dense_resource<__elided__> : tensor<3072x1024xi8>
    %147 = onnx.Constant dense<0.00504429126> : tensor<f32>
    %148 = onnx.Constant dense<0> : tensor<i8>
    %149 = onnx.Constant dense_resource<__elided__> : tensor<1024x2048xi8>
    %150 = onnx.Constant dense<0.00281434553> : tensor<f32>
    %151 = onnx.Constant dense<0> : tensor<i8>
    %152 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %153 = onnx.Constant dense<0.00158403045> : tensor<f32>
    %154 = onnx.Constant dense<0> : tensor<i8>
    %155 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %156 = onnx.Constant dense<0.00226070383> : tensor<f32>
    %157 = onnx.Constant dense<0> : tensor<i8>
    %158 = onnx.Constant dense_resource<__elided__> : tensor<2048x1024xi8>
    %159 = onnx.Constant dense<0.00342950295> : tensor<f32>
    %160 = onnx.Constant dense<0> : tensor<i8>
    %161 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %162 = onnx.Constant dense<0.00232221955> : tensor<f32>
    %163 = onnx.Constant dense<0> : tensor<i8>
    %164 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %165 = onnx.Constant dense<0.00295275589> : tensor<f32>
    %166 = onnx.Constant dense<0> : tensor<i8>
    %167 = onnx.Constant dense_resource<__elided__> : tensor<3072x1024xi8>
    %168 = onnx.Constant dense<0.0040600393> : tensor<f32>
    %169 = onnx.Constant dense<0> : tensor<i8>
    %170 = onnx.Constant dense_resource<__elided__> : tensor<1024x2048xi8>
    %171 = onnx.Constant dense<0.00203001965> : tensor<f32>
    %172 = onnx.Constant dense<0> : tensor<i8>
    %173 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %174 = onnx.Constant dense<0.00143793062> : tensor<f32>
    %175 = onnx.Constant dense<0> : tensor<i8>
    %176 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %177 = onnx.Constant dense<2.460630e-03> : tensor<f32>
    %178 = onnx.Constant dense<0> : tensor<i8>
    %179 = onnx.Constant dense_resource<__elided__> : tensor<2048x1024xi8>
    %180 = onnx.Constant dense<0.003198819> : tensor<f32>
    %181 = onnx.Constant dense<0> : tensor<i8>
    %182 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %183 = onnx.Constant dense<0.00418307073> : tensor<f32>
    %184 = onnx.Constant dense<0> : tensor<i8>
    %185 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %186 = onnx.Constant dense<0.00458292337> : tensor<f32>
    %187 = onnx.Constant dense<0> : tensor<i8>
    %188 = onnx.Constant dense_resource<__elided__> : tensor<3072x1024xi8>
    %189 = onnx.Constant dense<0.00336798723> : tensor<f32>
    %190 = onnx.Constant dense<0> : tensor<i8>
    %191 = onnx.Constant dense_resource<__elided__> : tensor<1024x2048xi8>
    %192 = onnx.Constant dense<4.336860e-03> : tensor<f32>
    %193 = onnx.Constant dense<0> : tensor<i8>
    %194 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %195 = onnx.Constant dense<0.00302965054> : tensor<f32>
    %196 = onnx.Constant dense<0> : tensor<i8>
    %197 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %198 = onnx.Constant dense<0.00227608276> : tensor<f32>
    %199 = onnx.Constant dense<0> : tensor<i8>
    %200 = onnx.Constant dense_resource<__elided__> : tensor<2048x1024xi8>
    %201 = onnx.Constant dense<0.00376783963> : tensor<f32>
    %202 = onnx.Constant dense<0> : tensor<i8>
    %203 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %204 = onnx.Constant dense<0.0023529774> : tensor<f32>
    %205 = onnx.Constant dense<0> : tensor<i8>
    %206 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %207 = onnx.Constant dense<0.00309116626> : tensor<f32>
    %208 = onnx.Constant dense<0> : tensor<i8>
    %209 = onnx.Constant dense_resource<__elided__> : tensor<3072x1024xi8>
    %210 = onnx.Constant dense<0.00427534431> : tensor<f32>
    %211 = onnx.Constant dense<0> : tensor<i8>
    %212 = onnx.Constant dense_resource<__elided__> : tensor<1024x2048xi8>
    %213 = onnx.Constant dense<0.00296813482> : tensor<f32>
    %214 = onnx.Constant dense<0> : tensor<i8>
    %215 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %216 = onnx.Constant dense<0.00160709897> : tensor<f32>
    %217 = onnx.Constant dense<0> : tensor<i8>
    %218 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %219 = onnx.Constant dense<0.00286048232> : tensor<f32>
    %220 = onnx.Constant dense<0> : tensor<i8>
    %221 = onnx.Constant dense_resource<__elided__> : tensor<2048x1024xi8>
    %222 = onnx.Constant dense<0.00327571365> : tensor<f32>
    %223 = onnx.Constant dense<0> : tensor<i8>
    %224 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %225 = onnx.Constant dense<0.00209153537> : tensor<f32>
    %226 = onnx.Constant dense<0> : tensor<i8>
    %227 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %228 = onnx.Constant dense<0.0030604084> : tensor<f32>
    %229 = onnx.Constant dense<0> : tensor<i8>
    %230 = onnx.Constant dense_resource<__elided__> : tensor<3072x1024xi8>
    %231 = onnx.Constant dense<0.0033526083> : tensor<f32>
    %232 = onnx.Constant dense<0> : tensor<i8>
    %233 = onnx.Constant dense_resource<__elided__> : tensor<1024x2048xi8>
    %234 = onnx.Constant dense<3.183440e-03> : tensor<f32>
    %235 = onnx.Constant dense<0> : tensor<i8>
    %236 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %237 = onnx.Constant dense<0.00163785683> : tensor<f32>
    %238 = onnx.Constant dense<0> : tensor<i8>
    %239 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %240 = onnx.Constant dense<0.00467519695> : tensor<f32>
    %241 = onnx.Constant dense<0> : tensor<i8>
    %242 = onnx.Constant dense_resource<__elided__> : tensor<2048x1024xi8>
    %243 = onnx.Constant dense<4.490650e-03> : tensor<f32>
    %244 = onnx.Constant dense<0> : tensor<i8>
    %245 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %246 = onnx.Constant dense<0.00292199804> : tensor<f32>
    %247 = onnx.Constant dense<0> : tensor<i8>
    %248 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %249 = onnx.Constant dense<0.00326033472> : tensor<f32>
    %250 = onnx.Constant dense<0> : tensor<i8>
    %251 = onnx.Constant dense_resource<__elided__> : tensor<3072x1024xi8>
    %252 = onnx.Constant dense<0.00341412402> : tensor<f32>
    %253 = onnx.Constant dense<0> : tensor<i8>
    %254 = onnx.Constant dense_resource<__elided__> : tensor<1024x2048xi8>
    %255 = onnx.Constant dense<0.00396776572> : tensor<f32>
    %256 = onnx.Constant dense<0> : tensor<i8>
    %257 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %258 = onnx.Constant dense<0.00129952014> : tensor<f32>
    %259 = onnx.Constant dense<0> : tensor<i8>
    %260 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %261 = onnx.Constant dense<0.00282972446> : tensor<f32>
    %262 = onnx.Constant dense<0> : tensor<i8>
    %263 = onnx.Constant dense_resource<__elided__> : tensor<2048x1024xi8>
    %264 = onnx.Constant dense<0.00244525098> : tensor<f32>
    %265 = onnx.Constant dense<0> : tensor<i8>
    %266 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %267 = onnx.Constant dense<0.00221456704> : tensor<f32>
    %268 = onnx.Constant dense<0> : tensor<i8>
    %269 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %270 = onnx.Constant dense<0.00556717534> : tensor<f32>
    %271 = onnx.Constant dense<0> : tensor<i8>
    %272 = onnx.Constant dense_resource<__elided__> : tensor<3072x1024xi8>
    %273 = onnx.Constant dense<0.00312192412> : tensor<f32>
    %274 = onnx.Constant dense<0> : tensor<i8>
    %275 = onnx.Constant dense_resource<__elided__> : tensor<1024x2048xi8>
    %276 = onnx.Constant dense<0.00213767216> : tensor<f32>
    %277 = onnx.Constant dense<0> : tensor<i8>
    %278 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %279 = onnx.Constant dense<0.00399852358> : tensor<f32>
    %280 = onnx.Constant dense<0> : tensor<i8>
    %281 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %282 = onnx.Constant dense<0.00262979814> : tensor<f32>
    %283 = onnx.Constant dense<0> : tensor<i8>
    %284 = onnx.Constant dense_resource<__elided__> : tensor<2048x1024xi8>
    %285 = onnx.Constant dense<0.00259904028> : tensor<f32>
    %286 = onnx.Constant dense<0> : tensor<i8>
    %287 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %288 = onnx.Constant dense<0.00227608276> : tensor<f32>
    %289 = onnx.Constant dense<0> : tensor<i8>
    %290 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %291 = onnx.Constant dense<4.490650e-03> : tensor<f32>
    %292 = onnx.Constant dense<0> : tensor<i8>
    %293 = onnx.Constant dense_resource<__elided__> : tensor<3072x1024xi8>
    %294 = onnx.Constant dense<0.00264517707> : tensor<f32>
    %295 = onnx.Constant dense<0> : tensor<i8>
    %296 = onnx.Constant dense_resource<__elided__> : tensor<1024x2048xi8>
    %297 = onnx.Constant dense<0.00529035414> : tensor<f32>
    %298 = onnx.Constant dense<0> : tensor<i8>
    %299 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %300 = onnx.Constant dense<0.00156865153> : tensor<f32>
    %301 = onnx.Constant dense<0> : tensor<i8>
    %302 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %303 = onnx.Constant dense<0.00267593493> : tensor<f32>
    %304 = onnx.Constant dense<0> : tensor<i8>
    %305 = onnx.Constant dense_resource<__elided__> : tensor<2048x1024xi8>
    %306 = onnx.Constant dense<0.00648991158> : tensor<f32>
    %307 = onnx.Constant dense<0> : tensor<i8>
    %308 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %309 = onnx.Constant dense<0.00339874509> : tensor<f32>
    %310 = onnx.Constant dense<0> : tensor<i8>
    %311 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %312 = onnx.Constant dense<0.00296813482> : tensor<f32>
    %313 = onnx.Constant dense<0> : tensor<i8>
    %314 = onnx.Constant dense_resource<__elided__> : tensor<3072x1024xi8>
    %315 = onnx.Constant dense<0.00281434553> : tensor<f32>
    %316 = onnx.Constant dense<0> : tensor<i8>
    %317 = onnx.Constant dense_resource<__elided__> : tensor<1024x2048xi8>
    %318 = onnx.Constant dense<0.00421382859> : tensor<f32>
    %319 = onnx.Constant dense<0> : tensor<i8>
    %320 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %321 = onnx.Constant dense<0.00275282981> : tensor<f32>
    %322 = onnx.Constant dense<0> : tensor<i8>
    %323 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %324 = onnx.Constant dense<0.00276820874> : tensor<f32>
    %325 = onnx.Constant dense<0> : tensor<i8>
    %326 = onnx.Constant dense_resource<__elided__> : tensor<2048x1024xi8>
    %327 = onnx.Constant dense<0.00355253438> : tensor<f32>
    %328 = onnx.Constant dense<0> : tensor<i8>
    %329 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %330 = onnx.Constant dense<0.00198388286> : tensor<f32>
    %331 = onnx.Constant dense<0> : tensor<i8>
    %332 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %333 = onnx.Constant dense<3.183440e-03> : tensor<f32>
    %334 = onnx.Constant dense<0> : tensor<i8>
    %335 = onnx.Constant dense_resource<__elided__> : tensor<3072x1024xi8>
    %336 = onnx.Constant dense<0.00273745088> : tensor<f32>
    %337 = onnx.Constant dense<0> : tensor<i8>
    %338 = onnx.Constant dense_resource<__elided__> : tensor<1024x2048xi8>
    %339 = onnx.Constant dense<0.00203001965> : tensor<f32>
    %340 = onnx.Constant dense<0> : tensor<i8>
    %341 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %342 = onnx.Constant dense<0.0013072096> : tensor<f32>
    %343 = onnx.Constant dense<0> : tensor<i8>
    %344 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %345 = onnx.Constant dense<0.00253752456> : tensor<f32>
    %346 = onnx.Constant dense<0> : tensor<i8>
    %347 = onnx.Constant dense_resource<__elided__> : tensor<2048x1024xi8>
    %348 = onnx.Constant dense<0.00412155502> : tensor<f32>
    %349 = onnx.Constant dense<0> : tensor<i8>
    %350 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %351 = onnx.Constant dense<0.00341412402> : tensor<f32>
    %352 = onnx.Constant dense<0> : tensor<i8>
    %353 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %354 = onnx.Constant dense<0.00304502947> : tensor<f32>
    %355 = onnx.Constant dense<0> : tensor<i8>
    %356 = onnx.Constant dense_resource<__elided__> : tensor<3072x1024xi8>
    %357 = onnx.Constant dense<0.0035063976> : tensor<f32>
    %358 = onnx.Constant dense<0> : tensor<i8>
    %359 = onnx.Constant dense_resource<__elided__> : tensor<1024x2048xi8>
    %360 = onnx.Constant dense<0.00295275589> : tensor<f32>
    %361 = onnx.Constant dense<0> : tensor<i8>
    %362 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %363 = onnx.Constant dense<0.00169168308> : tensor<f32>
    %364 = onnx.Constant dense<0> : tensor<i8>
    %365 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %366 = onnx.Constant dense<0.00310654519> : tensor<f32>
    %367 = onnx.Constant dense<0> : tensor<i8>
    %368 = onnx.Constant dense_resource<__elided__> : tensor<2048x1024xi8>
    %369 = onnx.Constant dense<0.00399852358> : tensor<f32>
    %370 = onnx.Constant dense<0> : tensor<i8>
    %371 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %372 = onnx.Constant dense<0.00229146169> : tensor<f32>
    %373 = onnx.Constant dense<0> : tensor<i8>
    %374 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %375 = onnx.Constant dense<3.183440e-03> : tensor<f32>
    %376 = onnx.Constant dense<0> : tensor<i8>
    %377 = onnx.Constant dense_resource<__elided__> : tensor<3072x1024xi8>
    %378 = onnx.Constant dense<0.00290661911> : tensor<f32>
    %379 = onnx.Constant dense<0> : tensor<i8>
    %380 = onnx.Constant dense_resource<__elided__> : tensor<1024x2048xi8>
    %381 = onnx.Constant dense<0.00293737696> : tensor<f32>
    %382 = onnx.Constant dense<0> : tensor<i8>
    %383 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %384 = onnx.Constant dense<0.00123031496> : tensor<f32>
    %385 = onnx.Constant dense<0> : tensor<i8>
    %386 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %387 = onnx.Constant dense<0.00226070383> : tensor<f32>
    %388 = onnx.Constant dense<0> : tensor<i8>
    %389 = onnx.Constant dense_resource<__elided__> : tensor<2048x1024xi8>
    %390 = onnx.Constant dense<0.0025067667> : tensor<f32>
    %391 = onnx.Constant dense<0> : tensor<i8>
    %392 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %393 = onnx.Constant dense<0.00227608276> : tensor<f32>
    %394 = onnx.Constant dense<0> : tensor<i8>
    %395 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %396 = onnx.Constant dense<0.00384473428> : tensor<f32>
    %397 = onnx.Constant dense<0> : tensor<i8>
    %398 = onnx.Constant dense_resource<__elided__> : tensor<3072x1024xi8>
    %399 = onnx.Constant dense<0.00364480796> : tensor<f32>
    %400 = onnx.Constant dense<0> : tensor<i8>
    %401 = onnx.Constant dense_resource<__elided__> : tensor<1024x2048xi8>
    %402 = onnx.Constant dense<0.0023529774> : tensor<f32>
    %403 = onnx.Constant dense<0> : tensor<i8>
    %404 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %405 = onnx.Constant dense<0.00121493603> : tensor<f32>
    %406 = onnx.Constant dense<0> : tensor<i8>
    %407 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %408 = onnx.Constant dense<0.00196850393> : tensor<f32>
    %409 = onnx.Constant dense<0> : tensor<i8>
    %410 = onnx.Constant dense_resource<__elided__> : tensor<2048x1024xi8>
    %411 = onnx.Constant dense<0.00402928144> : tensor<f32>
    %412 = onnx.Constant dense<0> : tensor<i8>
    %413 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %414 = onnx.Constant dense<0.00189929875> : tensor<f32>
    %415 = onnx.Constant dense<0> : tensor<i8>
    %416 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %417 = onnx.Constant dense<0.00366018689> : tensor<f32>
    %418 = onnx.Constant dense<0> : tensor<i8>
    %419 = onnx.Constant dense_resource<__elided__> : tensor<3072x1024xi8>
    %420 = onnx.Constant dense<0.00273745088> : tensor<f32>
    %421 = onnx.Constant dense<0> : tensor<i8>
    %422 = onnx.Constant dense_resource<__elided__> : tensor<1024x2048xi8>
    %423 = onnx.Constant dense<0.00270669302> : tensor<f32>
    %424 = onnx.Constant dense<0> : tensor<i8>
    %425 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %426 = onnx.Constant dense<0.0011995571> : tensor<f32>
    %427 = onnx.Constant dense<0> : tensor<i8>
    %428 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %429 = onnx.Constant dense<0.00219918811> : tensor<f32>
    %430 = onnx.Constant dense<0> : tensor<i8>
    %431 = onnx.Constant dense_resource<__elided__> : tensor<2048x1024xi8>
    %432 = onnx.Constant dense<0.00342950295> : tensor<f32>
    %433 = onnx.Constant dense<0> : tensor<i8>
    %434 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %435 = onnx.Constant dense<0.00264517707> : tensor<f32>
    %436 = onnx.Constant dense<0> : tensor<i8>
    %437 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %438 = onnx.Constant dense<0.00273745088> : tensor<f32>
    %439 = onnx.Constant dense<0> : tensor<i8>
    %440 = onnx.Constant dense_resource<__elided__> : tensor<3072x1024xi8>
    %441 = onnx.Constant dense<0.00364480796> : tensor<f32>
    %442 = onnx.Constant dense<0> : tensor<i8>
    %443 = onnx.Constant dense_resource<__elided__> : tensor<1024x2048xi8>
    %444 = onnx.Constant dense<0.00209153537> : tensor<f32>
    %445 = onnx.Constant dense<0> : tensor<i8>
    %446 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %447 = onnx.Constant dense<0.00198388286> : tensor<f32>
    %448 = onnx.Constant dense<0> : tensor<i8>
    %449 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %450 = onnx.Constant dense<0.00253752456> : tensor<f32>
    %451 = onnx.Constant dense<0> : tensor<i8>
    %452 = onnx.Constant dense_resource<__elided__> : tensor<2048x1024xi8>
    %453 = onnx.Constant dense<0.00418307073> : tensor<f32>
    %454 = onnx.Constant dense<0> : tensor<i8>
    %455 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %456 = onnx.Constant dense<0.00253752456> : tensor<f32>
    %457 = onnx.Constant dense<0> : tensor<i8>
    %458 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %459 = onnx.Constant dense<0.00310654519> : tensor<f32>
    %460 = onnx.Constant dense<0> : tensor<i8>
    %461 = onnx.Constant dense_resource<__elided__> : tensor<3072x1024xi8>
    %462 = onnx.Constant dense<0.00346026081> : tensor<f32>
    %463 = onnx.Constant dense<0> : tensor<i8>
    %464 = onnx.Constant dense_resource<__elided__> : tensor<1024x2048xi8>
    %465 = onnx.Constant dense<0.00652066944> : tensor<f32>
    %466 = onnx.Constant dense<0> : tensor<i8>
    %467 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %468 = onnx.Constant dense<0.00126876228> : tensor<f32>
    %469 = onnx.Constant dense<0> : tensor<i8>
    %470 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %471 = onnx.Constant dense<0.00267593493> : tensor<f32>
    %472 = onnx.Constant dense<0> : tensor<i8>
    %473 = onnx.Constant dense_resource<__elided__> : tensor<2048x1024xi8>
    %474 = onnx.Constant dense<3.183440e-03> : tensor<f32>
    %475 = onnx.Constant dense<0> : tensor<i8>
    %476 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %477 = onnx.Constant dense<3.183440e-03> : tensor<f32>
    %478 = onnx.Constant dense<0> : tensor<i8>
    %479 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %480 = onnx.Constant dense<0.00376783963> : tensor<f32>
    %481 = onnx.Constant dense<0> : tensor<i8>
    %482 = onnx.Constant dense_resource<__elided__> : tensor<3072x1024xi8>
    %483 = onnx.Constant dense<0.00275282981> : tensor<f32>
    %484 = onnx.Constant dense<0> : tensor<i8>
    %485 = onnx.Constant dense_resource<__elided__> : tensor<1024x2048xi8>
    %486 = onnx.Constant dense<0.00201464072> : tensor<f32>
    %487 = onnx.Constant dense<0> : tensor<i8>
    %488 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %489 = onnx.Constant dense<0.00154558313> : tensor<f32>
    %490 = onnx.Constant dense<0> : tensor<i8>
    %491 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %492 = onnx.Constant dense<0.0021069143> : tensor<f32>
    %493 = onnx.Constant dense<0> : tensor<i8>
    %494 = onnx.Constant dense_resource<__elided__> : tensor<2048x1024xi8>
    %495 = onnx.Constant dense<0.00307578733> : tensor<f32>
    %496 = onnx.Constant dense<0> : tensor<i8>
    %497 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %498 = onnx.Constant dense<0.00232221955> : tensor<f32>
    %499 = onnx.Constant dense<0> : tensor<i8>
    %500 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %501 = onnx.Constant dense<0.00293737696> : tensor<f32>
    %502 = onnx.Constant dense<0> : tensor<i8>
    %503 = onnx.Constant dense_resource<__elided__> : tensor<3072x1024xi8>
    %504 = onnx.Constant dense<0.00327571365> : tensor<f32>
    %505 = onnx.Constant dense<0> : tensor<i8>
    %506 = onnx.Constant dense_resource<__elided__> : tensor<1024x2048xi8>
    %507 = onnx.Constant dense<0.00312192412> : tensor<f32>
    %508 = onnx.Constant dense<0> : tensor<i8>
    %509 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %510 = onnx.Constant dense<0.0013764149> : tensor<f32>
    %511 = onnx.Constant dense<0> : tensor<i8>
    %512 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %513 = onnx.Constant dense<0.00199926179> : tensor<f32>
    %514 = onnx.Constant dense<0> : tensor<i8>
    %515 = onnx.Constant dense_resource<__elided__> : tensor<2048x1024xi8>
    %516 = onnx.Constant dense<0.00473671267> : tensor<f32>
    %517 = onnx.Constant dense<0> : tensor<i8>
    %518 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %519 = onnx.Constant dense<0.00252214563> : tensor<f32>
    %520 = onnx.Constant dense<0> : tensor<i8>
    %521 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %522 = onnx.Constant dense<0.00356791331> : tensor<f32>
    %523 = onnx.Constant dense<0> : tensor<i8>
    %524 = onnx.Constant dense_resource<__elided__> : tensor<3072x1024xi8>
    %525 = onnx.Constant dense<3.183440e-03> : tensor<f32>
    %526 = onnx.Constant dense<0> : tensor<i8>
    %527 = onnx.Constant dense_resource<__elided__> : tensor<1024x2048xi8>
    %528 = onnx.Constant dense<0.00324495579> : tensor<f32>
    %529 = onnx.Constant dense<0> : tensor<i8>
    %530 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %531 = onnx.Constant dense<0.0015994095> : tensor<f32>
    %532 = onnx.Constant dense<0> : tensor<i8>
    %533 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %534 = onnx.Constant dense<0.0022453249> : tensor<f32>
    %535 = onnx.Constant dense<0> : tensor<i8>
    %536 = onnx.Constant dense_resource<__elided__> : tensor<2048x1024xi8>
    %537 = onnx.Constant dense<0.00342950295> : tensor<f32>
    %538 = onnx.Constant dense<0> : tensor<i8>
    %539 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %540 = onnx.Constant dense<0.00473671267> : tensor<f32>
    %541 = onnx.Constant dense<0> : tensor<i8>
    %542 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %543 = onnx.Constant dense<0.0033526083> : tensor<f32>
    %544 = onnx.Constant dense<0> : tensor<i8>
    %545 = onnx.Constant dense_resource<__elided__> : tensor<3072x1024xi8>
    %546 = onnx.Constant dense<0.00346026081> : tensor<f32>
    %547 = onnx.Constant dense<0> : tensor<i8>
    %548 = onnx.Constant dense_resource<__elided__> : tensor<1024x2048xi8>
    %549 = onnx.Constant dense<0.0025067667> : tensor<f32>
    %550 = onnx.Constant dense<0> : tensor<i8>
    %551 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %552 = onnx.Constant dense<0.00212229323> : tensor<f32>
    %553 = onnx.Constant dense<0> : tensor<i8>
    %554 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %555 = onnx.Constant dense<0.00281434553> : tensor<f32>
    %556 = onnx.Constant dense<0> : tensor<i8>
    %557 = onnx.Constant dense_resource<__elided__> : tensor<2048x1024xi8>
    %558 = onnx.Constant dense<0.00470595481> : tensor<f32>
    %559 = onnx.Constant dense<0> : tensor<i8>
    %560 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %561 = onnx.Constant dense<0.00379859749> : tensor<f32>
    %562 = onnx.Constant dense<0> : tensor<i8>
    %563 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %564 = onnx.Constant dense<0.00421382859> : tensor<f32>
    %565 = onnx.Constant dense<0> : tensor<i8>
    %566 = onnx.Constant dense_resource<__elided__> : tensor<3072x1024xi8>
    %567 = onnx.Constant dense<0.00316806091> : tensor<f32>
    %568 = onnx.Constant dense<0> : tensor<i8>
    %569 = onnx.Constant dense_resource<__elided__> : tensor<1024x2048xi8>
    %570 = onnx.Constant dense<0.00495201768> : tensor<f32>
    %571 = onnx.Constant dense<0> : tensor<i8>
    %572 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %573 = onnx.Constant dense<0.00203001965> : tensor<f32>
    %574 = onnx.Constant dense<0> : tensor<i8>
    %575 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %576 = onnx.Constant dense<0.00239911419> : tensor<f32>
    %577 = onnx.Constant dense<0> : tensor<i8>
    %578 = onnx.Constant dense_resource<__elided__> : tensor<2048x1024xi8>
    %579 = onnx.Constant dense<0.00485974411> : tensor<f32>
    %580 = onnx.Constant dense<0> : tensor<i8>
    %581 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %582 = onnx.Constant dense<4.921260e-03> : tensor<f32>
    %583 = onnx.Constant dense<0> : tensor<i8>
    %584 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %585 = onnx.Constant dense<0.00473671267> : tensor<f32>
    %586 = onnx.Constant dense<0> : tensor<i8>
    %587 = onnx.Constant dense_resource<__elided__> : tensor<3072x1024xi8>
    %588 = onnx.Constant dense<0.00270669302> : tensor<f32>
    %589 = onnx.Constant dense<0> : tensor<i8>
    %590 = onnx.Constant dense_resource<__elided__> : tensor<1024x2048xi8>
    %591 = onnx.Constant dense<0.00232221955> : tensor<f32>
    %592 = onnx.Constant dense<0> : tensor<i8>
    %593 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %594 = onnx.Constant dense<0.00191467768> : tensor<f32>
    %595 = onnx.Constant dense<0> : tensor<i8>
    %596 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %597 = onnx.Constant dense<0.00256828242> : tensor<f32>
    %598 = onnx.Constant dense<0> : tensor<i8>
    %599 = onnx.Constant dense_resource<__elided__> : tensor<2048x1024xi8>
    %600 = onnx.Constant dense<5.351870e-03> : tensor<f32>
    %601 = onnx.Constant dense<0> : tensor<i8>
    %602 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %603 = onnx.Constant dense<4.921260e-03> : tensor<f32>
    %604 = onnx.Constant dense<0> : tensor<i8>
    %605 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %606 = onnx.Constant dense<0.0050135334> : tensor<f32>
    %607 = onnx.Constant dense<0> : tensor<i8>
    %608 = onnx.Constant dense_resource<__elided__> : tensor<3072x1024xi8>
    %609 = onnx.Constant dense<0.00301427161> : tensor<f32>
    %610 = onnx.Constant dense<0> : tensor<i8>
    %611 = onnx.Constant dense_resource<__elided__> : tensor<1024x2048xi8>
    %612 = onnx.Constant dense<0.00221456704> : tensor<f32>
    %613 = onnx.Constant dense<0> : tensor<i8>
    %614 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %615 = onnx.Constant dense<0.00233759847> : tensor<f32>
    %616 = onnx.Constant dense<0> : tensor<i8>
    %617 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %618 = onnx.Constant dense<2.168430e-03> : tensor<f32>
    %619 = onnx.Constant dense<0> : tensor<i8>
    %620 = onnx.Constant dense_resource<__elided__> : tensor<2048x1024xi8>
    %621 = onnx.Constant dense<0.00396776572> : tensor<f32>
    %622 = onnx.Constant dense<0> : tensor<i8>
    %623 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %624 = onnx.Constant dense<0.00569020677> : tensor<f32>
    %625 = onnx.Constant dense<0> : tensor<i8>
    %626 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %627 = onnx.Constant dense<0.00556717534> : tensor<f32>
    %628 = onnx.Constant dense<0> : tensor<i8>
    %629 = onnx.Constant dense_resource<__elided__> : tensor<3072x1024xi8>
    %630 = onnx.Constant dense<0.00346026081> : tensor<f32>
    %631 = onnx.Constant dense<0> : tensor<i8>
    %632 = onnx.Constant dense_resource<__elided__> : tensor<1024x2048xi8>
    %633 = onnx.Constant dense<0.00193005661> : tensor<f32>
    %634 = onnx.Constant dense<0> : tensor<i8>
    %635 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %636 = onnx.Constant dense<0.0015994095> : tensor<f32>
    %637 = onnx.Constant dense<0> : tensor<i8>
    %638 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %639 = onnx.Constant dense<0.00196850393> : tensor<f32>
    %640 = onnx.Constant dense<0> : tensor<i8>
    %641 = onnx.Constant dense_resource<__elided__> : tensor<2048x1024xi8>
    %642 = onnx.Constant dense<0.00479822839> : tensor<f32>
    %643 = onnx.Constant dense<0> : tensor<i8>
    %644 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %645 = onnx.Constant dense<3.614050e-03> : tensor<f32>
    %646 = onnx.Constant dense<0> : tensor<i8>
    %647 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %648 = onnx.Constant dense<0.00381397642> : tensor<f32>
    %649 = onnx.Constant dense<0> : tensor<i8>
    %650 = onnx.Constant dense_resource<__elided__> : tensor<3072x1024xi8>
    %651 = onnx.Constant dense<0.00310654519> : tensor<f32>
    %652 = onnx.Constant dense<0> : tensor<i8>
    %653 = onnx.Constant dense_resource<__elided__> : tensor<1024x2048xi8>
    %654 = onnx.Constant dense<1.591720e-03> : tensor<f32>
    %655 = onnx.Constant dense<0> : tensor<i8>
    %656 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %657 = onnx.Constant dense<0.0017301304> : tensor<f32>
    %658 = onnx.Constant dense<0> : tensor<i8>
    %659 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %660 = onnx.Constant dense<0.00186854089> : tensor<f32>
    %661 = onnx.Constant dense<0> : tensor<i8>
    %662 = onnx.Constant dense_resource<__elided__> : tensor<2048x1024xi8>
    %663 = onnx.Constant dense<0.00399852358> : tensor<f32>
    %664 = onnx.Constant dense<0> : tensor<i8>
    %665 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %666 = onnx.Constant dense<5.351870e-03> : tensor<f32>
    %667 = onnx.Constant dense<0> : tensor<i8>
    %668 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %669 = onnx.Constant dense<0.00479822839> : tensor<f32>
    %670 = onnx.Constant dense<0> : tensor<i8>
    %671 = onnx.Constant dense_resource<__elided__> : tensor<3072x1024xi8>
    %672 = onnx.Constant dense<0.00270669302> : tensor<f32>
    %673 = onnx.Constant dense<0> : tensor<i8>
    %674 = onnx.Constant dense_resource<__elided__> : tensor<1024x2048xi8>
    %675 = onnx.Constant dense<0.00187623035> : tensor<f32>
    %676 = onnx.Constant dense<0> : tensor<i8>
    %677 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %678 = onnx.Constant dense<0.00150713581> : tensor<f32>
    %679 = onnx.Constant dense<0> : tensor<i8>
    %680 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %681 = onnx.Constant dense<0.00312192412> : tensor<f32>
    %682 = onnx.Constant dense<0> : tensor<i8>
    %683 = onnx.Constant dense_resource<__elided__> : tensor<2048x1024xi8>
    %684 = onnx.Constant dense<0.00836614146> : tensor<f32>
    %685 = onnx.Constant dense<0> : tensor<i8>
    %686 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %687 = onnx.Constant dense<8.673720e-03> : tensor<f32>
    %688 = onnx.Constant dense<0> : tensor<i8>
    %689 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %690 = onnx.Constant dense<0.00504429126> : tensor<f32>
    %691 = onnx.Constant dense<0> : tensor<i8>
    %692 = onnx.Constant dense_resource<__elided__> : tensor<3072x1024xi8>
    %693 = onnx.Constant dense<0.00336798723> : tensor<f32>
    %694 = onnx.Constant dense<0> : tensor<i8>
    %695 = onnx.Constant dense_resource<__elided__> : tensor<1024x2048xi8>
    %696 = onnx.Constant dense<0.00538262818> : tensor<f32>
    %697 = onnx.Constant dense<0> : tensor<i8>
    %698 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %699 = onnx.Constant dense<0.00206077751> : tensor<f32>
    %700 = onnx.Constant dense<0> : tensor<i8>
    %701 = onnx.Constant dense_resource<__elided__> : tensor<1024x1024xi8>
    %702 = onnx.Constant dense<0.00239911419> : tensor<f32>
    %703 = onnx.Constant dense<0> : tensor<i8>
    %704 = onnx.Constant dense_resource<__elided__> : tensor<2048x1024xi8>
    %705 = onnx.Constant dense<0.00910433102> : tensor<f32>
    %706 = onnx.Constant dense<0> : tensor<i8>
    %707 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %708 = onnx.Constant dense<0.00971948821> : tensor<f32>
    %709 = onnx.Constant dense<0> : tensor<i8>
    %710 = onnx.Constant dense_resource<__elided__> : tensor<1024x3072xi8>
    %711 = onnx.Constant dense<0.00436761789> : tensor<f32>
    %712 = onnx.Constant dense<0> : tensor<i8>
    %713 = onnx.Constant dense_resource<__elided__> : tensor<3072x1024xi8>
    %714 = "onnx.Gather"(%125, %arg0) {axis = 0 : si64, onnx_node_name = "/model/embed_tokens/Gather"} : (tensor<151936x1024xui8>, tensor<1x128xi64>) -> tensor<1x128x1024xui8>
    %715 = "onnx.ReduceSum"(%arg1, %7) {keepdims = 1 : si64, noop_with_empty_axes = 0 : si64, onnx_node_name = "/model/attn_mask_reformat/attn_mask_subgraph/ReduceSum"} : (tensor<1x128xi64>, tensor<1xi64>) -> tensor<1x1xi64>
    %716 = "onnx.DequantizeLinear"(%714, %123, %124) {axis = 1 : si64, onnx_node_name = "/model/embed_tokens/Gather/output_0_DequantizeLinear"} : (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>) -> tensor<1x128x1024xf32>
    %717 = "onnx.Custom"(%716, %8) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.0/input_layernorm/LayerNorm", stash_type = 1 : si64} : (tensor<1x128x1024xf32>, tensor<1024xf32>) -> tensor<1x128x3072xf32>
    %718 = "onnx.Add"(%715, %1) {onnx_node_name = "/model/attn_mask_reformat/attn_mask_subgraph/Sub-/model/constant_nodes/TensorProto.INT64/1D/1_1"} : (tensor<1x1xi64>, tensor<1xi64>) -> tensor<1x1xi64>
    %y, %y_scale, %y_zero_point = "onnx.DynamicQuantizeLinear"(%717) {onnx_node_name = "/model/layers.0/input_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x3072xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %719 = "onnx.MatMulInteger"(%y, %128, %y_zero_point, %127) {onnx_node_name = "/model/layers.0/attn/q_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x2048xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x2048xi32>
    %720 = "onnx.Cast"(%719) {onnx_node_name = "/model/layers.0/attn/q_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x2048xi32>) -> tensor<1x128x2048xf32>
    %721 = "onnx.Mul"(%y_scale, %126) {onnx_node_name = "/model/layers.0/attn/q_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %722 = "onnx.Mul"(%720, %721) {onnx_node_name = "/model/layers.0/attn/q_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x2048xf32>, tensor<f32>) -> tensor<1x128x2048xf32>
    %723 = "onnx.MatMulInteger"(%y, %131, %y_zero_point, %130) {onnx_node_name = "/model/layers.0/attn/k_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %724 = "onnx.Cast"(%723) {onnx_node_name = "/model/layers.0/attn/k_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %725 = "onnx.Mul"(%y_scale, %129) {onnx_node_name = "/model/layers.0/attn/k_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %726 = "onnx.Mul"(%724, %725) {onnx_node_name = "/model/layers.0/attn/k_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %727 = "onnx.MatMulInteger"(%y, %134, %y_zero_point, %133) {onnx_node_name = "/model/layers.0/attn/v_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %728 = "onnx.Cast"(%727) {onnx_node_name = "/model/layers.0/attn/v_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %729 = "onnx.Mul"(%y_scale, %132) {onnx_node_name = "/model/layers.0/attn/v_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %730 = "onnx.Mul"(%728, %729) {onnx_node_name = "/model/layers.0/attn/v_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %731 = "onnx.Cast"(%718) {onnx_node_name = "/model/attn_mask_reformat/attn_mask_subgraph/Sub/Cast", saturate = 1 : si64, to = i32} : (tensor<1x1xi64>) -> tensor<1x1xi32>
    %732 = "onnx.Reshape"(%722, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.0/attn/q_norm/Reshape_1"} : (tensor<1x128x2048xf32>, tensor<3xi64>) -> tensor<1x128x3072xf32>
    %733 = "onnx.Reshape"(%726, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.0/attn/k_norm/Reshape_1"} : (tensor<1x128x1024xf32>, tensor<3xi64>) -> tensor<1x1024x128xf32>
    %734 = "onnx.Custom"(%732, %9) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.0/attn/q_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x128x3072xf32>, tensor<128xf32>) -> tensor<1x2048x128xf32>
    %735 = "onnx.Custom"(%733, %10) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.0/attn/k_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x1024x128xf32>, tensor<128xf32>) -> tensor<1x1024x128xf32>
    %736 = "onnx.Reshape"(%734, %5) {allowzero = 0 : si64, onnx_node_name = "/model/layers.0/attn/q_norm/Reshape_2"} : (tensor<1x2048x128xf32>, tensor<3xi64>) -> tensor<1x128x3072xf32>
    %737 = "onnx.Reshape"(%735, %4) {allowzero = 0 : si64, onnx_node_name = "/model/layers.0/attn/k_norm/Reshape_2"} : (tensor<1x1024x128xf32>, tensor<3xi64>) -> tensor<1x128x1024xf32>
    %738 = "onnx.Custom"(%736, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.0/attn/q_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x3072xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x2048xf32>
    %739 = "onnx.Custom"(%737, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.0/attn/k_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x1024xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x1024xf32>
    %740:3 = "onnx.Custom"(%738, %739, %730, %arg3, %arg4, %731, %0, %3, %3) {do_rotary = 0 : si64, domain_name = "com.microsoft", function_name = "GroupQueryAttention", kv_num_heads = 8 : si64, num_heads = 16 : si64, onnx_node_name = "/model/layers.0/attn/GroupQueryAttention", rotary_interleaved = 0 : si64, scale = 0.0883883461 : f32, softcap = 0.000000e+00 : f32} : (tensor<1x128x2048xf32>, tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1x8x1x128xf32>, tensor<1x8x1x128xf32>, tensor<1x1xi32>, tensor<i32>, none, none) -> (tensor<1x128x2048xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>)
    %y_0, %y_scale_1, %y_zero_point_2 = "onnx.DynamicQuantizeLinear"(%740#0) {onnx_node_name = "/model/layers.0/attn/GroupQueryAttention/output_0_QuantizeLinear"} : (tensor<1x128x2048xf32>) -> (tensor<1x128x2048xui8>, tensor<f32>, tensor<ui8>)
    %741 = "onnx.MatMulInteger"(%y_0, %137, %y_zero_point_2, %136) {onnx_node_name = "/model/layers.0/attn/o_proj/MatMul_quant"} : (tensor<1x128x2048xui8>, tensor<2048x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %742 = "onnx.Cast"(%741) {onnx_node_name = "/model/layers.0/attn/o_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %743 = "onnx.Mul"(%y_scale_1, %135) {onnx_node_name = "/model/layers.0/attn/o_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %744 = "onnx.Mul"(%742, %743) {onnx_node_name = "/model/layers.0/attn/o_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %745:4 = "onnx.Custom"(%716, %744, %13) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.0/post_attention_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_3, %y_scale_4, %y_zero_point_5 = "onnx.DynamicQuantizeLinear"(%745#0) {onnx_node_name = "/model/layers.0/post_attention_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %746 = "onnx.MatMulInteger"(%y_3, %140, %y_zero_point_5, %139) {onnx_node_name = "/model/layers.0/mlp/gate_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %747 = "onnx.Cast"(%746) {onnx_node_name = "/model/layers.0/mlp/gate_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %748 = "onnx.Mul"(%y_scale_4, %138) {onnx_node_name = "/model/layers.0/mlp/gate_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %749 = "onnx.Mul"(%747, %748) {onnx_node_name = "/model/layers.0/mlp/gate_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %750 = "onnx.MatMulInteger"(%y_3, %143, %y_zero_point_5, %142) {onnx_node_name = "/model/layers.0/mlp/up_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %751 = "onnx.Cast"(%750) {onnx_node_name = "/model/layers.0/mlp/up_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %752 = "onnx.Mul"(%y_scale_4, %141) {onnx_node_name = "/model/layers.0/mlp/up_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %753 = "onnx.Mul"(%751, %752) {onnx_node_name = "/model/layers.0/mlp/up_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %754 = "onnx.Sigmoid"(%749) {onnx_node_name = "/model/layers.0/mlp/act_fn/Sigmoid"} : (tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %755 = "onnx.Mul"(%749, %754) {onnx_node_name = "/model/layers.0/mlp/act_fn/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %756 = "onnx.Mul"(%755, %753) {onnx_node_name = "/model/layers.0/mlp/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %y_6, %y_scale_7, %y_zero_point_8 = "onnx.DynamicQuantizeLinear"(%756) {onnx_node_name = "/model/layers.0/mlp/Mul/output_0_QuantizeLinear"} : (tensor<1x128x3072xf32>) -> (tensor<1x128x3072xui8>, tensor<f32>, tensor<ui8>)
    %757 = "onnx.MatMulInteger"(%y_6, %146, %y_zero_point_8, %145) {onnx_node_name = "/model/layers.0/mlp/down_proj/MatMul_quant"} : (tensor<1x128x3072xui8>, tensor<3072x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %758 = "onnx.Cast"(%757) {onnx_node_name = "/model/layers.0/mlp/down_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %759 = "onnx.Mul"(%y_scale_7, %144) {onnx_node_name = "/model/layers.0/mlp/down_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %760 = "onnx.Mul"(%758, %759) {onnx_node_name = "/model/layers.0/mlp/down_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %761:4 = "onnx.Custom"(%745#3, %760, %14) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.1/input_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_9, %y_scale_10, %y_zero_point_11 = "onnx.DynamicQuantizeLinear"(%761#0) {onnx_node_name = "/model/layers.1/input_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %762 = "onnx.MatMulInteger"(%y_9, %149, %y_zero_point_11, %148) {onnx_node_name = "/model/layers.1/attn/q_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x2048xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x2048xi32>
    %763 = "onnx.Cast"(%762) {onnx_node_name = "/model/layers.1/attn/q_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x2048xi32>) -> tensor<1x128x2048xf32>
    %764 = "onnx.Mul"(%y_scale_10, %147) {onnx_node_name = "/model/layers.1/attn/q_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %765 = "onnx.Mul"(%763, %764) {onnx_node_name = "/model/layers.1/attn/q_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x2048xf32>, tensor<f32>) -> tensor<1x128x2048xf32>
    %766 = "onnx.MatMulInteger"(%y_9, %152, %y_zero_point_11, %151) {onnx_node_name = "/model/layers.1/attn/k_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %767 = "onnx.Cast"(%766) {onnx_node_name = "/model/layers.1/attn/k_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %768 = "onnx.Mul"(%y_scale_10, %150) {onnx_node_name = "/model/layers.1/attn/k_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %769 = "onnx.Mul"(%767, %768) {onnx_node_name = "/model/layers.1/attn/k_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %770 = "onnx.MatMulInteger"(%y_9, %155, %y_zero_point_11, %154) {onnx_node_name = "/model/layers.1/attn/v_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %771 = "onnx.Cast"(%770) {onnx_node_name = "/model/layers.1/attn/v_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %772 = "onnx.Mul"(%y_scale_10, %153) {onnx_node_name = "/model/layers.1/attn/v_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %773 = "onnx.Mul"(%771, %772) {onnx_node_name = "/model/layers.1/attn/v_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %774 = "onnx.Reshape"(%765, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.1/attn/q_norm/Reshape_1"} : (tensor<1x128x2048xf32>, tensor<3xi64>) -> tensor<1x2048x128xf32>
    %775 = "onnx.Reshape"(%769, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.1/attn/k_norm/Reshape_1"} : (tensor<1x128x1024xf32>, tensor<3xi64>) -> tensor<1x1024x128xf32>
    %776 = "onnx.Custom"(%774, %15) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.1/attn/q_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x2048x128xf32>, tensor<128xf32>) -> tensor<1x2048x128xf32>
    %777 = "onnx.Custom"(%775, %16) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.1/attn/k_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x1024x128xf32>, tensor<128xf32>) -> tensor<1x1024x128xf32>
    %778 = "onnx.Reshape"(%776, %5) {allowzero = 0 : si64, onnx_node_name = "/model/layers.1/attn/q_norm/Reshape_2"} : (tensor<1x2048x128xf32>, tensor<3xi64>) -> tensor<1x128x2048xf32>
    %779 = "onnx.Reshape"(%777, %4) {allowzero = 0 : si64, onnx_node_name = "/model/layers.1/attn/k_norm/Reshape_2"} : (tensor<1x1024x128xf32>, tensor<3xi64>) -> tensor<1x128x1024xf32>
    %780 = "onnx.Custom"(%778, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.1/attn/q_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x2048xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x2048xf32>
    %781 = "onnx.Custom"(%779, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.1/attn/k_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x1024xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x1024xf32>
    %782:3 = "onnx.Custom"(%780, %781, %773, %arg5, %arg6, %731, %0, %3, %3) {do_rotary = 0 : si64, domain_name = "com.microsoft", function_name = "GroupQueryAttention", kv_num_heads = 8 : si64, num_heads = 16 : si64, onnx_node_name = "/model/layers.1/attn/GroupQueryAttention", rotary_interleaved = 0 : si64, scale = 0.0883883461 : f32, softcap = 0.000000e+00 : f32} : (tensor<1x128x2048xf32>, tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1x8x1x128xf32>, tensor<1x8x1x128xf32>, tensor<1x1xi32>, tensor<i32>, none, none) -> (tensor<1x128x2048xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>)
    %y_12, %y_scale_13, %y_zero_point_14 = "onnx.DynamicQuantizeLinear"(%782#0) {onnx_node_name = "/model/layers.1/attn/GroupQueryAttention/output_0_QuantizeLinear"} : (tensor<1x128x2048xf32>) -> (tensor<1x128x2048xui8>, tensor<f32>, tensor<ui8>)
    %783 = "onnx.MatMulInteger"(%y_12, %158, %y_zero_point_14, %157) {onnx_node_name = "/model/layers.1/attn/o_proj/MatMul_quant"} : (tensor<1x128x2048xui8>, tensor<2048x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %784 = "onnx.Cast"(%783) {onnx_node_name = "/model/layers.1/attn/o_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %785 = "onnx.Mul"(%y_scale_13, %156) {onnx_node_name = "/model/layers.1/attn/o_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %786 = "onnx.Mul"(%784, %785) {onnx_node_name = "/model/layers.1/attn/o_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %787:4 = "onnx.Custom"(%761#3, %786, %17) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.1/post_attention_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_15, %y_scale_16, %y_zero_point_17 = "onnx.DynamicQuantizeLinear"(%787#0) {onnx_node_name = "/model/layers.1/post_attention_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %788 = "onnx.MatMulInteger"(%y_15, %161, %y_zero_point_17, %160) {onnx_node_name = "/model/layers.1/mlp/gate_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %789 = "onnx.Cast"(%788) {onnx_node_name = "/model/layers.1/mlp/gate_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %790 = "onnx.Mul"(%y_scale_16, %159) {onnx_node_name = "/model/layers.1/mlp/gate_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %791 = "onnx.Mul"(%789, %790) {onnx_node_name = "/model/layers.1/mlp/gate_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %792 = "onnx.MatMulInteger"(%y_15, %164, %y_zero_point_17, %163) {onnx_node_name = "/model/layers.1/mlp/up_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %793 = "onnx.Cast"(%792) {onnx_node_name = "/model/layers.1/mlp/up_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %794 = "onnx.Mul"(%y_scale_16, %162) {onnx_node_name = "/model/layers.1/mlp/up_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %795 = "onnx.Mul"(%793, %794) {onnx_node_name = "/model/layers.1/mlp/up_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %796 = "onnx.Sigmoid"(%791) {onnx_node_name = "/model/layers.1/mlp/act_fn/Sigmoid"} : (tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %797 = "onnx.Mul"(%791, %796) {onnx_node_name = "/model/layers.1/mlp/act_fn/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %798 = "onnx.Mul"(%797, %795) {onnx_node_name = "/model/layers.1/mlp/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %y_18, %y_scale_19, %y_zero_point_20 = "onnx.DynamicQuantizeLinear"(%798) {onnx_node_name = "/model/layers.1/mlp/Mul/output_0_QuantizeLinear"} : (tensor<1x128x3072xf32>) -> (tensor<1x128x3072xui8>, tensor<f32>, tensor<ui8>)
    %799 = "onnx.MatMulInteger"(%y_18, %167, %y_zero_point_20, %166) {onnx_node_name = "/model/layers.1/mlp/down_proj/MatMul_quant"} : (tensor<1x128x3072xui8>, tensor<3072x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %800 = "onnx.Cast"(%799) {onnx_node_name = "/model/layers.1/mlp/down_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %801 = "onnx.Mul"(%y_scale_19, %165) {onnx_node_name = "/model/layers.1/mlp/down_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %802 = "onnx.Mul"(%800, %801) {onnx_node_name = "/model/layers.1/mlp/down_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %803:4 = "onnx.Custom"(%787#3, %802, %18) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.2/input_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_21, %y_scale_22, %y_zero_point_23 = "onnx.DynamicQuantizeLinear"(%803#0) {onnx_node_name = "/model/layers.2/input_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %804 = "onnx.MatMulInteger"(%y_21, %170, %y_zero_point_23, %169) {onnx_node_name = "/model/layers.2/attn/q_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x2048xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x2048xi32>
    %805 = "onnx.Cast"(%804) {onnx_node_name = "/model/layers.2/attn/q_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x2048xi32>) -> tensor<1x128x2048xf32>
    %806 = "onnx.Mul"(%y_scale_22, %168) {onnx_node_name = "/model/layers.2/attn/q_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %807 = "onnx.Mul"(%805, %806) {onnx_node_name = "/model/layers.2/attn/q_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x2048xf32>, tensor<f32>) -> tensor<1x128x2048xf32>
    %808 = "onnx.MatMulInteger"(%y_21, %173, %y_zero_point_23, %172) {onnx_node_name = "/model/layers.2/attn/k_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %809 = "onnx.Cast"(%808) {onnx_node_name = "/model/layers.2/attn/k_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %810 = "onnx.Mul"(%y_scale_22, %171) {onnx_node_name = "/model/layers.2/attn/k_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %811 = "onnx.Mul"(%809, %810) {onnx_node_name = "/model/layers.2/attn/k_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %812 = "onnx.MatMulInteger"(%y_21, %176, %y_zero_point_23, %175) {onnx_node_name = "/model/layers.2/attn/v_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %813 = "onnx.Cast"(%812) {onnx_node_name = "/model/layers.2/attn/v_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %814 = "onnx.Mul"(%y_scale_22, %174) {onnx_node_name = "/model/layers.2/attn/v_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %815 = "onnx.Mul"(%813, %814) {onnx_node_name = "/model/layers.2/attn/v_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %816 = "onnx.Reshape"(%807, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.2/attn/q_norm/Reshape_1"} : (tensor<1x128x2048xf32>, tensor<3xi64>) -> tensor<1x2048x128xf32>
    %817 = "onnx.Reshape"(%811, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.2/attn/k_norm/Reshape_1"} : (tensor<1x128x1024xf32>, tensor<3xi64>) -> tensor<1x1024x128xf32>
    %818 = "onnx.Custom"(%816, %19) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.2/attn/q_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x2048x128xf32>, tensor<128xf32>) -> tensor<1x2048x128xf32>
    %819 = "onnx.Custom"(%817, %20) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.2/attn/k_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x1024x128xf32>, tensor<128xf32>) -> tensor<1x1024x128xf32>
    %820 = "onnx.Reshape"(%818, %5) {allowzero = 0 : si64, onnx_node_name = "/model/layers.2/attn/q_norm/Reshape_2"} : (tensor<1x2048x128xf32>, tensor<3xi64>) -> tensor<1x128x2048xf32>
    %821 = "onnx.Reshape"(%819, %4) {allowzero = 0 : si64, onnx_node_name = "/model/layers.2/attn/k_norm/Reshape_2"} : (tensor<1x1024x128xf32>, tensor<3xi64>) -> tensor<1x128x1024xf32>
    %822 = "onnx.Custom"(%820, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.2/attn/q_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x2048xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x2048xf32>
    %823 = "onnx.Custom"(%821, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.2/attn/k_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x1024xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x1024xf32>
    %824:3 = "onnx.Custom"(%822, %823, %815, %arg7, %arg8, %731, %0, %3, %3) {do_rotary = 0 : si64, domain_name = "com.microsoft", function_name = "GroupQueryAttention", kv_num_heads = 8 : si64, num_heads = 16 : si64, onnx_node_name = "/model/layers.2/attn/GroupQueryAttention", rotary_interleaved = 0 : si64, scale = 0.0883883461 : f32, softcap = 0.000000e+00 : f32} : (tensor<1x128x2048xf32>, tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1x8x1x128xf32>, tensor<1x8x1x128xf32>, tensor<1x1xi32>, tensor<i32>, none, none) -> (tensor<1x128x2048xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>)
    %y_24, %y_scale_25, %y_zero_point_26 = "onnx.DynamicQuantizeLinear"(%824#0) {onnx_node_name = "/model/layers.2/attn/GroupQueryAttention/output_0_QuantizeLinear"} : (tensor<1x128x2048xf32>) -> (tensor<1x128x2048xui8>, tensor<f32>, tensor<ui8>)
    %825 = "onnx.MatMulInteger"(%y_24, %179, %y_zero_point_26, %178) {onnx_node_name = "/model/layers.2/attn/o_proj/MatMul_quant"} : (tensor<1x128x2048xui8>, tensor<2048x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %826 = "onnx.Cast"(%825) {onnx_node_name = "/model/layers.2/attn/o_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %827 = "onnx.Mul"(%y_scale_25, %177) {onnx_node_name = "/model/layers.2/attn/o_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %828 = "onnx.Mul"(%826, %827) {onnx_node_name = "/model/layers.2/attn/o_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %829:4 = "onnx.Custom"(%803#3, %828, %21) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.2/post_attention_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_27, %y_scale_28, %y_zero_point_29 = "onnx.DynamicQuantizeLinear"(%829#0) {onnx_node_name = "/model/layers.2/post_attention_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %830 = "onnx.MatMulInteger"(%y_27, %182, %y_zero_point_29, %181) {onnx_node_name = "/model/layers.2/mlp/gate_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %831 = "onnx.Cast"(%830) {onnx_node_name = "/model/layers.2/mlp/gate_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %832 = "onnx.Mul"(%y_scale_28, %180) {onnx_node_name = "/model/layers.2/mlp/gate_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %833 = "onnx.Mul"(%831, %832) {onnx_node_name = "/model/layers.2/mlp/gate_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %834 = "onnx.MatMulInteger"(%y_27, %185, %y_zero_point_29, %184) {onnx_node_name = "/model/layers.2/mlp/up_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %835 = "onnx.Cast"(%834) {onnx_node_name = "/model/layers.2/mlp/up_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %836 = "onnx.Mul"(%y_scale_28, %183) {onnx_node_name = "/model/layers.2/mlp/up_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %837 = "onnx.Mul"(%835, %836) {onnx_node_name = "/model/layers.2/mlp/up_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %838 = "onnx.Sigmoid"(%833) {onnx_node_name = "/model/layers.2/mlp/act_fn/Sigmoid"} : (tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %839 = "onnx.Mul"(%833, %838) {onnx_node_name = "/model/layers.2/mlp/act_fn/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %840 = "onnx.Mul"(%839, %837) {onnx_node_name = "/model/layers.2/mlp/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %y_30, %y_scale_31, %y_zero_point_32 = "onnx.DynamicQuantizeLinear"(%840) {onnx_node_name = "/model/layers.2/mlp/Mul/output_0_QuantizeLinear"} : (tensor<1x128x3072xf32>) -> (tensor<1x128x3072xui8>, tensor<f32>, tensor<ui8>)
    %841 = "onnx.MatMulInteger"(%y_30, %188, %y_zero_point_32, %187) {onnx_node_name = "/model/layers.2/mlp/down_proj/MatMul_quant"} : (tensor<1x128x3072xui8>, tensor<3072x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %842 = "onnx.Cast"(%841) {onnx_node_name = "/model/layers.2/mlp/down_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %843 = "onnx.Mul"(%y_scale_31, %186) {onnx_node_name = "/model/layers.2/mlp/down_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %844 = "onnx.Mul"(%842, %843) {onnx_node_name = "/model/layers.2/mlp/down_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %845:4 = "onnx.Custom"(%829#3, %844, %22) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.3/input_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_33, %y_scale_34, %y_zero_point_35 = "onnx.DynamicQuantizeLinear"(%845#0) {onnx_node_name = "/model/layers.3/input_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %846 = "onnx.MatMulInteger"(%y_33, %191, %y_zero_point_35, %190) {onnx_node_name = "/model/layers.3/attn/q_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x2048xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x2048xi32>
    %847 = "onnx.Cast"(%846) {onnx_node_name = "/model/layers.3/attn/q_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x2048xi32>) -> tensor<1x128x2048xf32>
    %848 = "onnx.Mul"(%y_scale_34, %189) {onnx_node_name = "/model/layers.3/attn/q_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %849 = "onnx.Mul"(%847, %848) {onnx_node_name = "/model/layers.3/attn/q_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x2048xf32>, tensor<f32>) -> tensor<1x128x2048xf32>
    %850 = "onnx.MatMulInteger"(%y_33, %194, %y_zero_point_35, %193) {onnx_node_name = "/model/layers.3/attn/k_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %851 = "onnx.Cast"(%850) {onnx_node_name = "/model/layers.3/attn/k_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %852 = "onnx.Mul"(%y_scale_34, %192) {onnx_node_name = "/model/layers.3/attn/k_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %853 = "onnx.Mul"(%851, %852) {onnx_node_name = "/model/layers.3/attn/k_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %854 = "onnx.MatMulInteger"(%y_33, %197, %y_zero_point_35, %196) {onnx_node_name = "/model/layers.3/attn/v_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %855 = "onnx.Cast"(%854) {onnx_node_name = "/model/layers.3/attn/v_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %856 = "onnx.Mul"(%y_scale_34, %195) {onnx_node_name = "/model/layers.3/attn/v_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %857 = "onnx.Mul"(%855, %856) {onnx_node_name = "/model/layers.3/attn/v_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %858 = "onnx.Reshape"(%849, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.3/attn/q_norm/Reshape_1"} : (tensor<1x128x2048xf32>, tensor<3xi64>) -> tensor<1x2048x128xf32>
    %859 = "onnx.Reshape"(%853, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.3/attn/k_norm/Reshape_1"} : (tensor<1x128x1024xf32>, tensor<3xi64>) -> tensor<1x1024x128xf32>
    %860 = "onnx.Custom"(%858, %23) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.3/attn/q_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x2048x128xf32>, tensor<128xf32>) -> tensor<1x2048x128xf32>
    %861 = "onnx.Custom"(%859, %24) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.3/attn/k_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x1024x128xf32>, tensor<128xf32>) -> tensor<1x1024x128xf32>
    %862 = "onnx.Reshape"(%860, %5) {allowzero = 0 : si64, onnx_node_name = "/model/layers.3/attn/q_norm/Reshape_2"} : (tensor<1x2048x128xf32>, tensor<3xi64>) -> tensor<1x128x2048xf32>
    %863 = "onnx.Reshape"(%861, %4) {allowzero = 0 : si64, onnx_node_name = "/model/layers.3/attn/k_norm/Reshape_2"} : (tensor<1x1024x128xf32>, tensor<3xi64>) -> tensor<1x128x1024xf32>
    %864 = "onnx.Custom"(%862, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.3/attn/q_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x2048xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x2048xf32>
    %865 = "onnx.Custom"(%863, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.3/attn/k_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x1024xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x1024xf32>
    %866:3 = "onnx.Custom"(%864, %865, %857, %arg9, %arg10, %731, %0, %3, %3) {do_rotary = 0 : si64, domain_name = "com.microsoft", function_name = "GroupQueryAttention", kv_num_heads = 8 : si64, num_heads = 16 : si64, onnx_node_name = "/model/layers.3/attn/GroupQueryAttention", rotary_interleaved = 0 : si64, scale = 0.0883883461 : f32, softcap = 0.000000e+00 : f32} : (tensor<1x128x2048xf32>, tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1x8x1x128xf32>, tensor<1x8x1x128xf32>, tensor<1x1xi32>, tensor<i32>, none, none) -> (tensor<1x128x2048xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>)
    %y_36, %y_scale_37, %y_zero_point_38 = "onnx.DynamicQuantizeLinear"(%866#0) {onnx_node_name = "/model/layers.3/attn/GroupQueryAttention/output_0_QuantizeLinear"} : (tensor<1x128x2048xf32>) -> (tensor<1x128x2048xui8>, tensor<f32>, tensor<ui8>)
    %867 = "onnx.MatMulInteger"(%y_36, %200, %y_zero_point_38, %199) {onnx_node_name = "/model/layers.3/attn/o_proj/MatMul_quant"} : (tensor<1x128x2048xui8>, tensor<2048x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %868 = "onnx.Cast"(%867) {onnx_node_name = "/model/layers.3/attn/o_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %869 = "onnx.Mul"(%y_scale_37, %198) {onnx_node_name = "/model/layers.3/attn/o_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %870 = "onnx.Mul"(%868, %869) {onnx_node_name = "/model/layers.3/attn/o_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %871:4 = "onnx.Custom"(%845#3, %870, %25) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.3/post_attention_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_39, %y_scale_40, %y_zero_point_41 = "onnx.DynamicQuantizeLinear"(%871#0) {onnx_node_name = "/model/layers.3/post_attention_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %872 = "onnx.MatMulInteger"(%y_39, %203, %y_zero_point_41, %202) {onnx_node_name = "/model/layers.3/mlp/gate_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %873 = "onnx.Cast"(%872) {onnx_node_name = "/model/layers.3/mlp/gate_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %874 = "onnx.Mul"(%y_scale_40, %201) {onnx_node_name = "/model/layers.3/mlp/gate_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %875 = "onnx.Mul"(%873, %874) {onnx_node_name = "/model/layers.3/mlp/gate_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %876 = "onnx.MatMulInteger"(%y_39, %206, %y_zero_point_41, %205) {onnx_node_name = "/model/layers.3/mlp/up_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %877 = "onnx.Cast"(%876) {onnx_node_name = "/model/layers.3/mlp/up_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %878 = "onnx.Mul"(%y_scale_40, %204) {onnx_node_name = "/model/layers.3/mlp/up_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %879 = "onnx.Mul"(%877, %878) {onnx_node_name = "/model/layers.3/mlp/up_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %880 = "onnx.Sigmoid"(%875) {onnx_node_name = "/model/layers.3/mlp/act_fn/Sigmoid"} : (tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %881 = "onnx.Mul"(%875, %880) {onnx_node_name = "/model/layers.3/mlp/act_fn/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %882 = "onnx.Mul"(%881, %879) {onnx_node_name = "/model/layers.3/mlp/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %y_42, %y_scale_43, %y_zero_point_44 = "onnx.DynamicQuantizeLinear"(%882) {onnx_node_name = "/model/layers.3/mlp/Mul/output_0_QuantizeLinear"} : (tensor<1x128x3072xf32>) -> (tensor<1x128x3072xui8>, tensor<f32>, tensor<ui8>)
    %883 = "onnx.MatMulInteger"(%y_42, %209, %y_zero_point_44, %208) {onnx_node_name = "/model/layers.3/mlp/down_proj/MatMul_quant"} : (tensor<1x128x3072xui8>, tensor<3072x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %884 = "onnx.Cast"(%883) {onnx_node_name = "/model/layers.3/mlp/down_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %885 = "onnx.Mul"(%y_scale_43, %207) {onnx_node_name = "/model/layers.3/mlp/down_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %886 = "onnx.Mul"(%884, %885) {onnx_node_name = "/model/layers.3/mlp/down_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %887:4 = "onnx.Custom"(%871#3, %886, %26) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.4/input_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_45, %y_scale_46, %y_zero_point_47 = "onnx.DynamicQuantizeLinear"(%887#0) {onnx_node_name = "/model/layers.4/input_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %888 = "onnx.MatMulInteger"(%y_45, %212, %y_zero_point_47, %211) {onnx_node_name = "/model/layers.4/attn/q_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x2048xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x2048xi32>
    %889 = "onnx.Cast"(%888) {onnx_node_name = "/model/layers.4/attn/q_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x2048xi32>) -> tensor<1x128x2048xf32>
    %890 = "onnx.Mul"(%y_scale_46, %210) {onnx_node_name = "/model/layers.4/attn/q_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %891 = "onnx.Mul"(%889, %890) {onnx_node_name = "/model/layers.4/attn/q_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x2048xf32>, tensor<f32>) -> tensor<1x128x2048xf32>
    %892 = "onnx.MatMulInteger"(%y_45, %215, %y_zero_point_47, %214) {onnx_node_name = "/model/layers.4/attn/k_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %893 = "onnx.Cast"(%892) {onnx_node_name = "/model/layers.4/attn/k_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %894 = "onnx.Mul"(%y_scale_46, %213) {onnx_node_name = "/model/layers.4/attn/k_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %895 = "onnx.Mul"(%893, %894) {onnx_node_name = "/model/layers.4/attn/k_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %896 = "onnx.MatMulInteger"(%y_45, %218, %y_zero_point_47, %217) {onnx_node_name = "/model/layers.4/attn/v_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %897 = "onnx.Cast"(%896) {onnx_node_name = "/model/layers.4/attn/v_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %898 = "onnx.Mul"(%y_scale_46, %216) {onnx_node_name = "/model/layers.4/attn/v_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %899 = "onnx.Mul"(%897, %898) {onnx_node_name = "/model/layers.4/attn/v_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %900 = "onnx.Reshape"(%891, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.4/attn/q_norm/Reshape_1"} : (tensor<1x128x2048xf32>, tensor<3xi64>) -> tensor<1x2048x128xf32>
    %901 = "onnx.Reshape"(%895, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.4/attn/k_norm/Reshape_1"} : (tensor<1x128x1024xf32>, tensor<3xi64>) -> tensor<1x1024x128xf32>
    %902 = "onnx.Custom"(%900, %27) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.4/attn/q_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x2048x128xf32>, tensor<128xf32>) -> tensor<1x2048x128xf32>
    %903 = "onnx.Custom"(%901, %28) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.4/attn/k_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x1024x128xf32>, tensor<128xf32>) -> tensor<1x1024x128xf32>
    %904 = "onnx.Reshape"(%902, %5) {allowzero = 0 : si64, onnx_node_name = "/model/layers.4/attn/q_norm/Reshape_2"} : (tensor<1x2048x128xf32>, tensor<3xi64>) -> tensor<1x128x2048xf32>
    %905 = "onnx.Reshape"(%903, %4) {allowzero = 0 : si64, onnx_node_name = "/model/layers.4/attn/k_norm/Reshape_2"} : (tensor<1x1024x128xf32>, tensor<3xi64>) -> tensor<1x128x1024xf32>
    %906 = "onnx.Custom"(%904, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.4/attn/q_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x2048xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x2048xf32>
    %907 = "onnx.Custom"(%905, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.4/attn/k_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x1024xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x1024xf32>
    %908:3 = "onnx.Custom"(%906, %907, %899, %arg11, %arg12, %731, %0, %3, %3) {do_rotary = 0 : si64, domain_name = "com.microsoft", function_name = "GroupQueryAttention", kv_num_heads = 8 : si64, num_heads = 16 : si64, onnx_node_name = "/model/layers.4/attn/GroupQueryAttention", rotary_interleaved = 0 : si64, scale = 0.0883883461 : f32, softcap = 0.000000e+00 : f32} : (tensor<1x128x2048xf32>, tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1x8x1x128xf32>, tensor<1x8x1x128xf32>, tensor<1x1xi32>, tensor<i32>, none, none) -> (tensor<1x128x2048xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>)
    %y_48, %y_scale_49, %y_zero_point_50 = "onnx.DynamicQuantizeLinear"(%908#0) {onnx_node_name = "/model/layers.4/attn/GroupQueryAttention/output_0_QuantizeLinear"} : (tensor<1x128x2048xf32>) -> (tensor<1x128x2048xui8>, tensor<f32>, tensor<ui8>)
    %909 = "onnx.MatMulInteger"(%y_48, %221, %y_zero_point_50, %220) {onnx_node_name = "/model/layers.4/attn/o_proj/MatMul_quant"} : (tensor<1x128x2048xui8>, tensor<2048x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %910 = "onnx.Cast"(%909) {onnx_node_name = "/model/layers.4/attn/o_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %911 = "onnx.Mul"(%y_scale_49, %219) {onnx_node_name = "/model/layers.4/attn/o_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %912 = "onnx.Mul"(%910, %911) {onnx_node_name = "/model/layers.4/attn/o_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %913:4 = "onnx.Custom"(%887#3, %912, %29) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.4/post_attention_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_51, %y_scale_52, %y_zero_point_53 = "onnx.DynamicQuantizeLinear"(%913#0) {onnx_node_name = "/model/layers.4/post_attention_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %914 = "onnx.MatMulInteger"(%y_51, %224, %y_zero_point_53, %223) {onnx_node_name = "/model/layers.4/mlp/gate_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %915 = "onnx.Cast"(%914) {onnx_node_name = "/model/layers.4/mlp/gate_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %916 = "onnx.Mul"(%y_scale_52, %222) {onnx_node_name = "/model/layers.4/mlp/gate_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %917 = "onnx.Mul"(%915, %916) {onnx_node_name = "/model/layers.4/mlp/gate_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %918 = "onnx.MatMulInteger"(%y_51, %227, %y_zero_point_53, %226) {onnx_node_name = "/model/layers.4/mlp/up_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %919 = "onnx.Cast"(%918) {onnx_node_name = "/model/layers.4/mlp/up_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %920 = "onnx.Mul"(%y_scale_52, %225) {onnx_node_name = "/model/layers.4/mlp/up_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %921 = "onnx.Mul"(%919, %920) {onnx_node_name = "/model/layers.4/mlp/up_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %922 = "onnx.Sigmoid"(%917) {onnx_node_name = "/model/layers.4/mlp/act_fn/Sigmoid"} : (tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %923 = "onnx.Mul"(%917, %922) {onnx_node_name = "/model/layers.4/mlp/act_fn/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %924 = "onnx.Mul"(%923, %921) {onnx_node_name = "/model/layers.4/mlp/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %y_54, %y_scale_55, %y_zero_point_56 = "onnx.DynamicQuantizeLinear"(%924) {onnx_node_name = "/model/layers.4/mlp/Mul/output_0_QuantizeLinear"} : (tensor<1x128x3072xf32>) -> (tensor<1x128x3072xui8>, tensor<f32>, tensor<ui8>)
    %925 = "onnx.MatMulInteger"(%y_54, %230, %y_zero_point_56, %229) {onnx_node_name = "/model/layers.4/mlp/down_proj/MatMul_quant"} : (tensor<1x128x3072xui8>, tensor<3072x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %926 = "onnx.Cast"(%925) {onnx_node_name = "/model/layers.4/mlp/down_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %927 = "onnx.Mul"(%y_scale_55, %228) {onnx_node_name = "/model/layers.4/mlp/down_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %928 = "onnx.Mul"(%926, %927) {onnx_node_name = "/model/layers.4/mlp/down_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %929:4 = "onnx.Custom"(%913#3, %928, %30) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.5/input_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_57, %y_scale_58, %y_zero_point_59 = "onnx.DynamicQuantizeLinear"(%929#0) {onnx_node_name = "/model/layers.5/input_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %930 = "onnx.MatMulInteger"(%y_57, %233, %y_zero_point_59, %232) {onnx_node_name = "/model/layers.5/attn/q_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x2048xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x2048xi32>
    %931 = "onnx.Cast"(%930) {onnx_node_name = "/model/layers.5/attn/q_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x2048xi32>) -> tensor<1x128x2048xf32>
    %932 = "onnx.Mul"(%y_scale_58, %231) {onnx_node_name = "/model/layers.5/attn/q_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %933 = "onnx.Mul"(%931, %932) {onnx_node_name = "/model/layers.5/attn/q_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x2048xf32>, tensor<f32>) -> tensor<1x128x2048xf32>
    %934 = "onnx.MatMulInteger"(%y_57, %236, %y_zero_point_59, %235) {onnx_node_name = "/model/layers.5/attn/k_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %935 = "onnx.Cast"(%934) {onnx_node_name = "/model/layers.5/attn/k_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %936 = "onnx.Mul"(%y_scale_58, %234) {onnx_node_name = "/model/layers.5/attn/k_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %937 = "onnx.Mul"(%935, %936) {onnx_node_name = "/model/layers.5/attn/k_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %938 = "onnx.MatMulInteger"(%y_57, %239, %y_zero_point_59, %238) {onnx_node_name = "/model/layers.5/attn/v_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %939 = "onnx.Cast"(%938) {onnx_node_name = "/model/layers.5/attn/v_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %940 = "onnx.Mul"(%y_scale_58, %237) {onnx_node_name = "/model/layers.5/attn/v_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %941 = "onnx.Mul"(%939, %940) {onnx_node_name = "/model/layers.5/attn/v_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %942 = "onnx.Reshape"(%933, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.5/attn/q_norm/Reshape_1"} : (tensor<1x128x2048xf32>, tensor<3xi64>) -> tensor<1x2048x128xf32>
    %943 = "onnx.Reshape"(%937, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.5/attn/k_norm/Reshape_1"} : (tensor<1x128x1024xf32>, tensor<3xi64>) -> tensor<1x1024x128xf32>
    %944 = "onnx.Custom"(%942, %31) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.5/attn/q_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x2048x128xf32>, tensor<128xf32>) -> tensor<1x2048x128xf32>
    %945 = "onnx.Custom"(%943, %32) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.5/attn/k_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x1024x128xf32>, tensor<128xf32>) -> tensor<1x1024x128xf32>
    %946 = "onnx.Reshape"(%944, %5) {allowzero = 0 : si64, onnx_node_name = "/model/layers.5/attn/q_norm/Reshape_2"} : (tensor<1x2048x128xf32>, tensor<3xi64>) -> tensor<1x128x2048xf32>
    %947 = "onnx.Reshape"(%945, %4) {allowzero = 0 : si64, onnx_node_name = "/model/layers.5/attn/k_norm/Reshape_2"} : (tensor<1x1024x128xf32>, tensor<3xi64>) -> tensor<1x128x1024xf32>
    %948 = "onnx.Custom"(%946, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.5/attn/q_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x2048xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x2048xf32>
    %949 = "onnx.Custom"(%947, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.5/attn/k_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x1024xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x1024xf32>
    %950:3 = "onnx.Custom"(%948, %949, %941, %arg13, %arg14, %731, %0, %3, %3) {do_rotary = 0 : si64, domain_name = "com.microsoft", function_name = "GroupQueryAttention", kv_num_heads = 8 : si64, num_heads = 16 : si64, onnx_node_name = "/model/layers.5/attn/GroupQueryAttention", rotary_interleaved = 0 : si64, scale = 0.0883883461 : f32, softcap = 0.000000e+00 : f32} : (tensor<1x128x2048xf32>, tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1x8x1x128xf32>, tensor<1x8x1x128xf32>, tensor<1x1xi32>, tensor<i32>, none, none) -> (tensor<1x128x2048xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>)
    %y_60, %y_scale_61, %y_zero_point_62 = "onnx.DynamicQuantizeLinear"(%950#0) {onnx_node_name = "/model/layers.5/attn/GroupQueryAttention/output_0_QuantizeLinear"} : (tensor<1x128x2048xf32>) -> (tensor<1x128x2048xui8>, tensor<f32>, tensor<ui8>)
    %951 = "onnx.MatMulInteger"(%y_60, %242, %y_zero_point_62, %241) {onnx_node_name = "/model/layers.5/attn/o_proj/MatMul_quant"} : (tensor<1x128x2048xui8>, tensor<2048x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %952 = "onnx.Cast"(%951) {onnx_node_name = "/model/layers.5/attn/o_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %953 = "onnx.Mul"(%y_scale_61, %240) {onnx_node_name = "/model/layers.5/attn/o_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %954 = "onnx.Mul"(%952, %953) {onnx_node_name = "/model/layers.5/attn/o_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %955:4 = "onnx.Custom"(%929#3, %954, %33) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.5/post_attention_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_63, %y_scale_64, %y_zero_point_65 = "onnx.DynamicQuantizeLinear"(%955#0) {onnx_node_name = "/model/layers.5/post_attention_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %956 = "onnx.MatMulInteger"(%y_63, %245, %y_zero_point_65, %244) {onnx_node_name = "/model/layers.5/mlp/gate_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %957 = "onnx.Cast"(%956) {onnx_node_name = "/model/layers.5/mlp/gate_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %958 = "onnx.Mul"(%y_scale_64, %243) {onnx_node_name = "/model/layers.5/mlp/gate_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %959 = "onnx.Mul"(%957, %958) {onnx_node_name = "/model/layers.5/mlp/gate_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %960 = "onnx.MatMulInteger"(%y_63, %248, %y_zero_point_65, %247) {onnx_node_name = "/model/layers.5/mlp/up_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %961 = "onnx.Cast"(%960) {onnx_node_name = "/model/layers.5/mlp/up_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %962 = "onnx.Mul"(%y_scale_64, %246) {onnx_node_name = "/model/layers.5/mlp/up_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %963 = "onnx.Mul"(%961, %962) {onnx_node_name = "/model/layers.5/mlp/up_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %964 = "onnx.Sigmoid"(%959) {onnx_node_name = "/model/layers.5/mlp/act_fn/Sigmoid"} : (tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %965 = "onnx.Mul"(%959, %964) {onnx_node_name = "/model/layers.5/mlp/act_fn/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %966 = "onnx.Mul"(%965, %963) {onnx_node_name = "/model/layers.5/mlp/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %y_66, %y_scale_67, %y_zero_point_68 = "onnx.DynamicQuantizeLinear"(%966) {onnx_node_name = "/model/layers.5/mlp/Mul/output_0_QuantizeLinear"} : (tensor<1x128x3072xf32>) -> (tensor<1x128x3072xui8>, tensor<f32>, tensor<ui8>)
    %967 = "onnx.MatMulInteger"(%y_66, %251, %y_zero_point_68, %250) {onnx_node_name = "/model/layers.5/mlp/down_proj/MatMul_quant"} : (tensor<1x128x3072xui8>, tensor<3072x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %968 = "onnx.Cast"(%967) {onnx_node_name = "/model/layers.5/mlp/down_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %969 = "onnx.Mul"(%y_scale_67, %249) {onnx_node_name = "/model/layers.5/mlp/down_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %970 = "onnx.Mul"(%968, %969) {onnx_node_name = "/model/layers.5/mlp/down_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %971:4 = "onnx.Custom"(%955#3, %970, %34) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.6/input_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_69, %y_scale_70, %y_zero_point_71 = "onnx.DynamicQuantizeLinear"(%971#0) {onnx_node_name = "/model/layers.6/input_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %972 = "onnx.MatMulInteger"(%y_69, %254, %y_zero_point_71, %253) {onnx_node_name = "/model/layers.6/attn/q_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x2048xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x2048xi32>
    %973 = "onnx.Cast"(%972) {onnx_node_name = "/model/layers.6/attn/q_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x2048xi32>) -> tensor<1x128x2048xf32>
    %974 = "onnx.Mul"(%y_scale_70, %252) {onnx_node_name = "/model/layers.6/attn/q_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %975 = "onnx.Mul"(%973, %974) {onnx_node_name = "/model/layers.6/attn/q_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x2048xf32>, tensor<f32>) -> tensor<1x128x2048xf32>
    %976 = "onnx.MatMulInteger"(%y_69, %257, %y_zero_point_71, %256) {onnx_node_name = "/model/layers.6/attn/k_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %977 = "onnx.Cast"(%976) {onnx_node_name = "/model/layers.6/attn/k_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %978 = "onnx.Mul"(%y_scale_70, %255) {onnx_node_name = "/model/layers.6/attn/k_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %979 = "onnx.Mul"(%977, %978) {onnx_node_name = "/model/layers.6/attn/k_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %980 = "onnx.MatMulInteger"(%y_69, %260, %y_zero_point_71, %259) {onnx_node_name = "/model/layers.6/attn/v_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %981 = "onnx.Cast"(%980) {onnx_node_name = "/model/layers.6/attn/v_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %982 = "onnx.Mul"(%y_scale_70, %258) {onnx_node_name = "/model/layers.6/attn/v_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %983 = "onnx.Mul"(%981, %982) {onnx_node_name = "/model/layers.6/attn/v_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %984 = "onnx.Reshape"(%975, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.6/attn/q_norm/Reshape_1"} : (tensor<1x128x2048xf32>, tensor<3xi64>) -> tensor<1x2048x128xf32>
    %985 = "onnx.Reshape"(%979, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.6/attn/k_norm/Reshape_1"} : (tensor<1x128x1024xf32>, tensor<3xi64>) -> tensor<1x1024x128xf32>
    %986 = "onnx.Custom"(%984, %35) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.6/attn/q_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x2048x128xf32>, tensor<128xf32>) -> tensor<1x2048x128xf32>
    %987 = "onnx.Custom"(%985, %36) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.6/attn/k_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x1024x128xf32>, tensor<128xf32>) -> tensor<1x1024x128xf32>
    %988 = "onnx.Reshape"(%986, %5) {allowzero = 0 : si64, onnx_node_name = "/model/layers.6/attn/q_norm/Reshape_2"} : (tensor<1x2048x128xf32>, tensor<3xi64>) -> tensor<1x128x2048xf32>
    %989 = "onnx.Reshape"(%987, %4) {allowzero = 0 : si64, onnx_node_name = "/model/layers.6/attn/k_norm/Reshape_2"} : (tensor<1x1024x128xf32>, tensor<3xi64>) -> tensor<1x128x1024xf32>
    %990 = "onnx.Custom"(%988, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.6/attn/q_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x2048xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x2048xf32>
    %991 = "onnx.Custom"(%989, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.6/attn/k_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x1024xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x1024xf32>
    %992:3 = "onnx.Custom"(%990, %991, %983, %arg15, %arg16, %731, %0, %3, %3) {do_rotary = 0 : si64, domain_name = "com.microsoft", function_name = "GroupQueryAttention", kv_num_heads = 8 : si64, num_heads = 16 : si64, onnx_node_name = "/model/layers.6/attn/GroupQueryAttention", rotary_interleaved = 0 : si64, scale = 0.0883883461 : f32, softcap = 0.000000e+00 : f32} : (tensor<1x128x2048xf32>, tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1x8x1x128xf32>, tensor<1x8x1x128xf32>, tensor<1x1xi32>, tensor<i32>, none, none) -> (tensor<1x128x2048xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>)
    %y_72, %y_scale_73, %y_zero_point_74 = "onnx.DynamicQuantizeLinear"(%992#0) {onnx_node_name = "/model/layers.6/attn/GroupQueryAttention/output_0_QuantizeLinear"} : (tensor<1x128x2048xf32>) -> (tensor<1x128x2048xui8>, tensor<f32>, tensor<ui8>)
    %993 = "onnx.MatMulInteger"(%y_72, %263, %y_zero_point_74, %262) {onnx_node_name = "/model/layers.6/attn/o_proj/MatMul_quant"} : (tensor<1x128x2048xui8>, tensor<2048x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %994 = "onnx.Cast"(%993) {onnx_node_name = "/model/layers.6/attn/o_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %995 = "onnx.Mul"(%y_scale_73, %261) {onnx_node_name = "/model/layers.6/attn/o_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %996 = "onnx.Mul"(%994, %995) {onnx_node_name = "/model/layers.6/attn/o_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %997:4 = "onnx.Custom"(%971#3, %996, %37) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.6/post_attention_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_75, %y_scale_76, %y_zero_point_77 = "onnx.DynamicQuantizeLinear"(%997#0) {onnx_node_name = "/model/layers.6/post_attention_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %998 = "onnx.MatMulInteger"(%y_75, %266, %y_zero_point_77, %265) {onnx_node_name = "/model/layers.6/mlp/gate_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %999 = "onnx.Cast"(%998) {onnx_node_name = "/model/layers.6/mlp/gate_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %1000 = "onnx.Mul"(%y_scale_76, %264) {onnx_node_name = "/model/layers.6/mlp/gate_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1001 = "onnx.Mul"(%999, %1000) {onnx_node_name = "/model/layers.6/mlp/gate_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %1002 = "onnx.MatMulInteger"(%y_75, %269, %y_zero_point_77, %268) {onnx_node_name = "/model/layers.6/mlp/up_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %1003 = "onnx.Cast"(%1002) {onnx_node_name = "/model/layers.6/mlp/up_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %1004 = "onnx.Mul"(%y_scale_76, %267) {onnx_node_name = "/model/layers.6/mlp/up_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1005 = "onnx.Mul"(%1003, %1004) {onnx_node_name = "/model/layers.6/mlp/up_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %1006 = "onnx.Sigmoid"(%1001) {onnx_node_name = "/model/layers.6/mlp/act_fn/Sigmoid"} : (tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %1007 = "onnx.Mul"(%1001, %1006) {onnx_node_name = "/model/layers.6/mlp/act_fn/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %1008 = "onnx.Mul"(%1007, %1005) {onnx_node_name = "/model/layers.6/mlp/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %y_78, %y_scale_79, %y_zero_point_80 = "onnx.DynamicQuantizeLinear"(%1008) {onnx_node_name = "/model/layers.6/mlp/Mul/output_0_QuantizeLinear"} : (tensor<1x128x3072xf32>) -> (tensor<1x128x3072xui8>, tensor<f32>, tensor<ui8>)
    %1009 = "onnx.MatMulInteger"(%y_78, %272, %y_zero_point_80, %271) {onnx_node_name = "/model/layers.6/mlp/down_proj/MatMul_quant"} : (tensor<1x128x3072xui8>, tensor<3072x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1010 = "onnx.Cast"(%1009) {onnx_node_name = "/model/layers.6/mlp/down_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1011 = "onnx.Mul"(%y_scale_79, %270) {onnx_node_name = "/model/layers.6/mlp/down_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1012 = "onnx.Mul"(%1010, %1011) {onnx_node_name = "/model/layers.6/mlp/down_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1013:4 = "onnx.Custom"(%997#3, %1012, %38) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.7/input_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_81, %y_scale_82, %y_zero_point_83 = "onnx.DynamicQuantizeLinear"(%1013#0) {onnx_node_name = "/model/layers.7/input_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %1014 = "onnx.MatMulInteger"(%y_81, %275, %y_zero_point_83, %274) {onnx_node_name = "/model/layers.7/attn/q_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x2048xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x2048xi32>
    %1015 = "onnx.Cast"(%1014) {onnx_node_name = "/model/layers.7/attn/q_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x2048xi32>) -> tensor<1x128x2048xf32>
    %1016 = "onnx.Mul"(%y_scale_82, %273) {onnx_node_name = "/model/layers.7/attn/q_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1017 = "onnx.Mul"(%1015, %1016) {onnx_node_name = "/model/layers.7/attn/q_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x2048xf32>, tensor<f32>) -> tensor<1x128x2048xf32>
    %1018 = "onnx.MatMulInteger"(%y_81, %278, %y_zero_point_83, %277) {onnx_node_name = "/model/layers.7/attn/k_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1019 = "onnx.Cast"(%1018) {onnx_node_name = "/model/layers.7/attn/k_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1020 = "onnx.Mul"(%y_scale_82, %276) {onnx_node_name = "/model/layers.7/attn/k_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1021 = "onnx.Mul"(%1019, %1020) {onnx_node_name = "/model/layers.7/attn/k_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1022 = "onnx.MatMulInteger"(%y_81, %281, %y_zero_point_83, %280) {onnx_node_name = "/model/layers.7/attn/v_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1023 = "onnx.Cast"(%1022) {onnx_node_name = "/model/layers.7/attn/v_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1024 = "onnx.Mul"(%y_scale_82, %279) {onnx_node_name = "/model/layers.7/attn/v_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1025 = "onnx.Mul"(%1023, %1024) {onnx_node_name = "/model/layers.7/attn/v_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1026 = "onnx.Reshape"(%1017, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.7/attn/q_norm/Reshape_1"} : (tensor<1x128x2048xf32>, tensor<3xi64>) -> tensor<1x2048x128xf32>
    %1027 = "onnx.Reshape"(%1021, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.7/attn/k_norm/Reshape_1"} : (tensor<1x128x1024xf32>, tensor<3xi64>) -> tensor<1x1024x128xf32>
    %1028 = "onnx.Custom"(%1026, %39) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.7/attn/q_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x2048x128xf32>, tensor<128xf32>) -> tensor<1x2048x128xf32>
    %1029 = "onnx.Custom"(%1027, %40) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.7/attn/k_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x1024x128xf32>, tensor<128xf32>) -> tensor<1x1024x128xf32>
    %1030 = "onnx.Reshape"(%1028, %5) {allowzero = 0 : si64, onnx_node_name = "/model/layers.7/attn/q_norm/Reshape_2"} : (tensor<1x2048x128xf32>, tensor<3xi64>) -> tensor<1x128x2048xf32>
    %1031 = "onnx.Reshape"(%1029, %4) {allowzero = 0 : si64, onnx_node_name = "/model/layers.7/attn/k_norm/Reshape_2"} : (tensor<1x1024x128xf32>, tensor<3xi64>) -> tensor<1x128x1024xf32>
    %1032 = "onnx.Custom"(%1030, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.7/attn/q_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x2048xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x2048xf32>
    %1033 = "onnx.Custom"(%1031, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.7/attn/k_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x1024xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x1024xf32>
    %1034:3 = "onnx.Custom"(%1032, %1033, %1025, %arg17, %arg18, %731, %0, %3, %3) {do_rotary = 0 : si64, domain_name = "com.microsoft", function_name = "GroupQueryAttention", kv_num_heads = 8 : si64, num_heads = 16 : si64, onnx_node_name = "/model/layers.7/attn/GroupQueryAttention", rotary_interleaved = 0 : si64, scale = 0.0883883461 : f32, softcap = 0.000000e+00 : f32} : (tensor<1x128x2048xf32>, tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1x8x1x128xf32>, tensor<1x8x1x128xf32>, tensor<1x1xi32>, tensor<i32>, none, none) -> (tensor<1x128x2048xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>)
    %y_84, %y_scale_85, %y_zero_point_86 = "onnx.DynamicQuantizeLinear"(%1034#0) {onnx_node_name = "/model/layers.7/attn/GroupQueryAttention/output_0_QuantizeLinear"} : (tensor<1x128x2048xf32>) -> (tensor<1x128x2048xui8>, tensor<f32>, tensor<ui8>)
    %1035 = "onnx.MatMulInteger"(%y_84, %284, %y_zero_point_86, %283) {onnx_node_name = "/model/layers.7/attn/o_proj/MatMul_quant"} : (tensor<1x128x2048xui8>, tensor<2048x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1036 = "onnx.Cast"(%1035) {onnx_node_name = "/model/layers.7/attn/o_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1037 = "onnx.Mul"(%y_scale_85, %282) {onnx_node_name = "/model/layers.7/attn/o_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1038 = "onnx.Mul"(%1036, %1037) {onnx_node_name = "/model/layers.7/attn/o_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1039:4 = "onnx.Custom"(%1013#3, %1038, %41) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.7/post_attention_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_87, %y_scale_88, %y_zero_point_89 = "onnx.DynamicQuantizeLinear"(%1039#0) {onnx_node_name = "/model/layers.7/post_attention_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %1040 = "onnx.MatMulInteger"(%y_87, %287, %y_zero_point_89, %286) {onnx_node_name = "/model/layers.7/mlp/gate_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %1041 = "onnx.Cast"(%1040) {onnx_node_name = "/model/layers.7/mlp/gate_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %1042 = "onnx.Mul"(%y_scale_88, %285) {onnx_node_name = "/model/layers.7/mlp/gate_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1043 = "onnx.Mul"(%1041, %1042) {onnx_node_name = "/model/layers.7/mlp/gate_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %1044 = "onnx.MatMulInteger"(%y_87, %290, %y_zero_point_89, %289) {onnx_node_name = "/model/layers.7/mlp/up_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %1045 = "onnx.Cast"(%1044) {onnx_node_name = "/model/layers.7/mlp/up_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %1046 = "onnx.Mul"(%y_scale_88, %288) {onnx_node_name = "/model/layers.7/mlp/up_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1047 = "onnx.Mul"(%1045, %1046) {onnx_node_name = "/model/layers.7/mlp/up_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %1048 = "onnx.Sigmoid"(%1043) {onnx_node_name = "/model/layers.7/mlp/act_fn/Sigmoid"} : (tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %1049 = "onnx.Mul"(%1043, %1048) {onnx_node_name = "/model/layers.7/mlp/act_fn/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %1050 = "onnx.Mul"(%1049, %1047) {onnx_node_name = "/model/layers.7/mlp/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %y_90, %y_scale_91, %y_zero_point_92 = "onnx.DynamicQuantizeLinear"(%1050) {onnx_node_name = "/model/layers.7/mlp/Mul/output_0_QuantizeLinear"} : (tensor<1x128x3072xf32>) -> (tensor<1x128x3072xui8>, tensor<f32>, tensor<ui8>)
    %1051 = "onnx.MatMulInteger"(%y_90, %293, %y_zero_point_92, %292) {onnx_node_name = "/model/layers.7/mlp/down_proj/MatMul_quant"} : (tensor<1x128x3072xui8>, tensor<3072x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1052 = "onnx.Cast"(%1051) {onnx_node_name = "/model/layers.7/mlp/down_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1053 = "onnx.Mul"(%y_scale_91, %291) {onnx_node_name = "/model/layers.7/mlp/down_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1054 = "onnx.Mul"(%1052, %1053) {onnx_node_name = "/model/layers.7/mlp/down_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1055:4 = "onnx.Custom"(%1039#3, %1054, %42) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.8/input_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_93, %y_scale_94, %y_zero_point_95 = "onnx.DynamicQuantizeLinear"(%1055#0) {onnx_node_name = "/model/layers.8/input_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %1056 = "onnx.MatMulInteger"(%y_93, %296, %y_zero_point_95, %295) {onnx_node_name = "/model/layers.8/attn/q_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x2048xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x2048xi32>
    %1057 = "onnx.Cast"(%1056) {onnx_node_name = "/model/layers.8/attn/q_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x2048xi32>) -> tensor<1x128x2048xf32>
    %1058 = "onnx.Mul"(%y_scale_94, %294) {onnx_node_name = "/model/layers.8/attn/q_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1059 = "onnx.Mul"(%1057, %1058) {onnx_node_name = "/model/layers.8/attn/q_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x2048xf32>, tensor<f32>) -> tensor<1x128x2048xf32>
    %1060 = "onnx.MatMulInteger"(%y_93, %299, %y_zero_point_95, %298) {onnx_node_name = "/model/layers.8/attn/k_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1061 = "onnx.Cast"(%1060) {onnx_node_name = "/model/layers.8/attn/k_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1062 = "onnx.Mul"(%y_scale_94, %297) {onnx_node_name = "/model/layers.8/attn/k_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1063 = "onnx.Mul"(%1061, %1062) {onnx_node_name = "/model/layers.8/attn/k_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1064 = "onnx.MatMulInteger"(%y_93, %302, %y_zero_point_95, %301) {onnx_node_name = "/model/layers.8/attn/v_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1065 = "onnx.Cast"(%1064) {onnx_node_name = "/model/layers.8/attn/v_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1066 = "onnx.Mul"(%y_scale_94, %300) {onnx_node_name = "/model/layers.8/attn/v_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1067 = "onnx.Mul"(%1065, %1066) {onnx_node_name = "/model/layers.8/attn/v_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1068 = "onnx.Reshape"(%1059, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.8/attn/q_norm/Reshape_1"} : (tensor<1x128x2048xf32>, tensor<3xi64>) -> tensor<1x2048x128xf32>
    %1069 = "onnx.Reshape"(%1063, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.8/attn/k_norm/Reshape_1"} : (tensor<1x128x1024xf32>, tensor<3xi64>) -> tensor<1x1024x128xf32>
    %1070 = "onnx.Custom"(%1068, %43) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.8/attn/q_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x2048x128xf32>, tensor<128xf32>) -> tensor<1x2048x128xf32>
    %1071 = "onnx.Custom"(%1069, %44) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.8/attn/k_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x1024x128xf32>, tensor<128xf32>) -> tensor<1x1024x128xf32>
    %1072 = "onnx.Reshape"(%1070, %5) {allowzero = 0 : si64, onnx_node_name = "/model/layers.8/attn/q_norm/Reshape_2"} : (tensor<1x2048x128xf32>, tensor<3xi64>) -> tensor<1x128x2048xf32>
    %1073 = "onnx.Reshape"(%1071, %4) {allowzero = 0 : si64, onnx_node_name = "/model/layers.8/attn/k_norm/Reshape_2"} : (tensor<1x1024x128xf32>, tensor<3xi64>) -> tensor<1x128x1024xf32>
    %1074 = "onnx.Custom"(%1072, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.8/attn/q_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x2048xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x2048xf32>
    %1075 = "onnx.Custom"(%1073, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.8/attn/k_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x1024xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x1024xf32>
    %1076:3 = "onnx.Custom"(%1074, %1075, %1067, %arg19, %arg20, %731, %0, %3, %3) {do_rotary = 0 : si64, domain_name = "com.microsoft", function_name = "GroupQueryAttention", kv_num_heads = 8 : si64, num_heads = 16 : si64, onnx_node_name = "/model/layers.8/attn/GroupQueryAttention", rotary_interleaved = 0 : si64, scale = 0.0883883461 : f32, softcap = 0.000000e+00 : f32} : (tensor<1x128x2048xf32>, tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1x8x1x128xf32>, tensor<1x8x1x128xf32>, tensor<1x1xi32>, tensor<i32>, none, none) -> (tensor<1x128x2048xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>)
    %y_96, %y_scale_97, %y_zero_point_98 = "onnx.DynamicQuantizeLinear"(%1076#0) {onnx_node_name = "/model/layers.8/attn/GroupQueryAttention/output_0_QuantizeLinear"} : (tensor<1x128x2048xf32>) -> (tensor<1x128x2048xui8>, tensor<f32>, tensor<ui8>)
    %1077 = "onnx.MatMulInteger"(%y_96, %305, %y_zero_point_98, %304) {onnx_node_name = "/model/layers.8/attn/o_proj/MatMul_quant"} : (tensor<1x128x2048xui8>, tensor<2048x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1078 = "onnx.Cast"(%1077) {onnx_node_name = "/model/layers.8/attn/o_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1079 = "onnx.Mul"(%y_scale_97, %303) {onnx_node_name = "/model/layers.8/attn/o_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1080 = "onnx.Mul"(%1078, %1079) {onnx_node_name = "/model/layers.8/attn/o_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1081:4 = "onnx.Custom"(%1055#3, %1080, %45) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.8/post_attention_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_99, %y_scale_100, %y_zero_point_101 = "onnx.DynamicQuantizeLinear"(%1081#0) {onnx_node_name = "/model/layers.8/post_attention_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %1082 = "onnx.MatMulInteger"(%y_99, %308, %y_zero_point_101, %307) {onnx_node_name = "/model/layers.8/mlp/gate_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %1083 = "onnx.Cast"(%1082) {onnx_node_name = "/model/layers.8/mlp/gate_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %1084 = "onnx.Mul"(%y_scale_100, %306) {onnx_node_name = "/model/layers.8/mlp/gate_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1085 = "onnx.Mul"(%1083, %1084) {onnx_node_name = "/model/layers.8/mlp/gate_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %1086 = "onnx.MatMulInteger"(%y_99, %311, %y_zero_point_101, %310) {onnx_node_name = "/model/layers.8/mlp/up_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %1087 = "onnx.Cast"(%1086) {onnx_node_name = "/model/layers.8/mlp/up_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %1088 = "onnx.Mul"(%y_scale_100, %309) {onnx_node_name = "/model/layers.8/mlp/up_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1089 = "onnx.Mul"(%1087, %1088) {onnx_node_name = "/model/layers.8/mlp/up_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %1090 = "onnx.Sigmoid"(%1085) {onnx_node_name = "/model/layers.8/mlp/act_fn/Sigmoid"} : (tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %1091 = "onnx.Mul"(%1085, %1090) {onnx_node_name = "/model/layers.8/mlp/act_fn/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %1092 = "onnx.Mul"(%1091, %1089) {onnx_node_name = "/model/layers.8/mlp/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %y_102, %y_scale_103, %y_zero_point_104 = "onnx.DynamicQuantizeLinear"(%1092) {onnx_node_name = "/model/layers.8/mlp/Mul/output_0_QuantizeLinear"} : (tensor<1x128x3072xf32>) -> (tensor<1x128x3072xui8>, tensor<f32>, tensor<ui8>)
    %1093 = "onnx.MatMulInteger"(%y_102, %314, %y_zero_point_104, %313) {onnx_node_name = "/model/layers.8/mlp/down_proj/MatMul_quant"} : (tensor<1x128x3072xui8>, tensor<3072x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1094 = "onnx.Cast"(%1093) {onnx_node_name = "/model/layers.8/mlp/down_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1095 = "onnx.Mul"(%y_scale_103, %312) {onnx_node_name = "/model/layers.8/mlp/down_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1096 = "onnx.Mul"(%1094, %1095) {onnx_node_name = "/model/layers.8/mlp/down_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1097:4 = "onnx.Custom"(%1081#3, %1096, %46) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.9/input_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_105, %y_scale_106, %y_zero_point_107 = "onnx.DynamicQuantizeLinear"(%1097#0) {onnx_node_name = "/model/layers.9/input_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %1098 = "onnx.MatMulInteger"(%y_105, %317, %y_zero_point_107, %316) {onnx_node_name = "/model/layers.9/attn/q_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x2048xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x2048xi32>
    %1099 = "onnx.Cast"(%1098) {onnx_node_name = "/model/layers.9/attn/q_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x2048xi32>) -> tensor<1x128x2048xf32>
    %1100 = "onnx.Mul"(%y_scale_106, %315) {onnx_node_name = "/model/layers.9/attn/q_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1101 = "onnx.Mul"(%1099, %1100) {onnx_node_name = "/model/layers.9/attn/q_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x2048xf32>, tensor<f32>) -> tensor<1x128x2048xf32>
    %1102 = "onnx.MatMulInteger"(%y_105, %320, %y_zero_point_107, %319) {onnx_node_name = "/model/layers.9/attn/k_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1103 = "onnx.Cast"(%1102) {onnx_node_name = "/model/layers.9/attn/k_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1104 = "onnx.Mul"(%y_scale_106, %318) {onnx_node_name = "/model/layers.9/attn/k_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1105 = "onnx.Mul"(%1103, %1104) {onnx_node_name = "/model/layers.9/attn/k_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1106 = "onnx.MatMulInteger"(%y_105, %323, %y_zero_point_107, %322) {onnx_node_name = "/model/layers.9/attn/v_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1107 = "onnx.Cast"(%1106) {onnx_node_name = "/model/layers.9/attn/v_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1108 = "onnx.Mul"(%y_scale_106, %321) {onnx_node_name = "/model/layers.9/attn/v_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1109 = "onnx.Mul"(%1107, %1108) {onnx_node_name = "/model/layers.9/attn/v_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1110 = "onnx.Reshape"(%1101, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.9/attn/q_norm/Reshape_1"} : (tensor<1x128x2048xf32>, tensor<3xi64>) -> tensor<1x2048x128xf32>
    %1111 = "onnx.Reshape"(%1105, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.9/attn/k_norm/Reshape_1"} : (tensor<1x128x1024xf32>, tensor<3xi64>) -> tensor<1x1024x128xf32>
    %1112 = "onnx.Custom"(%1110, %47) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.9/attn/q_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x2048x128xf32>, tensor<128xf32>) -> tensor<1x2048x128xf32>
    %1113 = "onnx.Custom"(%1111, %48) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.9/attn/k_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x1024x128xf32>, tensor<128xf32>) -> tensor<1x1024x128xf32>
    %1114 = "onnx.Reshape"(%1112, %5) {allowzero = 0 : si64, onnx_node_name = "/model/layers.9/attn/q_norm/Reshape_2"} : (tensor<1x2048x128xf32>, tensor<3xi64>) -> tensor<1x128x2048xf32>
    %1115 = "onnx.Reshape"(%1113, %4) {allowzero = 0 : si64, onnx_node_name = "/model/layers.9/attn/k_norm/Reshape_2"} : (tensor<1x1024x128xf32>, tensor<3xi64>) -> tensor<1x128x1024xf32>
    %1116 = "onnx.Custom"(%1114, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.9/attn/q_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x2048xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x2048xf32>
    %1117 = "onnx.Custom"(%1115, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.9/attn/k_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x1024xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x1024xf32>
    %1118:3 = "onnx.Custom"(%1116, %1117, %1109, %arg21, %arg22, %731, %0, %3, %3) {do_rotary = 0 : si64, domain_name = "com.microsoft", function_name = "GroupQueryAttention", kv_num_heads = 8 : si64, num_heads = 16 : si64, onnx_node_name = "/model/layers.9/attn/GroupQueryAttention", rotary_interleaved = 0 : si64, scale = 0.0883883461 : f32, softcap = 0.000000e+00 : f32} : (tensor<1x128x2048xf32>, tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1x8x1x128xf32>, tensor<1x8x1x128xf32>, tensor<1x1xi32>, tensor<i32>, none, none) -> (tensor<1x128x2048xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>)
    %y_108, %y_scale_109, %y_zero_point_110 = "onnx.DynamicQuantizeLinear"(%1118#0) {onnx_node_name = "/model/layers.9/attn/GroupQueryAttention/output_0_QuantizeLinear"} : (tensor<1x128x2048xf32>) -> (tensor<1x128x2048xui8>, tensor<f32>, tensor<ui8>)
    %1119 = "onnx.MatMulInteger"(%y_108, %326, %y_zero_point_110, %325) {onnx_node_name = "/model/layers.9/attn/o_proj/MatMul_quant"} : (tensor<1x128x2048xui8>, tensor<2048x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1120 = "onnx.Cast"(%1119) {onnx_node_name = "/model/layers.9/attn/o_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1121 = "onnx.Mul"(%y_scale_109, %324) {onnx_node_name = "/model/layers.9/attn/o_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1122 = "onnx.Mul"(%1120, %1121) {onnx_node_name = "/model/layers.9/attn/o_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1123:4 = "onnx.Custom"(%1097#3, %1122, %49) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.9/post_attention_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_111, %y_scale_112, %y_zero_point_113 = "onnx.DynamicQuantizeLinear"(%1123#0) {onnx_node_name = "/model/layers.9/post_attention_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %1124 = "onnx.MatMulInteger"(%y_111, %329, %y_zero_point_113, %328) {onnx_node_name = "/model/layers.9/mlp/gate_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %1125 = "onnx.Cast"(%1124) {onnx_node_name = "/model/layers.9/mlp/gate_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %1126 = "onnx.Mul"(%y_scale_112, %327) {onnx_node_name = "/model/layers.9/mlp/gate_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1127 = "onnx.Mul"(%1125, %1126) {onnx_node_name = "/model/layers.9/mlp/gate_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %1128 = "onnx.MatMulInteger"(%y_111, %332, %y_zero_point_113, %331) {onnx_node_name = "/model/layers.9/mlp/up_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %1129 = "onnx.Cast"(%1128) {onnx_node_name = "/model/layers.9/mlp/up_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %1130 = "onnx.Mul"(%y_scale_112, %330) {onnx_node_name = "/model/layers.9/mlp/up_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1131 = "onnx.Mul"(%1129, %1130) {onnx_node_name = "/model/layers.9/mlp/up_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %1132 = "onnx.Sigmoid"(%1127) {onnx_node_name = "/model/layers.9/mlp/act_fn/Sigmoid"} : (tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %1133 = "onnx.Mul"(%1127, %1132) {onnx_node_name = "/model/layers.9/mlp/act_fn/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %1134 = "onnx.Mul"(%1133, %1131) {onnx_node_name = "/model/layers.9/mlp/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %y_114, %y_scale_115, %y_zero_point_116 = "onnx.DynamicQuantizeLinear"(%1134) {onnx_node_name = "/model/layers.9/mlp/Mul/output_0_QuantizeLinear"} : (tensor<1x128x3072xf32>) -> (tensor<1x128x3072xui8>, tensor<f32>, tensor<ui8>)
    %1135 = "onnx.MatMulInteger"(%y_114, %335, %y_zero_point_116, %334) {onnx_node_name = "/model/layers.9/mlp/down_proj/MatMul_quant"} : (tensor<1x128x3072xui8>, tensor<3072x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1136 = "onnx.Cast"(%1135) {onnx_node_name = "/model/layers.9/mlp/down_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1137 = "onnx.Mul"(%y_scale_115, %333) {onnx_node_name = "/model/layers.9/mlp/down_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1138 = "onnx.Mul"(%1136, %1137) {onnx_node_name = "/model/layers.9/mlp/down_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1139:4 = "onnx.Custom"(%1123#3, %1138, %50) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.10/input_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_117, %y_scale_118, %y_zero_point_119 = "onnx.DynamicQuantizeLinear"(%1139#0) {onnx_node_name = "/model/layers.10/input_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %1140 = "onnx.MatMulInteger"(%y_117, %338, %y_zero_point_119, %337) {onnx_node_name = "/model/layers.10/attn/q_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x2048xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x2048xi32>
    %1141 = "onnx.Cast"(%1140) {onnx_node_name = "/model/layers.10/attn/q_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x2048xi32>) -> tensor<1x128x2048xf32>
    %1142 = "onnx.Mul"(%y_scale_118, %336) {onnx_node_name = "/model/layers.10/attn/q_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1143 = "onnx.Mul"(%1141, %1142) {onnx_node_name = "/model/layers.10/attn/q_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x2048xf32>, tensor<f32>) -> tensor<1x128x2048xf32>
    %1144 = "onnx.MatMulInteger"(%y_117, %341, %y_zero_point_119, %340) {onnx_node_name = "/model/layers.10/attn/k_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1145 = "onnx.Cast"(%1144) {onnx_node_name = "/model/layers.10/attn/k_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1146 = "onnx.Mul"(%y_scale_118, %339) {onnx_node_name = "/model/layers.10/attn/k_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1147 = "onnx.Mul"(%1145, %1146) {onnx_node_name = "/model/layers.10/attn/k_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1148 = "onnx.MatMulInteger"(%y_117, %344, %y_zero_point_119, %343) {onnx_node_name = "/model/layers.10/attn/v_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1149 = "onnx.Cast"(%1148) {onnx_node_name = "/model/layers.10/attn/v_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1150 = "onnx.Mul"(%y_scale_118, %342) {onnx_node_name = "/model/layers.10/attn/v_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1151 = "onnx.Mul"(%1149, %1150) {onnx_node_name = "/model/layers.10/attn/v_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1152 = "onnx.Reshape"(%1143, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.10/attn/q_norm/Reshape_1"} : (tensor<1x128x2048xf32>, tensor<3xi64>) -> tensor<1x2048x128xf32>
    %1153 = "onnx.Reshape"(%1147, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.10/attn/k_norm/Reshape_1"} : (tensor<1x128x1024xf32>, tensor<3xi64>) -> tensor<1x1024x128xf32>
    %1154 = "onnx.Custom"(%1152, %51) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.10/attn/q_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x2048x128xf32>, tensor<128xf32>) -> tensor<1x2048x128xf32>
    %1155 = "onnx.Custom"(%1153, %52) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.10/attn/k_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x1024x128xf32>, tensor<128xf32>) -> tensor<1x1024x128xf32>
    %1156 = "onnx.Reshape"(%1154, %5) {allowzero = 0 : si64, onnx_node_name = "/model/layers.10/attn/q_norm/Reshape_2"} : (tensor<1x2048x128xf32>, tensor<3xi64>) -> tensor<1x128x2048xf32>
    %1157 = "onnx.Reshape"(%1155, %4) {allowzero = 0 : si64, onnx_node_name = "/model/layers.10/attn/k_norm/Reshape_2"} : (tensor<1x1024x128xf32>, tensor<3xi64>) -> tensor<1x128x1024xf32>
    %1158 = "onnx.Custom"(%1156, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.10/attn/q_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x2048xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x2048xf32>
    %1159 = "onnx.Custom"(%1157, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.10/attn/k_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x1024xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x1024xf32>
    %1160:3 = "onnx.Custom"(%1158, %1159, %1151, %arg23, %arg24, %731, %0, %3, %3) {do_rotary = 0 : si64, domain_name = "com.microsoft", function_name = "GroupQueryAttention", kv_num_heads = 8 : si64, num_heads = 16 : si64, onnx_node_name = "/model/layers.10/attn/GroupQueryAttention", rotary_interleaved = 0 : si64, scale = 0.0883883461 : f32, softcap = 0.000000e+00 : f32} : (tensor<1x128x2048xf32>, tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1x8x1x128xf32>, tensor<1x8x1x128xf32>, tensor<1x1xi32>, tensor<i32>, none, none) -> (tensor<1x128x2048xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>)
    %y_120, %y_scale_121, %y_zero_point_122 = "onnx.DynamicQuantizeLinear"(%1160#0) {onnx_node_name = "/model/layers.10/attn/GroupQueryAttention/output_0_QuantizeLinear"} : (tensor<1x128x2048xf32>) -> (tensor<1x128x2048xui8>, tensor<f32>, tensor<ui8>)
    %1161 = "onnx.MatMulInteger"(%y_120, %347, %y_zero_point_122, %346) {onnx_node_name = "/model/layers.10/attn/o_proj/MatMul_quant"} : (tensor<1x128x2048xui8>, tensor<2048x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1162 = "onnx.Cast"(%1161) {onnx_node_name = "/model/layers.10/attn/o_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1163 = "onnx.Mul"(%y_scale_121, %345) {onnx_node_name = "/model/layers.10/attn/o_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1164 = "onnx.Mul"(%1162, %1163) {onnx_node_name = "/model/layers.10/attn/o_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1165:4 = "onnx.Custom"(%1139#3, %1164, %53) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.10/post_attention_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_123, %y_scale_124, %y_zero_point_125 = "onnx.DynamicQuantizeLinear"(%1165#0) {onnx_node_name = "/model/layers.10/post_attention_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %1166 = "onnx.MatMulInteger"(%y_123, %350, %y_zero_point_125, %349) {onnx_node_name = "/model/layers.10/mlp/gate_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %1167 = "onnx.Cast"(%1166) {onnx_node_name = "/model/layers.10/mlp/gate_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %1168 = "onnx.Mul"(%y_scale_124, %348) {onnx_node_name = "/model/layers.10/mlp/gate_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1169 = "onnx.Mul"(%1167, %1168) {onnx_node_name = "/model/layers.10/mlp/gate_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %1170 = "onnx.MatMulInteger"(%y_123, %353, %y_zero_point_125, %352) {onnx_node_name = "/model/layers.10/mlp/up_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %1171 = "onnx.Cast"(%1170) {onnx_node_name = "/model/layers.10/mlp/up_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %1172 = "onnx.Mul"(%y_scale_124, %351) {onnx_node_name = "/model/layers.10/mlp/up_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1173 = "onnx.Mul"(%1171, %1172) {onnx_node_name = "/model/layers.10/mlp/up_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %1174 = "onnx.Sigmoid"(%1169) {onnx_node_name = "/model/layers.10/mlp/act_fn/Sigmoid"} : (tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %1175 = "onnx.Mul"(%1169, %1174) {onnx_node_name = "/model/layers.10/mlp/act_fn/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %1176 = "onnx.Mul"(%1175, %1173) {onnx_node_name = "/model/layers.10/mlp/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %y_126, %y_scale_127, %y_zero_point_128 = "onnx.DynamicQuantizeLinear"(%1176) {onnx_node_name = "/model/layers.10/mlp/Mul/output_0_QuantizeLinear"} : (tensor<1x128x3072xf32>) -> (tensor<1x128x3072xui8>, tensor<f32>, tensor<ui8>)
    %1177 = "onnx.MatMulInteger"(%y_126, %356, %y_zero_point_128, %355) {onnx_node_name = "/model/layers.10/mlp/down_proj/MatMul_quant"} : (tensor<1x128x3072xui8>, tensor<3072x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1178 = "onnx.Cast"(%1177) {onnx_node_name = "/model/layers.10/mlp/down_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1179 = "onnx.Mul"(%y_scale_127, %354) {onnx_node_name = "/model/layers.10/mlp/down_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1180 = "onnx.Mul"(%1178, %1179) {onnx_node_name = "/model/layers.10/mlp/down_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1181:4 = "onnx.Custom"(%1165#3, %1180, %54) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.11/input_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_129, %y_scale_130, %y_zero_point_131 = "onnx.DynamicQuantizeLinear"(%1181#0) {onnx_node_name = "/model/layers.11/input_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %1182 = "onnx.MatMulInteger"(%y_129, %359, %y_zero_point_131, %358) {onnx_node_name = "/model/layers.11/attn/q_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x2048xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x2048xi32>
    %1183 = "onnx.Cast"(%1182) {onnx_node_name = "/model/layers.11/attn/q_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x2048xi32>) -> tensor<1x128x2048xf32>
    %1184 = "onnx.Mul"(%y_scale_130, %357) {onnx_node_name = "/model/layers.11/attn/q_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1185 = "onnx.Mul"(%1183, %1184) {onnx_node_name = "/model/layers.11/attn/q_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x2048xf32>, tensor<f32>) -> tensor<1x128x2048xf32>
    %1186 = "onnx.MatMulInteger"(%y_129, %362, %y_zero_point_131, %361) {onnx_node_name = "/model/layers.11/attn/k_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1187 = "onnx.Cast"(%1186) {onnx_node_name = "/model/layers.11/attn/k_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1188 = "onnx.Mul"(%y_scale_130, %360) {onnx_node_name = "/model/layers.11/attn/k_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1189 = "onnx.Mul"(%1187, %1188) {onnx_node_name = "/model/layers.11/attn/k_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1190 = "onnx.MatMulInteger"(%y_129, %365, %y_zero_point_131, %364) {onnx_node_name = "/model/layers.11/attn/v_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1191 = "onnx.Cast"(%1190) {onnx_node_name = "/model/layers.11/attn/v_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1192 = "onnx.Mul"(%y_scale_130, %363) {onnx_node_name = "/model/layers.11/attn/v_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1193 = "onnx.Mul"(%1191, %1192) {onnx_node_name = "/model/layers.11/attn/v_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1194 = "onnx.Reshape"(%1185, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.11/attn/q_norm/Reshape_1"} : (tensor<1x128x2048xf32>, tensor<3xi64>) -> tensor<1x2048x128xf32>
    %1195 = "onnx.Reshape"(%1189, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.11/attn/k_norm/Reshape_1"} : (tensor<1x128x1024xf32>, tensor<3xi64>) -> tensor<1x1024x128xf32>
    %1196 = "onnx.Custom"(%1194, %55) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.11/attn/q_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x2048x128xf32>, tensor<128xf32>) -> tensor<1x2048x128xf32>
    %1197 = "onnx.Custom"(%1195, %56) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.11/attn/k_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x1024x128xf32>, tensor<128xf32>) -> tensor<1x1024x128xf32>
    %1198 = "onnx.Reshape"(%1196, %5) {allowzero = 0 : si64, onnx_node_name = "/model/layers.11/attn/q_norm/Reshape_2"} : (tensor<1x2048x128xf32>, tensor<3xi64>) -> tensor<1x128x2048xf32>
    %1199 = "onnx.Reshape"(%1197, %4) {allowzero = 0 : si64, onnx_node_name = "/model/layers.11/attn/k_norm/Reshape_2"} : (tensor<1x1024x128xf32>, tensor<3xi64>) -> tensor<1x128x1024xf32>
    %1200 = "onnx.Custom"(%1198, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.11/attn/q_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x2048xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x2048xf32>
    %1201 = "onnx.Custom"(%1199, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.11/attn/k_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x1024xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x1024xf32>
    %1202:3 = "onnx.Custom"(%1200, %1201, %1193, %arg25, %arg26, %731, %0, %3, %3) {do_rotary = 0 : si64, domain_name = "com.microsoft", function_name = "GroupQueryAttention", kv_num_heads = 8 : si64, num_heads = 16 : si64, onnx_node_name = "/model/layers.11/attn/GroupQueryAttention", rotary_interleaved = 0 : si64, scale = 0.0883883461 : f32, softcap = 0.000000e+00 : f32} : (tensor<1x128x2048xf32>, tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1x8x1x128xf32>, tensor<1x8x1x128xf32>, tensor<1x1xi32>, tensor<i32>, none, none) -> (tensor<1x128x2048xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>)
    %y_132, %y_scale_133, %y_zero_point_134 = "onnx.DynamicQuantizeLinear"(%1202#0) {onnx_node_name = "/model/layers.11/attn/GroupQueryAttention/output_0_QuantizeLinear"} : (tensor<1x128x2048xf32>) -> (tensor<1x128x2048xui8>, tensor<f32>, tensor<ui8>)
    %1203 = "onnx.MatMulInteger"(%y_132, %368, %y_zero_point_134, %367) {onnx_node_name = "/model/layers.11/attn/o_proj/MatMul_quant"} : (tensor<1x128x2048xui8>, tensor<2048x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1204 = "onnx.Cast"(%1203) {onnx_node_name = "/model/layers.11/attn/o_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1205 = "onnx.Mul"(%y_scale_133, %366) {onnx_node_name = "/model/layers.11/attn/o_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1206 = "onnx.Mul"(%1204, %1205) {onnx_node_name = "/model/layers.11/attn/o_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1207:4 = "onnx.Custom"(%1181#3, %1206, %57) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.11/post_attention_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_135, %y_scale_136, %y_zero_point_137 = "onnx.DynamicQuantizeLinear"(%1207#0) {onnx_node_name = "/model/layers.11/post_attention_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %1208 = "onnx.MatMulInteger"(%y_135, %371, %y_zero_point_137, %370) {onnx_node_name = "/model/layers.11/mlp/gate_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %1209 = "onnx.Cast"(%1208) {onnx_node_name = "/model/layers.11/mlp/gate_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %1210 = "onnx.Mul"(%y_scale_136, %369) {onnx_node_name = "/model/layers.11/mlp/gate_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1211 = "onnx.Mul"(%1209, %1210) {onnx_node_name = "/model/layers.11/mlp/gate_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %1212 = "onnx.MatMulInteger"(%y_135, %374, %y_zero_point_137, %373) {onnx_node_name = "/model/layers.11/mlp/up_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %1213 = "onnx.Cast"(%1212) {onnx_node_name = "/model/layers.11/mlp/up_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %1214 = "onnx.Mul"(%y_scale_136, %372) {onnx_node_name = "/model/layers.11/mlp/up_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1215 = "onnx.Mul"(%1213, %1214) {onnx_node_name = "/model/layers.11/mlp/up_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %1216 = "onnx.Sigmoid"(%1211) {onnx_node_name = "/model/layers.11/mlp/act_fn/Sigmoid"} : (tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %1217 = "onnx.Mul"(%1211, %1216) {onnx_node_name = "/model/layers.11/mlp/act_fn/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %1218 = "onnx.Mul"(%1217, %1215) {onnx_node_name = "/model/layers.11/mlp/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %y_138, %y_scale_139, %y_zero_point_140 = "onnx.DynamicQuantizeLinear"(%1218) {onnx_node_name = "/model/layers.11/mlp/Mul/output_0_QuantizeLinear"} : (tensor<1x128x3072xf32>) -> (tensor<1x128x3072xui8>, tensor<f32>, tensor<ui8>)
    %1219 = "onnx.MatMulInteger"(%y_138, %377, %y_zero_point_140, %376) {onnx_node_name = "/model/layers.11/mlp/down_proj/MatMul_quant"} : (tensor<1x128x3072xui8>, tensor<3072x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1220 = "onnx.Cast"(%1219) {onnx_node_name = "/model/layers.11/mlp/down_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1221 = "onnx.Mul"(%y_scale_139, %375) {onnx_node_name = "/model/layers.11/mlp/down_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1222 = "onnx.Mul"(%1220, %1221) {onnx_node_name = "/model/layers.11/mlp/down_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1223:4 = "onnx.Custom"(%1207#3, %1222, %58) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.12/input_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_141, %y_scale_142, %y_zero_point_143 = "onnx.DynamicQuantizeLinear"(%1223#0) {onnx_node_name = "/model/layers.12/input_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %1224 = "onnx.MatMulInteger"(%y_141, %380, %y_zero_point_143, %379) {onnx_node_name = "/model/layers.12/attn/q_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x2048xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x2048xi32>
    %1225 = "onnx.Cast"(%1224) {onnx_node_name = "/model/layers.12/attn/q_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x2048xi32>) -> tensor<1x128x2048xf32>
    %1226 = "onnx.Mul"(%y_scale_142, %378) {onnx_node_name = "/model/layers.12/attn/q_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1227 = "onnx.Mul"(%1225, %1226) {onnx_node_name = "/model/layers.12/attn/q_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x2048xf32>, tensor<f32>) -> tensor<1x128x2048xf32>
    %1228 = "onnx.MatMulInteger"(%y_141, %383, %y_zero_point_143, %382) {onnx_node_name = "/model/layers.12/attn/k_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1229 = "onnx.Cast"(%1228) {onnx_node_name = "/model/layers.12/attn/k_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1230 = "onnx.Mul"(%y_scale_142, %381) {onnx_node_name = "/model/layers.12/attn/k_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1231 = "onnx.Mul"(%1229, %1230) {onnx_node_name = "/model/layers.12/attn/k_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1232 = "onnx.MatMulInteger"(%y_141, %386, %y_zero_point_143, %385) {onnx_node_name = "/model/layers.12/attn/v_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1233 = "onnx.Cast"(%1232) {onnx_node_name = "/model/layers.12/attn/v_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1234 = "onnx.Mul"(%y_scale_142, %384) {onnx_node_name = "/model/layers.12/attn/v_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1235 = "onnx.Mul"(%1233, %1234) {onnx_node_name = "/model/layers.12/attn/v_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1236 = "onnx.Reshape"(%1227, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.12/attn/q_norm/Reshape_1"} : (tensor<1x128x2048xf32>, tensor<3xi64>) -> tensor<1x2048x128xf32>
    %1237 = "onnx.Reshape"(%1231, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.12/attn/k_norm/Reshape_1"} : (tensor<1x128x1024xf32>, tensor<3xi64>) -> tensor<1x1024x128xf32>
    %1238 = "onnx.Custom"(%1236, %59) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.12/attn/q_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x2048x128xf32>, tensor<128xf32>) -> tensor<1x2048x128xf32>
    %1239 = "onnx.Custom"(%1237, %60) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.12/attn/k_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x1024x128xf32>, tensor<128xf32>) -> tensor<1x1024x128xf32>
    %1240 = "onnx.Reshape"(%1238, %5) {allowzero = 0 : si64, onnx_node_name = "/model/layers.12/attn/q_norm/Reshape_2"} : (tensor<1x2048x128xf32>, tensor<3xi64>) -> tensor<1x128x2048xf32>
    %1241 = "onnx.Reshape"(%1239, %4) {allowzero = 0 : si64, onnx_node_name = "/model/layers.12/attn/k_norm/Reshape_2"} : (tensor<1x1024x128xf32>, tensor<3xi64>) -> tensor<1x128x1024xf32>
    %1242 = "onnx.Custom"(%1240, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.12/attn/q_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x2048xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x2048xf32>
    %1243 = "onnx.Custom"(%1241, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.12/attn/k_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x1024xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x1024xf32>
    %1244:3 = "onnx.Custom"(%1242, %1243, %1235, %arg27, %arg28, %731, %0, %3, %3) {do_rotary = 0 : si64, domain_name = "com.microsoft", function_name = "GroupQueryAttention", kv_num_heads = 8 : si64, num_heads = 16 : si64, onnx_node_name = "/model/layers.12/attn/GroupQueryAttention", rotary_interleaved = 0 : si64, scale = 0.0883883461 : f32, softcap = 0.000000e+00 : f32} : (tensor<1x128x2048xf32>, tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1x8x1x128xf32>, tensor<1x8x1x128xf32>, tensor<1x1xi32>, tensor<i32>, none, none) -> (tensor<1x128x2048xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>)
    %y_144, %y_scale_145, %y_zero_point_146 = "onnx.DynamicQuantizeLinear"(%1244#0) {onnx_node_name = "/model/layers.12/attn/GroupQueryAttention/output_0_QuantizeLinear"} : (tensor<1x128x2048xf32>) -> (tensor<1x128x2048xui8>, tensor<f32>, tensor<ui8>)
    %1245 = "onnx.MatMulInteger"(%y_144, %389, %y_zero_point_146, %388) {onnx_node_name = "/model/layers.12/attn/o_proj/MatMul_quant"} : (tensor<1x128x2048xui8>, tensor<2048x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1246 = "onnx.Cast"(%1245) {onnx_node_name = "/model/layers.12/attn/o_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1247 = "onnx.Mul"(%y_scale_145, %387) {onnx_node_name = "/model/layers.12/attn/o_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1248 = "onnx.Mul"(%1246, %1247) {onnx_node_name = "/model/layers.12/attn/o_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1249:4 = "onnx.Custom"(%1223#3, %1248, %61) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.12/post_attention_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_147, %y_scale_148, %y_zero_point_149 = "onnx.DynamicQuantizeLinear"(%1249#0) {onnx_node_name = "/model/layers.12/post_attention_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %1250 = "onnx.MatMulInteger"(%y_147, %392, %y_zero_point_149, %391) {onnx_node_name = "/model/layers.12/mlp/gate_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %1251 = "onnx.Cast"(%1250) {onnx_node_name = "/model/layers.12/mlp/gate_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %1252 = "onnx.Mul"(%y_scale_148, %390) {onnx_node_name = "/model/layers.12/mlp/gate_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1253 = "onnx.Mul"(%1251, %1252) {onnx_node_name = "/model/layers.12/mlp/gate_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %1254 = "onnx.MatMulInteger"(%y_147, %395, %y_zero_point_149, %394) {onnx_node_name = "/model/layers.12/mlp/up_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %1255 = "onnx.Cast"(%1254) {onnx_node_name = "/model/layers.12/mlp/up_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %1256 = "onnx.Mul"(%y_scale_148, %393) {onnx_node_name = "/model/layers.12/mlp/up_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1257 = "onnx.Mul"(%1255, %1256) {onnx_node_name = "/model/layers.12/mlp/up_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %1258 = "onnx.Sigmoid"(%1253) {onnx_node_name = "/model/layers.12/mlp/act_fn/Sigmoid"} : (tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %1259 = "onnx.Mul"(%1253, %1258) {onnx_node_name = "/model/layers.12/mlp/act_fn/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %1260 = "onnx.Mul"(%1259, %1257) {onnx_node_name = "/model/layers.12/mlp/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %y_150, %y_scale_151, %y_zero_point_152 = "onnx.DynamicQuantizeLinear"(%1260) {onnx_node_name = "/model/layers.12/mlp/Mul/output_0_QuantizeLinear"} : (tensor<1x128x3072xf32>) -> (tensor<1x128x3072xui8>, tensor<f32>, tensor<ui8>)
    %1261 = "onnx.MatMulInteger"(%y_150, %398, %y_zero_point_152, %397) {onnx_node_name = "/model/layers.12/mlp/down_proj/MatMul_quant"} : (tensor<1x128x3072xui8>, tensor<3072x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1262 = "onnx.Cast"(%1261) {onnx_node_name = "/model/layers.12/mlp/down_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1263 = "onnx.Mul"(%y_scale_151, %396) {onnx_node_name = "/model/layers.12/mlp/down_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1264 = "onnx.Mul"(%1262, %1263) {onnx_node_name = "/model/layers.12/mlp/down_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1265:4 = "onnx.Custom"(%1249#3, %1264, %62) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.13/input_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_153, %y_scale_154, %y_zero_point_155 = "onnx.DynamicQuantizeLinear"(%1265#0) {onnx_node_name = "/model/layers.13/input_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %1266 = "onnx.MatMulInteger"(%y_153, %401, %y_zero_point_155, %400) {onnx_node_name = "/model/layers.13/attn/q_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x2048xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x2048xi32>
    %1267 = "onnx.Cast"(%1266) {onnx_node_name = "/model/layers.13/attn/q_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x2048xi32>) -> tensor<1x128x2048xf32>
    %1268 = "onnx.Mul"(%y_scale_154, %399) {onnx_node_name = "/model/layers.13/attn/q_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1269 = "onnx.Mul"(%1267, %1268) {onnx_node_name = "/model/layers.13/attn/q_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x2048xf32>, tensor<f32>) -> tensor<1x128x2048xf32>
    %1270 = "onnx.MatMulInteger"(%y_153, %404, %y_zero_point_155, %403) {onnx_node_name = "/model/layers.13/attn/k_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1271 = "onnx.Cast"(%1270) {onnx_node_name = "/model/layers.13/attn/k_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1272 = "onnx.Mul"(%y_scale_154, %402) {onnx_node_name = "/model/layers.13/attn/k_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1273 = "onnx.Mul"(%1271, %1272) {onnx_node_name = "/model/layers.13/attn/k_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1274 = "onnx.MatMulInteger"(%y_153, %407, %y_zero_point_155, %406) {onnx_node_name = "/model/layers.13/attn/v_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1275 = "onnx.Cast"(%1274) {onnx_node_name = "/model/layers.13/attn/v_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1276 = "onnx.Mul"(%y_scale_154, %405) {onnx_node_name = "/model/layers.13/attn/v_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1277 = "onnx.Mul"(%1275, %1276) {onnx_node_name = "/model/layers.13/attn/v_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1278 = "onnx.Reshape"(%1269, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.13/attn/q_norm/Reshape_1"} : (tensor<1x128x2048xf32>, tensor<3xi64>) -> tensor<1x2048x128xf32>
    %1279 = "onnx.Reshape"(%1273, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.13/attn/k_norm/Reshape_1"} : (tensor<1x128x1024xf32>, tensor<3xi64>) -> tensor<1x1024x128xf32>
    %1280 = "onnx.Custom"(%1278, %63) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.13/attn/q_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x2048x128xf32>, tensor<128xf32>) -> tensor<1x2048x128xf32>
    %1281 = "onnx.Custom"(%1279, %64) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.13/attn/k_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x1024x128xf32>, tensor<128xf32>) -> tensor<1x1024x128xf32>
    %1282 = "onnx.Reshape"(%1280, %5) {allowzero = 0 : si64, onnx_node_name = "/model/layers.13/attn/q_norm/Reshape_2"} : (tensor<1x2048x128xf32>, tensor<3xi64>) -> tensor<1x128x2048xf32>
    %1283 = "onnx.Reshape"(%1281, %4) {allowzero = 0 : si64, onnx_node_name = "/model/layers.13/attn/k_norm/Reshape_2"} : (tensor<1x1024x128xf32>, tensor<3xi64>) -> tensor<1x128x1024xf32>
    %1284 = "onnx.Custom"(%1282, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.13/attn/q_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x2048xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x2048xf32>
    %1285 = "onnx.Custom"(%1283, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.13/attn/k_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x1024xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x1024xf32>
    %1286:3 = "onnx.Custom"(%1284, %1285, %1277, %arg29, %arg30, %731, %0, %3, %3) {do_rotary = 0 : si64, domain_name = "com.microsoft", function_name = "GroupQueryAttention", kv_num_heads = 8 : si64, num_heads = 16 : si64, onnx_node_name = "/model/layers.13/attn/GroupQueryAttention", rotary_interleaved = 0 : si64, scale = 0.0883883461 : f32, softcap = 0.000000e+00 : f32} : (tensor<1x128x2048xf32>, tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1x8x1x128xf32>, tensor<1x8x1x128xf32>, tensor<1x1xi32>, tensor<i32>, none, none) -> (tensor<1x128x2048xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>)
    %y_156, %y_scale_157, %y_zero_point_158 = "onnx.DynamicQuantizeLinear"(%1286#0) {onnx_node_name = "/model/layers.13/attn/GroupQueryAttention/output_0_QuantizeLinear"} : (tensor<1x128x2048xf32>) -> (tensor<1x128x2048xui8>, tensor<f32>, tensor<ui8>)
    %1287 = "onnx.MatMulInteger"(%y_156, %410, %y_zero_point_158, %409) {onnx_node_name = "/model/layers.13/attn/o_proj/MatMul_quant"} : (tensor<1x128x2048xui8>, tensor<2048x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1288 = "onnx.Cast"(%1287) {onnx_node_name = "/model/layers.13/attn/o_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1289 = "onnx.Mul"(%y_scale_157, %408) {onnx_node_name = "/model/layers.13/attn/o_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1290 = "onnx.Mul"(%1288, %1289) {onnx_node_name = "/model/layers.13/attn/o_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1291:4 = "onnx.Custom"(%1265#3, %1290, %65) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.13/post_attention_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_159, %y_scale_160, %y_zero_point_161 = "onnx.DynamicQuantizeLinear"(%1291#0) {onnx_node_name = "/model/layers.13/post_attention_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %1292 = "onnx.MatMulInteger"(%y_159, %413, %y_zero_point_161, %412) {onnx_node_name = "/model/layers.13/mlp/gate_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %1293 = "onnx.Cast"(%1292) {onnx_node_name = "/model/layers.13/mlp/gate_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %1294 = "onnx.Mul"(%y_scale_160, %411) {onnx_node_name = "/model/layers.13/mlp/gate_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1295 = "onnx.Mul"(%1293, %1294) {onnx_node_name = "/model/layers.13/mlp/gate_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %1296 = "onnx.MatMulInteger"(%y_159, %416, %y_zero_point_161, %415) {onnx_node_name = "/model/layers.13/mlp/up_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %1297 = "onnx.Cast"(%1296) {onnx_node_name = "/model/layers.13/mlp/up_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %1298 = "onnx.Mul"(%y_scale_160, %414) {onnx_node_name = "/model/layers.13/mlp/up_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1299 = "onnx.Mul"(%1297, %1298) {onnx_node_name = "/model/layers.13/mlp/up_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %1300 = "onnx.Sigmoid"(%1295) {onnx_node_name = "/model/layers.13/mlp/act_fn/Sigmoid"} : (tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %1301 = "onnx.Mul"(%1295, %1300) {onnx_node_name = "/model/layers.13/mlp/act_fn/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %1302 = "onnx.Mul"(%1301, %1299) {onnx_node_name = "/model/layers.13/mlp/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %y_162, %y_scale_163, %y_zero_point_164 = "onnx.DynamicQuantizeLinear"(%1302) {onnx_node_name = "/model/layers.13/mlp/Mul/output_0_QuantizeLinear"} : (tensor<1x128x3072xf32>) -> (tensor<1x128x3072xui8>, tensor<f32>, tensor<ui8>)
    %1303 = "onnx.MatMulInteger"(%y_162, %419, %y_zero_point_164, %418) {onnx_node_name = "/model/layers.13/mlp/down_proj/MatMul_quant"} : (tensor<1x128x3072xui8>, tensor<3072x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1304 = "onnx.Cast"(%1303) {onnx_node_name = "/model/layers.13/mlp/down_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1305 = "onnx.Mul"(%y_scale_163, %417) {onnx_node_name = "/model/layers.13/mlp/down_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1306 = "onnx.Mul"(%1304, %1305) {onnx_node_name = "/model/layers.13/mlp/down_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1307:4 = "onnx.Custom"(%1291#3, %1306, %66) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.14/input_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_165, %y_scale_166, %y_zero_point_167 = "onnx.DynamicQuantizeLinear"(%1307#0) {onnx_node_name = "/model/layers.14/input_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %1308 = "onnx.MatMulInteger"(%y_165, %422, %y_zero_point_167, %421) {onnx_node_name = "/model/layers.14/attn/q_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x2048xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x2048xi32>
    %1309 = "onnx.Cast"(%1308) {onnx_node_name = "/model/layers.14/attn/q_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x2048xi32>) -> tensor<1x128x2048xf32>
    %1310 = "onnx.Mul"(%y_scale_166, %420) {onnx_node_name = "/model/layers.14/attn/q_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1311 = "onnx.Mul"(%1309, %1310) {onnx_node_name = "/model/layers.14/attn/q_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x2048xf32>, tensor<f32>) -> tensor<1x128x2048xf32>
    %1312 = "onnx.MatMulInteger"(%y_165, %425, %y_zero_point_167, %424) {onnx_node_name = "/model/layers.14/attn/k_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1313 = "onnx.Cast"(%1312) {onnx_node_name = "/model/layers.14/attn/k_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1314 = "onnx.Mul"(%y_scale_166, %423) {onnx_node_name = "/model/layers.14/attn/k_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1315 = "onnx.Mul"(%1313, %1314) {onnx_node_name = "/model/layers.14/attn/k_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1316 = "onnx.MatMulInteger"(%y_165, %428, %y_zero_point_167, %427) {onnx_node_name = "/model/layers.14/attn/v_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1317 = "onnx.Cast"(%1316) {onnx_node_name = "/model/layers.14/attn/v_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1318 = "onnx.Mul"(%y_scale_166, %426) {onnx_node_name = "/model/layers.14/attn/v_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1319 = "onnx.Mul"(%1317, %1318) {onnx_node_name = "/model/layers.14/attn/v_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1320 = "onnx.Reshape"(%1311, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.14/attn/q_norm/Reshape_1"} : (tensor<1x128x2048xf32>, tensor<3xi64>) -> tensor<1x2048x128xf32>
    %1321 = "onnx.Reshape"(%1315, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.14/attn/k_norm/Reshape_1"} : (tensor<1x128x1024xf32>, tensor<3xi64>) -> tensor<1x1024x128xf32>
    %1322 = "onnx.Custom"(%1320, %67) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.14/attn/q_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x2048x128xf32>, tensor<128xf32>) -> tensor<1x2048x128xf32>
    %1323 = "onnx.Custom"(%1321, %68) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.14/attn/k_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x1024x128xf32>, tensor<128xf32>) -> tensor<1x1024x128xf32>
    %1324 = "onnx.Reshape"(%1322, %5) {allowzero = 0 : si64, onnx_node_name = "/model/layers.14/attn/q_norm/Reshape_2"} : (tensor<1x2048x128xf32>, tensor<3xi64>) -> tensor<1x128x2048xf32>
    %1325 = "onnx.Reshape"(%1323, %4) {allowzero = 0 : si64, onnx_node_name = "/model/layers.14/attn/k_norm/Reshape_2"} : (tensor<1x1024x128xf32>, tensor<3xi64>) -> tensor<1x128x1024xf32>
    %1326 = "onnx.Custom"(%1324, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.14/attn/q_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x2048xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x2048xf32>
    %1327 = "onnx.Custom"(%1325, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.14/attn/k_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x1024xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x1024xf32>
    %1328:3 = "onnx.Custom"(%1326, %1327, %1319, %arg31, %arg32, %731, %0, %3, %3) {do_rotary = 0 : si64, domain_name = "com.microsoft", function_name = "GroupQueryAttention", kv_num_heads = 8 : si64, num_heads = 16 : si64, onnx_node_name = "/model/layers.14/attn/GroupQueryAttention", rotary_interleaved = 0 : si64, scale = 0.0883883461 : f32, softcap = 0.000000e+00 : f32} : (tensor<1x128x2048xf32>, tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1x8x1x128xf32>, tensor<1x8x1x128xf32>, tensor<1x1xi32>, tensor<i32>, none, none) -> (tensor<1x128x2048xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>)
    %y_168, %y_scale_169, %y_zero_point_170 = "onnx.DynamicQuantizeLinear"(%1328#0) {onnx_node_name = "/model/layers.14/attn/GroupQueryAttention/output_0_QuantizeLinear"} : (tensor<1x128x2048xf32>) -> (tensor<1x128x2048xui8>, tensor<f32>, tensor<ui8>)
    %1329 = "onnx.MatMulInteger"(%y_168, %431, %y_zero_point_170, %430) {onnx_node_name = "/model/layers.14/attn/o_proj/MatMul_quant"} : (tensor<1x128x2048xui8>, tensor<2048x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1330 = "onnx.Cast"(%1329) {onnx_node_name = "/model/layers.14/attn/o_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1331 = "onnx.Mul"(%y_scale_169, %429) {onnx_node_name = "/model/layers.14/attn/o_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1332 = "onnx.Mul"(%1330, %1331) {onnx_node_name = "/model/layers.14/attn/o_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1333:4 = "onnx.Custom"(%1307#3, %1332, %69) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.14/post_attention_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_171, %y_scale_172, %y_zero_point_173 = "onnx.DynamicQuantizeLinear"(%1333#0) {onnx_node_name = "/model/layers.14/post_attention_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %1334 = "onnx.MatMulInteger"(%y_171, %434, %y_zero_point_173, %433) {onnx_node_name = "/model/layers.14/mlp/gate_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %1335 = "onnx.Cast"(%1334) {onnx_node_name = "/model/layers.14/mlp/gate_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %1336 = "onnx.Mul"(%y_scale_172, %432) {onnx_node_name = "/model/layers.14/mlp/gate_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1337 = "onnx.Mul"(%1335, %1336) {onnx_node_name = "/model/layers.14/mlp/gate_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %1338 = "onnx.MatMulInteger"(%y_171, %437, %y_zero_point_173, %436) {onnx_node_name = "/model/layers.14/mlp/up_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %1339 = "onnx.Cast"(%1338) {onnx_node_name = "/model/layers.14/mlp/up_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %1340 = "onnx.Mul"(%y_scale_172, %435) {onnx_node_name = "/model/layers.14/mlp/up_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1341 = "onnx.Mul"(%1339, %1340) {onnx_node_name = "/model/layers.14/mlp/up_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %1342 = "onnx.Sigmoid"(%1337) {onnx_node_name = "/model/layers.14/mlp/act_fn/Sigmoid"} : (tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %1343 = "onnx.Mul"(%1337, %1342) {onnx_node_name = "/model/layers.14/mlp/act_fn/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %1344 = "onnx.Mul"(%1343, %1341) {onnx_node_name = "/model/layers.14/mlp/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %y_174, %y_scale_175, %y_zero_point_176 = "onnx.DynamicQuantizeLinear"(%1344) {onnx_node_name = "/model/layers.14/mlp/Mul/output_0_QuantizeLinear"} : (tensor<1x128x3072xf32>) -> (tensor<1x128x3072xui8>, tensor<f32>, tensor<ui8>)
    %1345 = "onnx.MatMulInteger"(%y_174, %440, %y_zero_point_176, %439) {onnx_node_name = "/model/layers.14/mlp/down_proj/MatMul_quant"} : (tensor<1x128x3072xui8>, tensor<3072x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1346 = "onnx.Cast"(%1345) {onnx_node_name = "/model/layers.14/mlp/down_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1347 = "onnx.Mul"(%y_scale_175, %438) {onnx_node_name = "/model/layers.14/mlp/down_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1348 = "onnx.Mul"(%1346, %1347) {onnx_node_name = "/model/layers.14/mlp/down_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1349:4 = "onnx.Custom"(%1333#3, %1348, %70) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.15/input_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_177, %y_scale_178, %y_zero_point_179 = "onnx.DynamicQuantizeLinear"(%1349#0) {onnx_node_name = "/model/layers.15/input_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %1350 = "onnx.MatMulInteger"(%y_177, %443, %y_zero_point_179, %442) {onnx_node_name = "/model/layers.15/attn/q_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x2048xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x2048xi32>
    %1351 = "onnx.Cast"(%1350) {onnx_node_name = "/model/layers.15/attn/q_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x2048xi32>) -> tensor<1x128x2048xf32>
    %1352 = "onnx.Mul"(%y_scale_178, %441) {onnx_node_name = "/model/layers.15/attn/q_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1353 = "onnx.Mul"(%1351, %1352) {onnx_node_name = "/model/layers.15/attn/q_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x2048xf32>, tensor<f32>) -> tensor<1x128x2048xf32>
    %1354 = "onnx.MatMulInteger"(%y_177, %446, %y_zero_point_179, %445) {onnx_node_name = "/model/layers.15/attn/k_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1355 = "onnx.Cast"(%1354) {onnx_node_name = "/model/layers.15/attn/k_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1356 = "onnx.Mul"(%y_scale_178, %444) {onnx_node_name = "/model/layers.15/attn/k_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1357 = "onnx.Mul"(%1355, %1356) {onnx_node_name = "/model/layers.15/attn/k_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1358 = "onnx.MatMulInteger"(%y_177, %449, %y_zero_point_179, %448) {onnx_node_name = "/model/layers.15/attn/v_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1359 = "onnx.Cast"(%1358) {onnx_node_name = "/model/layers.15/attn/v_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1360 = "onnx.Mul"(%y_scale_178, %447) {onnx_node_name = "/model/layers.15/attn/v_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1361 = "onnx.Mul"(%1359, %1360) {onnx_node_name = "/model/layers.15/attn/v_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1362 = "onnx.Reshape"(%1353, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.15/attn/q_norm/Reshape_1"} : (tensor<1x128x2048xf32>, tensor<3xi64>) -> tensor<1x2048x128xf32>
    %1363 = "onnx.Reshape"(%1357, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.15/attn/k_norm/Reshape_1"} : (tensor<1x128x1024xf32>, tensor<3xi64>) -> tensor<1x1024x128xf32>
    %1364 = "onnx.Custom"(%1362, %71) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.15/attn/q_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x2048x128xf32>, tensor<128xf32>) -> tensor<1x2048x128xf32>
    %1365 = "onnx.Custom"(%1363, %72) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.15/attn/k_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x1024x128xf32>, tensor<128xf32>) -> tensor<1x1024x128xf32>
    %1366 = "onnx.Reshape"(%1364, %5) {allowzero = 0 : si64, onnx_node_name = "/model/layers.15/attn/q_norm/Reshape_2"} : (tensor<1x2048x128xf32>, tensor<3xi64>) -> tensor<1x128x2048xf32>
    %1367 = "onnx.Reshape"(%1365, %4) {allowzero = 0 : si64, onnx_node_name = "/model/layers.15/attn/k_norm/Reshape_2"} : (tensor<1x1024x128xf32>, tensor<3xi64>) -> tensor<1x128x1024xf32>
    %1368 = "onnx.Custom"(%1366, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.15/attn/q_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x2048xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x2048xf32>
    %1369 = "onnx.Custom"(%1367, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.15/attn/k_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x1024xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x1024xf32>
    %1370:3 = "onnx.Custom"(%1368, %1369, %1361, %arg33, %arg34, %731, %0, %3, %3) {do_rotary = 0 : si64, domain_name = "com.microsoft", function_name = "GroupQueryAttention", kv_num_heads = 8 : si64, num_heads = 16 : si64, onnx_node_name = "/model/layers.15/attn/GroupQueryAttention", rotary_interleaved = 0 : si64, scale = 0.0883883461 : f32, softcap = 0.000000e+00 : f32} : (tensor<1x128x2048xf32>, tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1x8x1x128xf32>, tensor<1x8x1x128xf32>, tensor<1x1xi32>, tensor<i32>, none, none) -> (tensor<1x128x2048xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>)
    %y_180, %y_scale_181, %y_zero_point_182 = "onnx.DynamicQuantizeLinear"(%1370#0) {onnx_node_name = "/model/layers.15/attn/GroupQueryAttention/output_0_QuantizeLinear"} : (tensor<1x128x2048xf32>) -> (tensor<1x128x2048xui8>, tensor<f32>, tensor<ui8>)
    %1371 = "onnx.MatMulInteger"(%y_180, %452, %y_zero_point_182, %451) {onnx_node_name = "/model/layers.15/attn/o_proj/MatMul_quant"} : (tensor<1x128x2048xui8>, tensor<2048x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1372 = "onnx.Cast"(%1371) {onnx_node_name = "/model/layers.15/attn/o_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1373 = "onnx.Mul"(%y_scale_181, %450) {onnx_node_name = "/model/layers.15/attn/o_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1374 = "onnx.Mul"(%1372, %1373) {onnx_node_name = "/model/layers.15/attn/o_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1375:4 = "onnx.Custom"(%1349#3, %1374, %73) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.15/post_attention_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_183, %y_scale_184, %y_zero_point_185 = "onnx.DynamicQuantizeLinear"(%1375#0) {onnx_node_name = "/model/layers.15/post_attention_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %1376 = "onnx.MatMulInteger"(%y_183, %455, %y_zero_point_185, %454) {onnx_node_name = "/model/layers.15/mlp/gate_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %1377 = "onnx.Cast"(%1376) {onnx_node_name = "/model/layers.15/mlp/gate_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %1378 = "onnx.Mul"(%y_scale_184, %453) {onnx_node_name = "/model/layers.15/mlp/gate_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1379 = "onnx.Mul"(%1377, %1378) {onnx_node_name = "/model/layers.15/mlp/gate_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %1380 = "onnx.MatMulInteger"(%y_183, %458, %y_zero_point_185, %457) {onnx_node_name = "/model/layers.15/mlp/up_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %1381 = "onnx.Cast"(%1380) {onnx_node_name = "/model/layers.15/mlp/up_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %1382 = "onnx.Mul"(%y_scale_184, %456) {onnx_node_name = "/model/layers.15/mlp/up_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1383 = "onnx.Mul"(%1381, %1382) {onnx_node_name = "/model/layers.15/mlp/up_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %1384 = "onnx.Sigmoid"(%1379) {onnx_node_name = "/model/layers.15/mlp/act_fn/Sigmoid"} : (tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %1385 = "onnx.Mul"(%1379, %1384) {onnx_node_name = "/model/layers.15/mlp/act_fn/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %1386 = "onnx.Mul"(%1385, %1383) {onnx_node_name = "/model/layers.15/mlp/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %y_186, %y_scale_187, %y_zero_point_188 = "onnx.DynamicQuantizeLinear"(%1386) {onnx_node_name = "/model/layers.15/mlp/Mul/output_0_QuantizeLinear"} : (tensor<1x128x3072xf32>) -> (tensor<1x128x3072xui8>, tensor<f32>, tensor<ui8>)
    %1387 = "onnx.MatMulInteger"(%y_186, %461, %y_zero_point_188, %460) {onnx_node_name = "/model/layers.15/mlp/down_proj/MatMul_quant"} : (tensor<1x128x3072xui8>, tensor<3072x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1388 = "onnx.Cast"(%1387) {onnx_node_name = "/model/layers.15/mlp/down_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1389 = "onnx.Mul"(%y_scale_187, %459) {onnx_node_name = "/model/layers.15/mlp/down_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1390 = "onnx.Mul"(%1388, %1389) {onnx_node_name = "/model/layers.15/mlp/down_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1391:4 = "onnx.Custom"(%1375#3, %1390, %74) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.16/input_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_189, %y_scale_190, %y_zero_point_191 = "onnx.DynamicQuantizeLinear"(%1391#0) {onnx_node_name = "/model/layers.16/input_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %1392 = "onnx.MatMulInteger"(%y_189, %464, %y_zero_point_191, %463) {onnx_node_name = "/model/layers.16/attn/q_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x2048xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x2048xi32>
    %1393 = "onnx.Cast"(%1392) {onnx_node_name = "/model/layers.16/attn/q_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x2048xi32>) -> tensor<1x128x2048xf32>
    %1394 = "onnx.Mul"(%y_scale_190, %462) {onnx_node_name = "/model/layers.16/attn/q_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1395 = "onnx.Mul"(%1393, %1394) {onnx_node_name = "/model/layers.16/attn/q_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x2048xf32>, tensor<f32>) -> tensor<1x128x2048xf32>
    %1396 = "onnx.MatMulInteger"(%y_189, %467, %y_zero_point_191, %466) {onnx_node_name = "/model/layers.16/attn/k_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1397 = "onnx.Cast"(%1396) {onnx_node_name = "/model/layers.16/attn/k_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1398 = "onnx.Mul"(%y_scale_190, %465) {onnx_node_name = "/model/layers.16/attn/k_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1399 = "onnx.Mul"(%1397, %1398) {onnx_node_name = "/model/layers.16/attn/k_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1400 = "onnx.MatMulInteger"(%y_189, %470, %y_zero_point_191, %469) {onnx_node_name = "/model/layers.16/attn/v_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1401 = "onnx.Cast"(%1400) {onnx_node_name = "/model/layers.16/attn/v_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1402 = "onnx.Mul"(%y_scale_190, %468) {onnx_node_name = "/model/layers.16/attn/v_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1403 = "onnx.Mul"(%1401, %1402) {onnx_node_name = "/model/layers.16/attn/v_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1404 = "onnx.Reshape"(%1395, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.16/attn/q_norm/Reshape_1"} : (tensor<1x128x2048xf32>, tensor<3xi64>) -> tensor<1x2048x128xf32>
    %1405 = "onnx.Reshape"(%1399, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.16/attn/k_norm/Reshape_1"} : (tensor<1x128x1024xf32>, tensor<3xi64>) -> tensor<1x1024x128xf32>
    %1406 = "onnx.Custom"(%1404, %75) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.16/attn/q_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x2048x128xf32>, tensor<128xf32>) -> tensor<1x2048x128xf32>
    %1407 = "onnx.Custom"(%1405, %76) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.16/attn/k_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x1024x128xf32>, tensor<128xf32>) -> tensor<1x1024x128xf32>
    %1408 = "onnx.Reshape"(%1406, %5) {allowzero = 0 : si64, onnx_node_name = "/model/layers.16/attn/q_norm/Reshape_2"} : (tensor<1x2048x128xf32>, tensor<3xi64>) -> tensor<1x128x2048xf32>
    %1409 = "onnx.Reshape"(%1407, %4) {allowzero = 0 : si64, onnx_node_name = "/model/layers.16/attn/k_norm/Reshape_2"} : (tensor<1x1024x128xf32>, tensor<3xi64>) -> tensor<1x128x1024xf32>
    %1410 = "onnx.Custom"(%1408, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.16/attn/q_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x2048xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x2048xf32>
    %1411 = "onnx.Custom"(%1409, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.16/attn/k_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x1024xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x1024xf32>
    %1412:3 = "onnx.Custom"(%1410, %1411, %1403, %arg35, %arg36, %731, %0, %3, %3) {do_rotary = 0 : si64, domain_name = "com.microsoft", function_name = "GroupQueryAttention", kv_num_heads = 8 : si64, num_heads = 16 : si64, onnx_node_name = "/model/layers.16/attn/GroupQueryAttention", rotary_interleaved = 0 : si64, scale = 0.0883883461 : f32, softcap = 0.000000e+00 : f32} : (tensor<1x128x2048xf32>, tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1x8x1x128xf32>, tensor<1x8x1x128xf32>, tensor<1x1xi32>, tensor<i32>, none, none) -> (tensor<1x128x2048xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>)
    %y_192, %y_scale_193, %y_zero_point_194 = "onnx.DynamicQuantizeLinear"(%1412#0) {onnx_node_name = "/model/layers.16/attn/GroupQueryAttention/output_0_QuantizeLinear"} : (tensor<1x128x2048xf32>) -> (tensor<1x128x2048xui8>, tensor<f32>, tensor<ui8>)
    %1413 = "onnx.MatMulInteger"(%y_192, %473, %y_zero_point_194, %472) {onnx_node_name = "/model/layers.16/attn/o_proj/MatMul_quant"} : (tensor<1x128x2048xui8>, tensor<2048x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1414 = "onnx.Cast"(%1413) {onnx_node_name = "/model/layers.16/attn/o_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1415 = "onnx.Mul"(%y_scale_193, %471) {onnx_node_name = "/model/layers.16/attn/o_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1416 = "onnx.Mul"(%1414, %1415) {onnx_node_name = "/model/layers.16/attn/o_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1417:4 = "onnx.Custom"(%1391#3, %1416, %77) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.16/post_attention_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_195, %y_scale_196, %y_zero_point_197 = "onnx.DynamicQuantizeLinear"(%1417#0) {onnx_node_name = "/model/layers.16/post_attention_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %1418 = "onnx.MatMulInteger"(%y_195, %476, %y_zero_point_197, %475) {onnx_node_name = "/model/layers.16/mlp/gate_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %1419 = "onnx.Cast"(%1418) {onnx_node_name = "/model/layers.16/mlp/gate_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %1420 = "onnx.Mul"(%y_scale_196, %474) {onnx_node_name = "/model/layers.16/mlp/gate_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1421 = "onnx.Mul"(%1419, %1420) {onnx_node_name = "/model/layers.16/mlp/gate_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %1422 = "onnx.MatMulInteger"(%y_195, %479, %y_zero_point_197, %478) {onnx_node_name = "/model/layers.16/mlp/up_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %1423 = "onnx.Cast"(%1422) {onnx_node_name = "/model/layers.16/mlp/up_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %1424 = "onnx.Mul"(%y_scale_196, %477) {onnx_node_name = "/model/layers.16/mlp/up_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1425 = "onnx.Mul"(%1423, %1424) {onnx_node_name = "/model/layers.16/mlp/up_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %1426 = "onnx.Sigmoid"(%1421) {onnx_node_name = "/model/layers.16/mlp/act_fn/Sigmoid"} : (tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %1427 = "onnx.Mul"(%1421, %1426) {onnx_node_name = "/model/layers.16/mlp/act_fn/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %1428 = "onnx.Mul"(%1427, %1425) {onnx_node_name = "/model/layers.16/mlp/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %y_198, %y_scale_199, %y_zero_point_200 = "onnx.DynamicQuantizeLinear"(%1428) {onnx_node_name = "/model/layers.16/mlp/Mul/output_0_QuantizeLinear"} : (tensor<1x128x3072xf32>) -> (tensor<1x128x3072xui8>, tensor<f32>, tensor<ui8>)
    %1429 = "onnx.MatMulInteger"(%y_198, %482, %y_zero_point_200, %481) {onnx_node_name = "/model/layers.16/mlp/down_proj/MatMul_quant"} : (tensor<1x128x3072xui8>, tensor<3072x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1430 = "onnx.Cast"(%1429) {onnx_node_name = "/model/layers.16/mlp/down_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1431 = "onnx.Mul"(%y_scale_199, %480) {onnx_node_name = "/model/layers.16/mlp/down_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1432 = "onnx.Mul"(%1430, %1431) {onnx_node_name = "/model/layers.16/mlp/down_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1433:4 = "onnx.Custom"(%1417#3, %1432, %78) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.17/input_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_201, %y_scale_202, %y_zero_point_203 = "onnx.DynamicQuantizeLinear"(%1433#0) {onnx_node_name = "/model/layers.17/input_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %1434 = "onnx.MatMulInteger"(%y_201, %485, %y_zero_point_203, %484) {onnx_node_name = "/model/layers.17/attn/q_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x2048xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x2048xi32>
    %1435 = "onnx.Cast"(%1434) {onnx_node_name = "/model/layers.17/attn/q_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x2048xi32>) -> tensor<1x128x2048xf32>
    %1436 = "onnx.Mul"(%y_scale_202, %483) {onnx_node_name = "/model/layers.17/attn/q_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1437 = "onnx.Mul"(%1435, %1436) {onnx_node_name = "/model/layers.17/attn/q_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x2048xf32>, tensor<f32>) -> tensor<1x128x2048xf32>
    %1438 = "onnx.MatMulInteger"(%y_201, %488, %y_zero_point_203, %487) {onnx_node_name = "/model/layers.17/attn/k_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1439 = "onnx.Cast"(%1438) {onnx_node_name = "/model/layers.17/attn/k_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1440 = "onnx.Mul"(%y_scale_202, %486) {onnx_node_name = "/model/layers.17/attn/k_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1441 = "onnx.Mul"(%1439, %1440) {onnx_node_name = "/model/layers.17/attn/k_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1442 = "onnx.MatMulInteger"(%y_201, %491, %y_zero_point_203, %490) {onnx_node_name = "/model/layers.17/attn/v_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1443 = "onnx.Cast"(%1442) {onnx_node_name = "/model/layers.17/attn/v_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1444 = "onnx.Mul"(%y_scale_202, %489) {onnx_node_name = "/model/layers.17/attn/v_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1445 = "onnx.Mul"(%1443, %1444) {onnx_node_name = "/model/layers.17/attn/v_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1446 = "onnx.Reshape"(%1437, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.17/attn/q_norm/Reshape_1"} : (tensor<1x128x2048xf32>, tensor<3xi64>) -> tensor<1x2048x128xf32>
    %1447 = "onnx.Reshape"(%1441, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.17/attn/k_norm/Reshape_1"} : (tensor<1x128x1024xf32>, tensor<3xi64>) -> tensor<1x1024x128xf32>
    %1448 = "onnx.Custom"(%1446, %79) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.17/attn/q_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x2048x128xf32>, tensor<128xf32>) -> tensor<1x2048x128xf32>
    %1449 = "onnx.Custom"(%1447, %80) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.17/attn/k_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x1024x128xf32>, tensor<128xf32>) -> tensor<1x1024x128xf32>
    %1450 = "onnx.Reshape"(%1448, %5) {allowzero = 0 : si64, onnx_node_name = "/model/layers.17/attn/q_norm/Reshape_2"} : (tensor<1x2048x128xf32>, tensor<3xi64>) -> tensor<1x128x2048xf32>
    %1451 = "onnx.Reshape"(%1449, %4) {allowzero = 0 : si64, onnx_node_name = "/model/layers.17/attn/k_norm/Reshape_2"} : (tensor<1x1024x128xf32>, tensor<3xi64>) -> tensor<1x128x1024xf32>
    %1452 = "onnx.Custom"(%1450, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.17/attn/q_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x2048xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x2048xf32>
    %1453 = "onnx.Custom"(%1451, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.17/attn/k_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x1024xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x1024xf32>
    %1454:3 = "onnx.Custom"(%1452, %1453, %1445, %arg37, %arg38, %731, %0, %3, %3) {do_rotary = 0 : si64, domain_name = "com.microsoft", function_name = "GroupQueryAttention", kv_num_heads = 8 : si64, num_heads = 16 : si64, onnx_node_name = "/model/layers.17/attn/GroupQueryAttention", rotary_interleaved = 0 : si64, scale = 0.0883883461 : f32, softcap = 0.000000e+00 : f32} : (tensor<1x128x2048xf32>, tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1x8x1x128xf32>, tensor<1x8x1x128xf32>, tensor<1x1xi32>, tensor<i32>, none, none) -> (tensor<1x128x2048xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>)
    %y_204, %y_scale_205, %y_zero_point_206 = "onnx.DynamicQuantizeLinear"(%1454#0) {onnx_node_name = "/model/layers.17/attn/GroupQueryAttention/output_0_QuantizeLinear"} : (tensor<1x128x2048xf32>) -> (tensor<1x128x2048xui8>, tensor<f32>, tensor<ui8>)
    %1455 = "onnx.MatMulInteger"(%y_204, %494, %y_zero_point_206, %493) {onnx_node_name = "/model/layers.17/attn/o_proj/MatMul_quant"} : (tensor<1x128x2048xui8>, tensor<2048x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1456 = "onnx.Cast"(%1455) {onnx_node_name = "/model/layers.17/attn/o_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1457 = "onnx.Mul"(%y_scale_205, %492) {onnx_node_name = "/model/layers.17/attn/o_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1458 = "onnx.Mul"(%1456, %1457) {onnx_node_name = "/model/layers.17/attn/o_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1459:4 = "onnx.Custom"(%1433#3, %1458, %81) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.17/post_attention_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_207, %y_scale_208, %y_zero_point_209 = "onnx.DynamicQuantizeLinear"(%1459#0) {onnx_node_name = "/model/layers.17/post_attention_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %1460 = "onnx.MatMulInteger"(%y_207, %497, %y_zero_point_209, %496) {onnx_node_name = "/model/layers.17/mlp/gate_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %1461 = "onnx.Cast"(%1460) {onnx_node_name = "/model/layers.17/mlp/gate_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %1462 = "onnx.Mul"(%y_scale_208, %495) {onnx_node_name = "/model/layers.17/mlp/gate_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1463 = "onnx.Mul"(%1461, %1462) {onnx_node_name = "/model/layers.17/mlp/gate_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %1464 = "onnx.MatMulInteger"(%y_207, %500, %y_zero_point_209, %499) {onnx_node_name = "/model/layers.17/mlp/up_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %1465 = "onnx.Cast"(%1464) {onnx_node_name = "/model/layers.17/mlp/up_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %1466 = "onnx.Mul"(%y_scale_208, %498) {onnx_node_name = "/model/layers.17/mlp/up_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1467 = "onnx.Mul"(%1465, %1466) {onnx_node_name = "/model/layers.17/mlp/up_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %1468 = "onnx.Sigmoid"(%1463) {onnx_node_name = "/model/layers.17/mlp/act_fn/Sigmoid"} : (tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %1469 = "onnx.Mul"(%1463, %1468) {onnx_node_name = "/model/layers.17/mlp/act_fn/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %1470 = "onnx.Mul"(%1469, %1467) {onnx_node_name = "/model/layers.17/mlp/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %y_210, %y_scale_211, %y_zero_point_212 = "onnx.DynamicQuantizeLinear"(%1470) {onnx_node_name = "/model/layers.17/mlp/Mul/output_0_QuantizeLinear"} : (tensor<1x128x3072xf32>) -> (tensor<1x128x3072xui8>, tensor<f32>, tensor<ui8>)
    %1471 = "onnx.MatMulInteger"(%y_210, %503, %y_zero_point_212, %502) {onnx_node_name = "/model/layers.17/mlp/down_proj/MatMul_quant"} : (tensor<1x128x3072xui8>, tensor<3072x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1472 = "onnx.Cast"(%1471) {onnx_node_name = "/model/layers.17/mlp/down_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1473 = "onnx.Mul"(%y_scale_211, %501) {onnx_node_name = "/model/layers.17/mlp/down_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1474 = "onnx.Mul"(%1472, %1473) {onnx_node_name = "/model/layers.17/mlp/down_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1475:4 = "onnx.Custom"(%1459#3, %1474, %82) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.18/input_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_213, %y_scale_214, %y_zero_point_215 = "onnx.DynamicQuantizeLinear"(%1475#0) {onnx_node_name = "/model/layers.18/input_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %1476 = "onnx.MatMulInteger"(%y_213, %506, %y_zero_point_215, %505) {onnx_node_name = "/model/layers.18/attn/q_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x2048xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x2048xi32>
    %1477 = "onnx.Cast"(%1476) {onnx_node_name = "/model/layers.18/attn/q_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x2048xi32>) -> tensor<1x128x2048xf32>
    %1478 = "onnx.Mul"(%y_scale_214, %504) {onnx_node_name = "/model/layers.18/attn/q_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1479 = "onnx.Mul"(%1477, %1478) {onnx_node_name = "/model/layers.18/attn/q_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x2048xf32>, tensor<f32>) -> tensor<1x128x2048xf32>
    %1480 = "onnx.MatMulInteger"(%y_213, %509, %y_zero_point_215, %508) {onnx_node_name = "/model/layers.18/attn/k_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1481 = "onnx.Cast"(%1480) {onnx_node_name = "/model/layers.18/attn/k_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1482 = "onnx.Mul"(%y_scale_214, %507) {onnx_node_name = "/model/layers.18/attn/k_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1483 = "onnx.Mul"(%1481, %1482) {onnx_node_name = "/model/layers.18/attn/k_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1484 = "onnx.MatMulInteger"(%y_213, %512, %y_zero_point_215, %511) {onnx_node_name = "/model/layers.18/attn/v_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1485 = "onnx.Cast"(%1484) {onnx_node_name = "/model/layers.18/attn/v_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1486 = "onnx.Mul"(%y_scale_214, %510) {onnx_node_name = "/model/layers.18/attn/v_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1487 = "onnx.Mul"(%1485, %1486) {onnx_node_name = "/model/layers.18/attn/v_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1488 = "onnx.Reshape"(%1479, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.18/attn/q_norm/Reshape_1"} : (tensor<1x128x2048xf32>, tensor<3xi64>) -> tensor<1x2048x128xf32>
    %1489 = "onnx.Reshape"(%1483, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.18/attn/k_norm/Reshape_1"} : (tensor<1x128x1024xf32>, tensor<3xi64>) -> tensor<1x1024x128xf32>
    %1490 = "onnx.Custom"(%1488, %83) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.18/attn/q_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x2048x128xf32>, tensor<128xf32>) -> tensor<1x2048x128xf32>
    %1491 = "onnx.Custom"(%1489, %84) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.18/attn/k_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x1024x128xf32>, tensor<128xf32>) -> tensor<1x1024x128xf32>
    %1492 = "onnx.Reshape"(%1490, %5) {allowzero = 0 : si64, onnx_node_name = "/model/layers.18/attn/q_norm/Reshape_2"} : (tensor<1x2048x128xf32>, tensor<3xi64>) -> tensor<1x128x2048xf32>
    %1493 = "onnx.Reshape"(%1491, %4) {allowzero = 0 : si64, onnx_node_name = "/model/layers.18/attn/k_norm/Reshape_2"} : (tensor<1x1024x128xf32>, tensor<3xi64>) -> tensor<1x128x1024xf32>
    %1494 = "onnx.Custom"(%1492, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.18/attn/q_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x2048xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x2048xf32>
    %1495 = "onnx.Custom"(%1493, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.18/attn/k_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x1024xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x1024xf32>
    %1496:3 = "onnx.Custom"(%1494, %1495, %1487, %arg39, %arg40, %731, %0, %3, %3) {do_rotary = 0 : si64, domain_name = "com.microsoft", function_name = "GroupQueryAttention", kv_num_heads = 8 : si64, num_heads = 16 : si64, onnx_node_name = "/model/layers.18/attn/GroupQueryAttention", rotary_interleaved = 0 : si64, scale = 0.0883883461 : f32, softcap = 0.000000e+00 : f32} : (tensor<1x128x2048xf32>, tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1x8x1x128xf32>, tensor<1x8x1x128xf32>, tensor<1x1xi32>, tensor<i32>, none, none) -> (tensor<1x128x2048xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>)
    %y_216, %y_scale_217, %y_zero_point_218 = "onnx.DynamicQuantizeLinear"(%1496#0) {onnx_node_name = "/model/layers.18/attn/GroupQueryAttention/output_0_QuantizeLinear"} : (tensor<1x128x2048xf32>) -> (tensor<1x128x2048xui8>, tensor<f32>, tensor<ui8>)
    %1497 = "onnx.MatMulInteger"(%y_216, %515, %y_zero_point_218, %514) {onnx_node_name = "/model/layers.18/attn/o_proj/MatMul_quant"} : (tensor<1x128x2048xui8>, tensor<2048x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1498 = "onnx.Cast"(%1497) {onnx_node_name = "/model/layers.18/attn/o_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1499 = "onnx.Mul"(%y_scale_217, %513) {onnx_node_name = "/model/layers.18/attn/o_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1500 = "onnx.Mul"(%1498, %1499) {onnx_node_name = "/model/layers.18/attn/o_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1501:4 = "onnx.Custom"(%1475#3, %1500, %85) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.18/post_attention_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_219, %y_scale_220, %y_zero_point_221 = "onnx.DynamicQuantizeLinear"(%1501#0) {onnx_node_name = "/model/layers.18/post_attention_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %1502 = "onnx.MatMulInteger"(%y_219, %518, %y_zero_point_221, %517) {onnx_node_name = "/model/layers.18/mlp/gate_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %1503 = "onnx.Cast"(%1502) {onnx_node_name = "/model/layers.18/mlp/gate_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %1504 = "onnx.Mul"(%y_scale_220, %516) {onnx_node_name = "/model/layers.18/mlp/gate_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1505 = "onnx.Mul"(%1503, %1504) {onnx_node_name = "/model/layers.18/mlp/gate_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %1506 = "onnx.MatMulInteger"(%y_219, %521, %y_zero_point_221, %520) {onnx_node_name = "/model/layers.18/mlp/up_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %1507 = "onnx.Cast"(%1506) {onnx_node_name = "/model/layers.18/mlp/up_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %1508 = "onnx.Mul"(%y_scale_220, %519) {onnx_node_name = "/model/layers.18/mlp/up_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1509 = "onnx.Mul"(%1507, %1508) {onnx_node_name = "/model/layers.18/mlp/up_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %1510 = "onnx.Sigmoid"(%1505) {onnx_node_name = "/model/layers.18/mlp/act_fn/Sigmoid"} : (tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %1511 = "onnx.Mul"(%1505, %1510) {onnx_node_name = "/model/layers.18/mlp/act_fn/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %1512 = "onnx.Mul"(%1511, %1509) {onnx_node_name = "/model/layers.18/mlp/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %y_222, %y_scale_223, %y_zero_point_224 = "onnx.DynamicQuantizeLinear"(%1512) {onnx_node_name = "/model/layers.18/mlp/Mul/output_0_QuantizeLinear"} : (tensor<1x128x3072xf32>) -> (tensor<1x128x3072xui8>, tensor<f32>, tensor<ui8>)
    %1513 = "onnx.MatMulInteger"(%y_222, %524, %y_zero_point_224, %523) {onnx_node_name = "/model/layers.18/mlp/down_proj/MatMul_quant"} : (tensor<1x128x3072xui8>, tensor<3072x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1514 = "onnx.Cast"(%1513) {onnx_node_name = "/model/layers.18/mlp/down_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1515 = "onnx.Mul"(%y_scale_223, %522) {onnx_node_name = "/model/layers.18/mlp/down_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1516 = "onnx.Mul"(%1514, %1515) {onnx_node_name = "/model/layers.18/mlp/down_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1517:4 = "onnx.Custom"(%1501#3, %1516, %86) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.19/input_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_225, %y_scale_226, %y_zero_point_227 = "onnx.DynamicQuantizeLinear"(%1517#0) {onnx_node_name = "/model/layers.19/input_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %1518 = "onnx.MatMulInteger"(%y_225, %527, %y_zero_point_227, %526) {onnx_node_name = "/model/layers.19/attn/q_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x2048xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x2048xi32>
    %1519 = "onnx.Cast"(%1518) {onnx_node_name = "/model/layers.19/attn/q_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x2048xi32>) -> tensor<1x128x2048xf32>
    %1520 = "onnx.Mul"(%y_scale_226, %525) {onnx_node_name = "/model/layers.19/attn/q_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1521 = "onnx.Mul"(%1519, %1520) {onnx_node_name = "/model/layers.19/attn/q_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x2048xf32>, tensor<f32>) -> tensor<1x128x2048xf32>
    %1522 = "onnx.MatMulInteger"(%y_225, %530, %y_zero_point_227, %529) {onnx_node_name = "/model/layers.19/attn/k_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1523 = "onnx.Cast"(%1522) {onnx_node_name = "/model/layers.19/attn/k_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1524 = "onnx.Mul"(%y_scale_226, %528) {onnx_node_name = "/model/layers.19/attn/k_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1525 = "onnx.Mul"(%1523, %1524) {onnx_node_name = "/model/layers.19/attn/k_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1526 = "onnx.MatMulInteger"(%y_225, %533, %y_zero_point_227, %532) {onnx_node_name = "/model/layers.19/attn/v_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1527 = "onnx.Cast"(%1526) {onnx_node_name = "/model/layers.19/attn/v_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1528 = "onnx.Mul"(%y_scale_226, %531) {onnx_node_name = "/model/layers.19/attn/v_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1529 = "onnx.Mul"(%1527, %1528) {onnx_node_name = "/model/layers.19/attn/v_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1530 = "onnx.Reshape"(%1521, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.19/attn/q_norm/Reshape_1"} : (tensor<1x128x2048xf32>, tensor<3xi64>) -> tensor<1x2048x128xf32>
    %1531 = "onnx.Reshape"(%1525, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.19/attn/k_norm/Reshape_1"} : (tensor<1x128x1024xf32>, tensor<3xi64>) -> tensor<1x1024x128xf32>
    %1532 = "onnx.Custom"(%1530, %87) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.19/attn/q_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x2048x128xf32>, tensor<128xf32>) -> tensor<1x2048x128xf32>
    %1533 = "onnx.Custom"(%1531, %88) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.19/attn/k_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x1024x128xf32>, tensor<128xf32>) -> tensor<1x1024x128xf32>
    %1534 = "onnx.Reshape"(%1532, %5) {allowzero = 0 : si64, onnx_node_name = "/model/layers.19/attn/q_norm/Reshape_2"} : (tensor<1x2048x128xf32>, tensor<3xi64>) -> tensor<1x128x2048xf32>
    %1535 = "onnx.Reshape"(%1533, %4) {allowzero = 0 : si64, onnx_node_name = "/model/layers.19/attn/k_norm/Reshape_2"} : (tensor<1x1024x128xf32>, tensor<3xi64>) -> tensor<1x128x1024xf32>
    %1536 = "onnx.Custom"(%1534, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.19/attn/q_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x2048xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x2048xf32>
    %1537 = "onnx.Custom"(%1535, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.19/attn/k_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x1024xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x1024xf32>
    %1538:3 = "onnx.Custom"(%1536, %1537, %1529, %arg41, %arg42, %731, %0, %3, %3) {do_rotary = 0 : si64, domain_name = "com.microsoft", function_name = "GroupQueryAttention", kv_num_heads = 8 : si64, num_heads = 16 : si64, onnx_node_name = "/model/layers.19/attn/GroupQueryAttention", rotary_interleaved = 0 : si64, scale = 0.0883883461 : f32, softcap = 0.000000e+00 : f32} : (tensor<1x128x2048xf32>, tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1x8x1x128xf32>, tensor<1x8x1x128xf32>, tensor<1x1xi32>, tensor<i32>, none, none) -> (tensor<1x128x2048xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>)
    %y_228, %y_scale_229, %y_zero_point_230 = "onnx.DynamicQuantizeLinear"(%1538#0) {onnx_node_name = "/model/layers.19/attn/GroupQueryAttention/output_0_QuantizeLinear"} : (tensor<1x128x2048xf32>) -> (tensor<1x128x2048xui8>, tensor<f32>, tensor<ui8>)
    %1539 = "onnx.MatMulInteger"(%y_228, %536, %y_zero_point_230, %535) {onnx_node_name = "/model/layers.19/attn/o_proj/MatMul_quant"} : (tensor<1x128x2048xui8>, tensor<2048x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1540 = "onnx.Cast"(%1539) {onnx_node_name = "/model/layers.19/attn/o_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1541 = "onnx.Mul"(%y_scale_229, %534) {onnx_node_name = "/model/layers.19/attn/o_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1542 = "onnx.Mul"(%1540, %1541) {onnx_node_name = "/model/layers.19/attn/o_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1543:4 = "onnx.Custom"(%1517#3, %1542, %89) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.19/post_attention_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_231, %y_scale_232, %y_zero_point_233 = "onnx.DynamicQuantizeLinear"(%1543#0) {onnx_node_name = "/model/layers.19/post_attention_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %1544 = "onnx.MatMulInteger"(%y_231, %539, %y_zero_point_233, %538) {onnx_node_name = "/model/layers.19/mlp/gate_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %1545 = "onnx.Cast"(%1544) {onnx_node_name = "/model/layers.19/mlp/gate_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %1546 = "onnx.Mul"(%y_scale_232, %537) {onnx_node_name = "/model/layers.19/mlp/gate_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1547 = "onnx.Mul"(%1545, %1546) {onnx_node_name = "/model/layers.19/mlp/gate_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %1548 = "onnx.MatMulInteger"(%y_231, %542, %y_zero_point_233, %541) {onnx_node_name = "/model/layers.19/mlp/up_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %1549 = "onnx.Cast"(%1548) {onnx_node_name = "/model/layers.19/mlp/up_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %1550 = "onnx.Mul"(%y_scale_232, %540) {onnx_node_name = "/model/layers.19/mlp/up_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1551 = "onnx.Mul"(%1549, %1550) {onnx_node_name = "/model/layers.19/mlp/up_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %1552 = "onnx.Sigmoid"(%1547) {onnx_node_name = "/model/layers.19/mlp/act_fn/Sigmoid"} : (tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %1553 = "onnx.Mul"(%1547, %1552) {onnx_node_name = "/model/layers.19/mlp/act_fn/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %1554 = "onnx.Mul"(%1553, %1551) {onnx_node_name = "/model/layers.19/mlp/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %y_234, %y_scale_235, %y_zero_point_236 = "onnx.DynamicQuantizeLinear"(%1554) {onnx_node_name = "/model/layers.19/mlp/Mul/output_0_QuantizeLinear"} : (tensor<1x128x3072xf32>) -> (tensor<1x128x3072xui8>, tensor<f32>, tensor<ui8>)
    %1555 = "onnx.MatMulInteger"(%y_234, %545, %y_zero_point_236, %544) {onnx_node_name = "/model/layers.19/mlp/down_proj/MatMul_quant"} : (tensor<1x128x3072xui8>, tensor<3072x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1556 = "onnx.Cast"(%1555) {onnx_node_name = "/model/layers.19/mlp/down_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1557 = "onnx.Mul"(%y_scale_235, %543) {onnx_node_name = "/model/layers.19/mlp/down_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1558 = "onnx.Mul"(%1556, %1557) {onnx_node_name = "/model/layers.19/mlp/down_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1559:4 = "onnx.Custom"(%1543#3, %1558, %90) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.20/input_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_237, %y_scale_238, %y_zero_point_239 = "onnx.DynamicQuantizeLinear"(%1559#0) {onnx_node_name = "/model/layers.20/input_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %1560 = "onnx.MatMulInteger"(%y_237, %548, %y_zero_point_239, %547) {onnx_node_name = "/model/layers.20/attn/q_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x2048xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x2048xi32>
    %1561 = "onnx.Cast"(%1560) {onnx_node_name = "/model/layers.20/attn/q_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x2048xi32>) -> tensor<1x128x2048xf32>
    %1562 = "onnx.Mul"(%y_scale_238, %546) {onnx_node_name = "/model/layers.20/attn/q_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1563 = "onnx.Mul"(%1561, %1562) {onnx_node_name = "/model/layers.20/attn/q_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x2048xf32>, tensor<f32>) -> tensor<1x128x2048xf32>
    %1564 = "onnx.MatMulInteger"(%y_237, %551, %y_zero_point_239, %550) {onnx_node_name = "/model/layers.20/attn/k_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1565 = "onnx.Cast"(%1564) {onnx_node_name = "/model/layers.20/attn/k_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1566 = "onnx.Mul"(%y_scale_238, %549) {onnx_node_name = "/model/layers.20/attn/k_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1567 = "onnx.Mul"(%1565, %1566) {onnx_node_name = "/model/layers.20/attn/k_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1568 = "onnx.MatMulInteger"(%y_237, %554, %y_zero_point_239, %553) {onnx_node_name = "/model/layers.20/attn/v_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1569 = "onnx.Cast"(%1568) {onnx_node_name = "/model/layers.20/attn/v_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1570 = "onnx.Mul"(%y_scale_238, %552) {onnx_node_name = "/model/layers.20/attn/v_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1571 = "onnx.Mul"(%1569, %1570) {onnx_node_name = "/model/layers.20/attn/v_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1572 = "onnx.Reshape"(%1563, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.20/attn/q_norm/Reshape_1"} : (tensor<1x128x2048xf32>, tensor<3xi64>) -> tensor<1x2048x128xf32>
    %1573 = "onnx.Reshape"(%1567, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.20/attn/k_norm/Reshape_1"} : (tensor<1x128x1024xf32>, tensor<3xi64>) -> tensor<1x1024x128xf32>
    %1574 = "onnx.Custom"(%1572, %91) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.20/attn/q_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x2048x128xf32>, tensor<128xf32>) -> tensor<1x2048x128xf32>
    %1575 = "onnx.Custom"(%1573, %92) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.20/attn/k_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x1024x128xf32>, tensor<128xf32>) -> tensor<1x1024x128xf32>
    %1576 = "onnx.Reshape"(%1574, %5) {allowzero = 0 : si64, onnx_node_name = "/model/layers.20/attn/q_norm/Reshape_2"} : (tensor<1x2048x128xf32>, tensor<3xi64>) -> tensor<1x128x2048xf32>
    %1577 = "onnx.Reshape"(%1575, %4) {allowzero = 0 : si64, onnx_node_name = "/model/layers.20/attn/k_norm/Reshape_2"} : (tensor<1x1024x128xf32>, tensor<3xi64>) -> tensor<1x128x1024xf32>
    %1578 = "onnx.Custom"(%1576, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.20/attn/q_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x2048xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x2048xf32>
    %1579 = "onnx.Custom"(%1577, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.20/attn/k_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x1024xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x1024xf32>
    %1580:3 = "onnx.Custom"(%1578, %1579, %1571, %arg43, %arg44, %731, %0, %3, %3) {do_rotary = 0 : si64, domain_name = "com.microsoft", function_name = "GroupQueryAttention", kv_num_heads = 8 : si64, num_heads = 16 : si64, onnx_node_name = "/model/layers.20/attn/GroupQueryAttention", rotary_interleaved = 0 : si64, scale = 0.0883883461 : f32, softcap = 0.000000e+00 : f32} : (tensor<1x128x2048xf32>, tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1x8x1x128xf32>, tensor<1x8x1x128xf32>, tensor<1x1xi32>, tensor<i32>, none, none) -> (tensor<1x128x2048xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>)
    %y_240, %y_scale_241, %y_zero_point_242 = "onnx.DynamicQuantizeLinear"(%1580#0) {onnx_node_name = "/model/layers.20/attn/GroupQueryAttention/output_0_QuantizeLinear"} : (tensor<1x128x2048xf32>) -> (tensor<1x128x2048xui8>, tensor<f32>, tensor<ui8>)
    %1581 = "onnx.MatMulInteger"(%y_240, %557, %y_zero_point_242, %556) {onnx_node_name = "/model/layers.20/attn/o_proj/MatMul_quant"} : (tensor<1x128x2048xui8>, tensor<2048x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1582 = "onnx.Cast"(%1581) {onnx_node_name = "/model/layers.20/attn/o_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1583 = "onnx.Mul"(%y_scale_241, %555) {onnx_node_name = "/model/layers.20/attn/o_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1584 = "onnx.Mul"(%1582, %1583) {onnx_node_name = "/model/layers.20/attn/o_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1585:4 = "onnx.Custom"(%1559#3, %1584, %93) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.20/post_attention_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_243, %y_scale_244, %y_zero_point_245 = "onnx.DynamicQuantizeLinear"(%1585#0) {onnx_node_name = "/model/layers.20/post_attention_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %1586 = "onnx.MatMulInteger"(%y_243, %560, %y_zero_point_245, %559) {onnx_node_name = "/model/layers.20/mlp/gate_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %1587 = "onnx.Cast"(%1586) {onnx_node_name = "/model/layers.20/mlp/gate_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %1588 = "onnx.Mul"(%y_scale_244, %558) {onnx_node_name = "/model/layers.20/mlp/gate_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1589 = "onnx.Mul"(%1587, %1588) {onnx_node_name = "/model/layers.20/mlp/gate_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %1590 = "onnx.MatMulInteger"(%y_243, %563, %y_zero_point_245, %562) {onnx_node_name = "/model/layers.20/mlp/up_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %1591 = "onnx.Cast"(%1590) {onnx_node_name = "/model/layers.20/mlp/up_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %1592 = "onnx.Mul"(%y_scale_244, %561) {onnx_node_name = "/model/layers.20/mlp/up_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1593 = "onnx.Mul"(%1591, %1592) {onnx_node_name = "/model/layers.20/mlp/up_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %1594 = "onnx.Sigmoid"(%1589) {onnx_node_name = "/model/layers.20/mlp/act_fn/Sigmoid"} : (tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %1595 = "onnx.Mul"(%1589, %1594) {onnx_node_name = "/model/layers.20/mlp/act_fn/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %1596 = "onnx.Mul"(%1595, %1593) {onnx_node_name = "/model/layers.20/mlp/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %y_246, %y_scale_247, %y_zero_point_248 = "onnx.DynamicQuantizeLinear"(%1596) {onnx_node_name = "/model/layers.20/mlp/Mul/output_0_QuantizeLinear"} : (tensor<1x128x3072xf32>) -> (tensor<1x128x3072xui8>, tensor<f32>, tensor<ui8>)
    %1597 = "onnx.MatMulInteger"(%y_246, %566, %y_zero_point_248, %565) {onnx_node_name = "/model/layers.20/mlp/down_proj/MatMul_quant"} : (tensor<1x128x3072xui8>, tensor<3072x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1598 = "onnx.Cast"(%1597) {onnx_node_name = "/model/layers.20/mlp/down_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1599 = "onnx.Mul"(%y_scale_247, %564) {onnx_node_name = "/model/layers.20/mlp/down_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1600 = "onnx.Mul"(%1598, %1599) {onnx_node_name = "/model/layers.20/mlp/down_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1601:4 = "onnx.Custom"(%1585#3, %1600, %94) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.21/input_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_249, %y_scale_250, %y_zero_point_251 = "onnx.DynamicQuantizeLinear"(%1601#0) {onnx_node_name = "/model/layers.21/input_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %1602 = "onnx.MatMulInteger"(%y_249, %569, %y_zero_point_251, %568) {onnx_node_name = "/model/layers.21/attn/q_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x2048xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x2048xi32>
    %1603 = "onnx.Cast"(%1602) {onnx_node_name = "/model/layers.21/attn/q_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x2048xi32>) -> tensor<1x128x2048xf32>
    %1604 = "onnx.Mul"(%y_scale_250, %567) {onnx_node_name = "/model/layers.21/attn/q_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1605 = "onnx.Mul"(%1603, %1604) {onnx_node_name = "/model/layers.21/attn/q_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x2048xf32>, tensor<f32>) -> tensor<1x128x2048xf32>
    %1606 = "onnx.MatMulInteger"(%y_249, %572, %y_zero_point_251, %571) {onnx_node_name = "/model/layers.21/attn/k_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1607 = "onnx.Cast"(%1606) {onnx_node_name = "/model/layers.21/attn/k_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1608 = "onnx.Mul"(%y_scale_250, %570) {onnx_node_name = "/model/layers.21/attn/k_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1609 = "onnx.Mul"(%1607, %1608) {onnx_node_name = "/model/layers.21/attn/k_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1610 = "onnx.MatMulInteger"(%y_249, %575, %y_zero_point_251, %574) {onnx_node_name = "/model/layers.21/attn/v_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1611 = "onnx.Cast"(%1610) {onnx_node_name = "/model/layers.21/attn/v_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1612 = "onnx.Mul"(%y_scale_250, %573) {onnx_node_name = "/model/layers.21/attn/v_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1613 = "onnx.Mul"(%1611, %1612) {onnx_node_name = "/model/layers.21/attn/v_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1614 = "onnx.Reshape"(%1605, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.21/attn/q_norm/Reshape_1"} : (tensor<1x128x2048xf32>, tensor<3xi64>) -> tensor<1x2048x128xf32>
    %1615 = "onnx.Reshape"(%1609, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.21/attn/k_norm/Reshape_1"} : (tensor<1x128x1024xf32>, tensor<3xi64>) -> tensor<1x1024x128xf32>
    %1616 = "onnx.Custom"(%1614, %95) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.21/attn/q_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x2048x128xf32>, tensor<128xf32>) -> tensor<1x2048x128xf32>
    %1617 = "onnx.Custom"(%1615, %96) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.21/attn/k_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x1024x128xf32>, tensor<128xf32>) -> tensor<1x1024x128xf32>
    %1618 = "onnx.Reshape"(%1616, %5) {allowzero = 0 : si64, onnx_node_name = "/model/layers.21/attn/q_norm/Reshape_2"} : (tensor<1x2048x128xf32>, tensor<3xi64>) -> tensor<1x128x2048xf32>
    %1619 = "onnx.Reshape"(%1617, %4) {allowzero = 0 : si64, onnx_node_name = "/model/layers.21/attn/k_norm/Reshape_2"} : (tensor<1x1024x128xf32>, tensor<3xi64>) -> tensor<1x128x1024xf32>
    %1620 = "onnx.Custom"(%1618, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.21/attn/q_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x2048xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x2048xf32>
    %1621 = "onnx.Custom"(%1619, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.21/attn/k_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x1024xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x1024xf32>
    %1622:3 = "onnx.Custom"(%1620, %1621, %1613, %arg45, %arg46, %731, %0, %3, %3) {do_rotary = 0 : si64, domain_name = "com.microsoft", function_name = "GroupQueryAttention", kv_num_heads = 8 : si64, num_heads = 16 : si64, onnx_node_name = "/model/layers.21/attn/GroupQueryAttention", rotary_interleaved = 0 : si64, scale = 0.0883883461 : f32, softcap = 0.000000e+00 : f32} : (tensor<1x128x2048xf32>, tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1x8x1x128xf32>, tensor<1x8x1x128xf32>, tensor<1x1xi32>, tensor<i32>, none, none) -> (tensor<1x128x2048xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>)
    %y_252, %y_scale_253, %y_zero_point_254 = "onnx.DynamicQuantizeLinear"(%1622#0) {onnx_node_name = "/model/layers.21/attn/GroupQueryAttention/output_0_QuantizeLinear"} : (tensor<1x128x2048xf32>) -> (tensor<1x128x2048xui8>, tensor<f32>, tensor<ui8>)
    %1623 = "onnx.MatMulInteger"(%y_252, %578, %y_zero_point_254, %577) {onnx_node_name = "/model/layers.21/attn/o_proj/MatMul_quant"} : (tensor<1x128x2048xui8>, tensor<2048x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1624 = "onnx.Cast"(%1623) {onnx_node_name = "/model/layers.21/attn/o_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1625 = "onnx.Mul"(%y_scale_253, %576) {onnx_node_name = "/model/layers.21/attn/o_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1626 = "onnx.Mul"(%1624, %1625) {onnx_node_name = "/model/layers.21/attn/o_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1627:4 = "onnx.Custom"(%1601#3, %1626, %97) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.21/post_attention_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_255, %y_scale_256, %y_zero_point_257 = "onnx.DynamicQuantizeLinear"(%1627#0) {onnx_node_name = "/model/layers.21/post_attention_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %1628 = "onnx.MatMulInteger"(%y_255, %581, %y_zero_point_257, %580) {onnx_node_name = "/model/layers.21/mlp/gate_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %1629 = "onnx.Cast"(%1628) {onnx_node_name = "/model/layers.21/mlp/gate_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %1630 = "onnx.Mul"(%y_scale_256, %579) {onnx_node_name = "/model/layers.21/mlp/gate_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1631 = "onnx.Mul"(%1629, %1630) {onnx_node_name = "/model/layers.21/mlp/gate_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %1632 = "onnx.MatMulInteger"(%y_255, %584, %y_zero_point_257, %583) {onnx_node_name = "/model/layers.21/mlp/up_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %1633 = "onnx.Cast"(%1632) {onnx_node_name = "/model/layers.21/mlp/up_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %1634 = "onnx.Mul"(%y_scale_256, %582) {onnx_node_name = "/model/layers.21/mlp/up_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1635 = "onnx.Mul"(%1633, %1634) {onnx_node_name = "/model/layers.21/mlp/up_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %1636 = "onnx.Sigmoid"(%1631) {onnx_node_name = "/model/layers.21/mlp/act_fn/Sigmoid"} : (tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %1637 = "onnx.Mul"(%1631, %1636) {onnx_node_name = "/model/layers.21/mlp/act_fn/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %1638 = "onnx.Mul"(%1637, %1635) {onnx_node_name = "/model/layers.21/mlp/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %y_258, %y_scale_259, %y_zero_point_260 = "onnx.DynamicQuantizeLinear"(%1638) {onnx_node_name = "/model/layers.21/mlp/Mul/output_0_QuantizeLinear"} : (tensor<1x128x3072xf32>) -> (tensor<1x128x3072xui8>, tensor<f32>, tensor<ui8>)
    %1639 = "onnx.MatMulInteger"(%y_258, %587, %y_zero_point_260, %586) {onnx_node_name = "/model/layers.21/mlp/down_proj/MatMul_quant"} : (tensor<1x128x3072xui8>, tensor<3072x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1640 = "onnx.Cast"(%1639) {onnx_node_name = "/model/layers.21/mlp/down_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1641 = "onnx.Mul"(%y_scale_259, %585) {onnx_node_name = "/model/layers.21/mlp/down_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1642 = "onnx.Mul"(%1640, %1641) {onnx_node_name = "/model/layers.21/mlp/down_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1643:4 = "onnx.Custom"(%1627#3, %1642, %98) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.22/input_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_261, %y_scale_262, %y_zero_point_263 = "onnx.DynamicQuantizeLinear"(%1643#0) {onnx_node_name = "/model/layers.22/input_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %1644 = "onnx.MatMulInteger"(%y_261, %590, %y_zero_point_263, %589) {onnx_node_name = "/model/layers.22/attn/q_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x2048xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x2048xi32>
    %1645 = "onnx.Cast"(%1644) {onnx_node_name = "/model/layers.22/attn/q_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x2048xi32>) -> tensor<1x128x2048xf32>
    %1646 = "onnx.Mul"(%y_scale_262, %588) {onnx_node_name = "/model/layers.22/attn/q_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1647 = "onnx.Mul"(%1645, %1646) {onnx_node_name = "/model/layers.22/attn/q_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x2048xf32>, tensor<f32>) -> tensor<1x128x2048xf32>
    %1648 = "onnx.MatMulInteger"(%y_261, %593, %y_zero_point_263, %592) {onnx_node_name = "/model/layers.22/attn/k_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1649 = "onnx.Cast"(%1648) {onnx_node_name = "/model/layers.22/attn/k_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1650 = "onnx.Mul"(%y_scale_262, %591) {onnx_node_name = "/model/layers.22/attn/k_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1651 = "onnx.Mul"(%1649, %1650) {onnx_node_name = "/model/layers.22/attn/k_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1652 = "onnx.MatMulInteger"(%y_261, %596, %y_zero_point_263, %595) {onnx_node_name = "/model/layers.22/attn/v_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1653 = "onnx.Cast"(%1652) {onnx_node_name = "/model/layers.22/attn/v_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1654 = "onnx.Mul"(%y_scale_262, %594) {onnx_node_name = "/model/layers.22/attn/v_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1655 = "onnx.Mul"(%1653, %1654) {onnx_node_name = "/model/layers.22/attn/v_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1656 = "onnx.Reshape"(%1647, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.22/attn/q_norm/Reshape_1"} : (tensor<1x128x2048xf32>, tensor<3xi64>) -> tensor<1x2048x128xf32>
    %1657 = "onnx.Reshape"(%1651, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.22/attn/k_norm/Reshape_1"} : (tensor<1x128x1024xf32>, tensor<3xi64>) -> tensor<1x1024x128xf32>
    %1658 = "onnx.Custom"(%1656, %99) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.22/attn/q_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x2048x128xf32>, tensor<128xf32>) -> tensor<1x2048x128xf32>
    %1659 = "onnx.Custom"(%1657, %100) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.22/attn/k_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x1024x128xf32>, tensor<128xf32>) -> tensor<1x1024x128xf32>
    %1660 = "onnx.Reshape"(%1658, %5) {allowzero = 0 : si64, onnx_node_name = "/model/layers.22/attn/q_norm/Reshape_2"} : (tensor<1x2048x128xf32>, tensor<3xi64>) -> tensor<1x128x2048xf32>
    %1661 = "onnx.Reshape"(%1659, %4) {allowzero = 0 : si64, onnx_node_name = "/model/layers.22/attn/k_norm/Reshape_2"} : (tensor<1x1024x128xf32>, tensor<3xi64>) -> tensor<1x128x1024xf32>
    %1662 = "onnx.Custom"(%1660, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.22/attn/q_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x2048xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x2048xf32>
    %1663 = "onnx.Custom"(%1661, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.22/attn/k_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x1024xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x1024xf32>
    %1664:3 = "onnx.Custom"(%1662, %1663, %1655, %arg47, %arg48, %731, %0, %3, %3) {do_rotary = 0 : si64, domain_name = "com.microsoft", function_name = "GroupQueryAttention", kv_num_heads = 8 : si64, num_heads = 16 : si64, onnx_node_name = "/model/layers.22/attn/GroupQueryAttention", rotary_interleaved = 0 : si64, scale = 0.0883883461 : f32, softcap = 0.000000e+00 : f32} : (tensor<1x128x2048xf32>, tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1x8x1x128xf32>, tensor<1x8x1x128xf32>, tensor<1x1xi32>, tensor<i32>, none, none) -> (tensor<1x128x2048xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>)
    %y_264, %y_scale_265, %y_zero_point_266 = "onnx.DynamicQuantizeLinear"(%1664#0) {onnx_node_name = "/model/layers.22/attn/GroupQueryAttention/output_0_QuantizeLinear"} : (tensor<1x128x2048xf32>) -> (tensor<1x128x2048xui8>, tensor<f32>, tensor<ui8>)
    %1665 = "onnx.MatMulInteger"(%y_264, %599, %y_zero_point_266, %598) {onnx_node_name = "/model/layers.22/attn/o_proj/MatMul_quant"} : (tensor<1x128x2048xui8>, tensor<2048x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1666 = "onnx.Cast"(%1665) {onnx_node_name = "/model/layers.22/attn/o_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1667 = "onnx.Mul"(%y_scale_265, %597) {onnx_node_name = "/model/layers.22/attn/o_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1668 = "onnx.Mul"(%1666, %1667) {onnx_node_name = "/model/layers.22/attn/o_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1669:4 = "onnx.Custom"(%1643#3, %1668, %101) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.22/post_attention_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_267, %y_scale_268, %y_zero_point_269 = "onnx.DynamicQuantizeLinear"(%1669#0) {onnx_node_name = "/model/layers.22/post_attention_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %1670 = "onnx.MatMulInteger"(%y_267, %602, %y_zero_point_269, %601) {onnx_node_name = "/model/layers.22/mlp/gate_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %1671 = "onnx.Cast"(%1670) {onnx_node_name = "/model/layers.22/mlp/gate_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %1672 = "onnx.Mul"(%y_scale_268, %600) {onnx_node_name = "/model/layers.22/mlp/gate_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1673 = "onnx.Mul"(%1671, %1672) {onnx_node_name = "/model/layers.22/mlp/gate_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %1674 = "onnx.MatMulInteger"(%y_267, %605, %y_zero_point_269, %604) {onnx_node_name = "/model/layers.22/mlp/up_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %1675 = "onnx.Cast"(%1674) {onnx_node_name = "/model/layers.22/mlp/up_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %1676 = "onnx.Mul"(%y_scale_268, %603) {onnx_node_name = "/model/layers.22/mlp/up_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1677 = "onnx.Mul"(%1675, %1676) {onnx_node_name = "/model/layers.22/mlp/up_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %1678 = "onnx.Sigmoid"(%1673) {onnx_node_name = "/model/layers.22/mlp/act_fn/Sigmoid"} : (tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %1679 = "onnx.Mul"(%1673, %1678) {onnx_node_name = "/model/layers.22/mlp/act_fn/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %1680 = "onnx.Mul"(%1679, %1677) {onnx_node_name = "/model/layers.22/mlp/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %y_270, %y_scale_271, %y_zero_point_272 = "onnx.DynamicQuantizeLinear"(%1680) {onnx_node_name = "/model/layers.22/mlp/Mul/output_0_QuantizeLinear"} : (tensor<1x128x3072xf32>) -> (tensor<1x128x3072xui8>, tensor<f32>, tensor<ui8>)
    %1681 = "onnx.MatMulInteger"(%y_270, %608, %y_zero_point_272, %607) {onnx_node_name = "/model/layers.22/mlp/down_proj/MatMul_quant"} : (tensor<1x128x3072xui8>, tensor<3072x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1682 = "onnx.Cast"(%1681) {onnx_node_name = "/model/layers.22/mlp/down_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1683 = "onnx.Mul"(%y_scale_271, %606) {onnx_node_name = "/model/layers.22/mlp/down_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1684 = "onnx.Mul"(%1682, %1683) {onnx_node_name = "/model/layers.22/mlp/down_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1685:4 = "onnx.Custom"(%1669#3, %1684, %102) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.23/input_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_273, %y_scale_274, %y_zero_point_275 = "onnx.DynamicQuantizeLinear"(%1685#0) {onnx_node_name = "/model/layers.23/input_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %1686 = "onnx.MatMulInteger"(%y_273, %611, %y_zero_point_275, %610) {onnx_node_name = "/model/layers.23/attn/q_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x2048xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x2048xi32>
    %1687 = "onnx.Cast"(%1686) {onnx_node_name = "/model/layers.23/attn/q_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x2048xi32>) -> tensor<1x128x2048xf32>
    %1688 = "onnx.Mul"(%y_scale_274, %609) {onnx_node_name = "/model/layers.23/attn/q_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1689 = "onnx.Mul"(%1687, %1688) {onnx_node_name = "/model/layers.23/attn/q_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x2048xf32>, tensor<f32>) -> tensor<1x128x2048xf32>
    %1690 = "onnx.MatMulInteger"(%y_273, %614, %y_zero_point_275, %613) {onnx_node_name = "/model/layers.23/attn/k_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1691 = "onnx.Cast"(%1690) {onnx_node_name = "/model/layers.23/attn/k_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1692 = "onnx.Mul"(%y_scale_274, %612) {onnx_node_name = "/model/layers.23/attn/k_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1693 = "onnx.Mul"(%1691, %1692) {onnx_node_name = "/model/layers.23/attn/k_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1694 = "onnx.MatMulInteger"(%y_273, %617, %y_zero_point_275, %616) {onnx_node_name = "/model/layers.23/attn/v_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1695 = "onnx.Cast"(%1694) {onnx_node_name = "/model/layers.23/attn/v_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1696 = "onnx.Mul"(%y_scale_274, %615) {onnx_node_name = "/model/layers.23/attn/v_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1697 = "onnx.Mul"(%1695, %1696) {onnx_node_name = "/model/layers.23/attn/v_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1698 = "onnx.Reshape"(%1689, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.23/attn/q_norm/Reshape_1"} : (tensor<1x128x2048xf32>, tensor<3xi64>) -> tensor<1x2048x128xf32>
    %1699 = "onnx.Reshape"(%1693, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.23/attn/k_norm/Reshape_1"} : (tensor<1x128x1024xf32>, tensor<3xi64>) -> tensor<1x1024x128xf32>
    %1700 = "onnx.Custom"(%1698, %103) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.23/attn/q_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x2048x128xf32>, tensor<128xf32>) -> tensor<1x2048x128xf32>
    %1701 = "onnx.Custom"(%1699, %104) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.23/attn/k_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x1024x128xf32>, tensor<128xf32>) -> tensor<1x1024x128xf32>
    %1702 = "onnx.Reshape"(%1700, %5) {allowzero = 0 : si64, onnx_node_name = "/model/layers.23/attn/q_norm/Reshape_2"} : (tensor<1x2048x128xf32>, tensor<3xi64>) -> tensor<1x128x2048xf32>
    %1703 = "onnx.Reshape"(%1701, %4) {allowzero = 0 : si64, onnx_node_name = "/model/layers.23/attn/k_norm/Reshape_2"} : (tensor<1x1024x128xf32>, tensor<3xi64>) -> tensor<1x128x1024xf32>
    %1704 = "onnx.Custom"(%1702, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.23/attn/q_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x2048xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x2048xf32>
    %1705 = "onnx.Custom"(%1703, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.23/attn/k_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x1024xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x1024xf32>
    %1706:3 = "onnx.Custom"(%1704, %1705, %1697, %arg49, %arg50, %731, %0, %3, %3) {do_rotary = 0 : si64, domain_name = "com.microsoft", function_name = "GroupQueryAttention", kv_num_heads = 8 : si64, num_heads = 16 : si64, onnx_node_name = "/model/layers.23/attn/GroupQueryAttention", rotary_interleaved = 0 : si64, scale = 0.0883883461 : f32, softcap = 0.000000e+00 : f32} : (tensor<1x128x2048xf32>, tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1x8x1x128xf32>, tensor<1x8x1x128xf32>, tensor<1x1xi32>, tensor<i32>, none, none) -> (tensor<1x128x2048xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>)
    %y_276, %y_scale_277, %y_zero_point_278 = "onnx.DynamicQuantizeLinear"(%1706#0) {onnx_node_name = "/model/layers.23/attn/GroupQueryAttention/output_0_QuantizeLinear"} : (tensor<1x128x2048xf32>) -> (tensor<1x128x2048xui8>, tensor<f32>, tensor<ui8>)
    %1707 = "onnx.MatMulInteger"(%y_276, %620, %y_zero_point_278, %619) {onnx_node_name = "/model/layers.23/attn/o_proj/MatMul_quant"} : (tensor<1x128x2048xui8>, tensor<2048x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1708 = "onnx.Cast"(%1707) {onnx_node_name = "/model/layers.23/attn/o_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1709 = "onnx.Mul"(%y_scale_277, %618) {onnx_node_name = "/model/layers.23/attn/o_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1710 = "onnx.Mul"(%1708, %1709) {onnx_node_name = "/model/layers.23/attn/o_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1711:4 = "onnx.Custom"(%1685#3, %1710, %105) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.23/post_attention_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_279, %y_scale_280, %y_zero_point_281 = "onnx.DynamicQuantizeLinear"(%1711#0) {onnx_node_name = "/model/layers.23/post_attention_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %1712 = "onnx.MatMulInteger"(%y_279, %623, %y_zero_point_281, %622) {onnx_node_name = "/model/layers.23/mlp/gate_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %1713 = "onnx.Cast"(%1712) {onnx_node_name = "/model/layers.23/mlp/gate_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %1714 = "onnx.Mul"(%y_scale_280, %621) {onnx_node_name = "/model/layers.23/mlp/gate_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1715 = "onnx.Mul"(%1713, %1714) {onnx_node_name = "/model/layers.23/mlp/gate_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %1716 = "onnx.MatMulInteger"(%y_279, %626, %y_zero_point_281, %625) {onnx_node_name = "/model/layers.23/mlp/up_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %1717 = "onnx.Cast"(%1716) {onnx_node_name = "/model/layers.23/mlp/up_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %1718 = "onnx.Mul"(%y_scale_280, %624) {onnx_node_name = "/model/layers.23/mlp/up_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1719 = "onnx.Mul"(%1717, %1718) {onnx_node_name = "/model/layers.23/mlp/up_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %1720 = "onnx.Sigmoid"(%1715) {onnx_node_name = "/model/layers.23/mlp/act_fn/Sigmoid"} : (tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %1721 = "onnx.Mul"(%1715, %1720) {onnx_node_name = "/model/layers.23/mlp/act_fn/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %1722 = "onnx.Mul"(%1721, %1719) {onnx_node_name = "/model/layers.23/mlp/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %y_282, %y_scale_283, %y_zero_point_284 = "onnx.DynamicQuantizeLinear"(%1722) {onnx_node_name = "/model/layers.23/mlp/Mul/output_0_QuantizeLinear"} : (tensor<1x128x3072xf32>) -> (tensor<1x128x3072xui8>, tensor<f32>, tensor<ui8>)
    %1723 = "onnx.MatMulInteger"(%y_282, %629, %y_zero_point_284, %628) {onnx_node_name = "/model/layers.23/mlp/down_proj/MatMul_quant"} : (tensor<1x128x3072xui8>, tensor<3072x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1724 = "onnx.Cast"(%1723) {onnx_node_name = "/model/layers.23/mlp/down_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1725 = "onnx.Mul"(%y_scale_283, %627) {onnx_node_name = "/model/layers.23/mlp/down_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1726 = "onnx.Mul"(%1724, %1725) {onnx_node_name = "/model/layers.23/mlp/down_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1727:4 = "onnx.Custom"(%1711#3, %1726, %106) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.24/input_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_285, %y_scale_286, %y_zero_point_287 = "onnx.DynamicQuantizeLinear"(%1727#0) {onnx_node_name = "/model/layers.24/input_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %1728 = "onnx.MatMulInteger"(%y_285, %632, %y_zero_point_287, %631) {onnx_node_name = "/model/layers.24/attn/q_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x2048xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x2048xi32>
    %1729 = "onnx.Cast"(%1728) {onnx_node_name = "/model/layers.24/attn/q_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x2048xi32>) -> tensor<1x128x2048xf32>
    %1730 = "onnx.Mul"(%y_scale_286, %630) {onnx_node_name = "/model/layers.24/attn/q_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1731 = "onnx.Mul"(%1729, %1730) {onnx_node_name = "/model/layers.24/attn/q_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x2048xf32>, tensor<f32>) -> tensor<1x128x2048xf32>
    %1732 = "onnx.MatMulInteger"(%y_285, %635, %y_zero_point_287, %634) {onnx_node_name = "/model/layers.24/attn/k_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1733 = "onnx.Cast"(%1732) {onnx_node_name = "/model/layers.24/attn/k_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1734 = "onnx.Mul"(%y_scale_286, %633) {onnx_node_name = "/model/layers.24/attn/k_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1735 = "onnx.Mul"(%1733, %1734) {onnx_node_name = "/model/layers.24/attn/k_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1736 = "onnx.MatMulInteger"(%y_285, %638, %y_zero_point_287, %637) {onnx_node_name = "/model/layers.24/attn/v_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1737 = "onnx.Cast"(%1736) {onnx_node_name = "/model/layers.24/attn/v_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1738 = "onnx.Mul"(%y_scale_286, %636) {onnx_node_name = "/model/layers.24/attn/v_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1739 = "onnx.Mul"(%1737, %1738) {onnx_node_name = "/model/layers.24/attn/v_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1740 = "onnx.Reshape"(%1731, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.24/attn/q_norm/Reshape_1"} : (tensor<1x128x2048xf32>, tensor<3xi64>) -> tensor<1x2048x128xf32>
    %1741 = "onnx.Reshape"(%1735, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.24/attn/k_norm/Reshape_1"} : (tensor<1x128x1024xf32>, tensor<3xi64>) -> tensor<1x1024x128xf32>
    %1742 = "onnx.Custom"(%1740, %107) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.24/attn/q_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x2048x128xf32>, tensor<128xf32>) -> tensor<1x2048x128xf32>
    %1743 = "onnx.Custom"(%1741, %108) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.24/attn/k_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x1024x128xf32>, tensor<128xf32>) -> tensor<1x1024x128xf32>
    %1744 = "onnx.Reshape"(%1742, %5) {allowzero = 0 : si64, onnx_node_name = "/model/layers.24/attn/q_norm/Reshape_2"} : (tensor<1x2048x128xf32>, tensor<3xi64>) -> tensor<1x128x2048xf32>
    %1745 = "onnx.Reshape"(%1743, %4) {allowzero = 0 : si64, onnx_node_name = "/model/layers.24/attn/k_norm/Reshape_2"} : (tensor<1x1024x128xf32>, tensor<3xi64>) -> tensor<1x128x1024xf32>
    %1746 = "onnx.Custom"(%1744, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.24/attn/q_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x2048xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x2048xf32>
    %1747 = "onnx.Custom"(%1745, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.24/attn/k_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x1024xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x1024xf32>
    %1748:3 = "onnx.Custom"(%1746, %1747, %1739, %arg51, %arg52, %731, %0, %3, %3) {do_rotary = 0 : si64, domain_name = "com.microsoft", function_name = "GroupQueryAttention", kv_num_heads = 8 : si64, num_heads = 16 : si64, onnx_node_name = "/model/layers.24/attn/GroupQueryAttention", rotary_interleaved = 0 : si64, scale = 0.0883883461 : f32, softcap = 0.000000e+00 : f32} : (tensor<1x128x2048xf32>, tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1x8x1x128xf32>, tensor<1x8x1x128xf32>, tensor<1x1xi32>, tensor<i32>, none, none) -> (tensor<1x128x2048xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>)
    %y_288, %y_scale_289, %y_zero_point_290 = "onnx.DynamicQuantizeLinear"(%1748#0) {onnx_node_name = "/model/layers.24/attn/GroupQueryAttention/output_0_QuantizeLinear"} : (tensor<1x128x2048xf32>) -> (tensor<1x128x2048xui8>, tensor<f32>, tensor<ui8>)
    %1749 = "onnx.MatMulInteger"(%y_288, %641, %y_zero_point_290, %640) {onnx_node_name = "/model/layers.24/attn/o_proj/MatMul_quant"} : (tensor<1x128x2048xui8>, tensor<2048x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1750 = "onnx.Cast"(%1749) {onnx_node_name = "/model/layers.24/attn/o_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1751 = "onnx.Mul"(%y_scale_289, %639) {onnx_node_name = "/model/layers.24/attn/o_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1752 = "onnx.Mul"(%1750, %1751) {onnx_node_name = "/model/layers.24/attn/o_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1753:4 = "onnx.Custom"(%1727#3, %1752, %109) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.24/post_attention_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_291, %y_scale_292, %y_zero_point_293 = "onnx.DynamicQuantizeLinear"(%1753#0) {onnx_node_name = "/model/layers.24/post_attention_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %1754 = "onnx.MatMulInteger"(%y_291, %644, %y_zero_point_293, %643) {onnx_node_name = "/model/layers.24/mlp/gate_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %1755 = "onnx.Cast"(%1754) {onnx_node_name = "/model/layers.24/mlp/gate_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %1756 = "onnx.Mul"(%y_scale_292, %642) {onnx_node_name = "/model/layers.24/mlp/gate_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1757 = "onnx.Mul"(%1755, %1756) {onnx_node_name = "/model/layers.24/mlp/gate_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %1758 = "onnx.MatMulInteger"(%y_291, %647, %y_zero_point_293, %646) {onnx_node_name = "/model/layers.24/mlp/up_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %1759 = "onnx.Cast"(%1758) {onnx_node_name = "/model/layers.24/mlp/up_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %1760 = "onnx.Mul"(%y_scale_292, %645) {onnx_node_name = "/model/layers.24/mlp/up_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1761 = "onnx.Mul"(%1759, %1760) {onnx_node_name = "/model/layers.24/mlp/up_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %1762 = "onnx.Sigmoid"(%1757) {onnx_node_name = "/model/layers.24/mlp/act_fn/Sigmoid"} : (tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %1763 = "onnx.Mul"(%1757, %1762) {onnx_node_name = "/model/layers.24/mlp/act_fn/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %1764 = "onnx.Mul"(%1763, %1761) {onnx_node_name = "/model/layers.24/mlp/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %y_294, %y_scale_295, %y_zero_point_296 = "onnx.DynamicQuantizeLinear"(%1764) {onnx_node_name = "/model/layers.24/mlp/Mul/output_0_QuantizeLinear"} : (tensor<1x128x3072xf32>) -> (tensor<1x128x3072xui8>, tensor<f32>, tensor<ui8>)
    %1765 = "onnx.MatMulInteger"(%y_294, %650, %y_zero_point_296, %649) {onnx_node_name = "/model/layers.24/mlp/down_proj/MatMul_quant"} : (tensor<1x128x3072xui8>, tensor<3072x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1766 = "onnx.Cast"(%1765) {onnx_node_name = "/model/layers.24/mlp/down_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1767 = "onnx.Mul"(%y_scale_295, %648) {onnx_node_name = "/model/layers.24/mlp/down_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1768 = "onnx.Mul"(%1766, %1767) {onnx_node_name = "/model/layers.24/mlp/down_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1769:4 = "onnx.Custom"(%1753#3, %1768, %110) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.25/input_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_297, %y_scale_298, %y_zero_point_299 = "onnx.DynamicQuantizeLinear"(%1769#0) {onnx_node_name = "/model/layers.25/input_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %1770 = "onnx.MatMulInteger"(%y_297, %653, %y_zero_point_299, %652) {onnx_node_name = "/model/layers.25/attn/q_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x2048xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x2048xi32>
    %1771 = "onnx.Cast"(%1770) {onnx_node_name = "/model/layers.25/attn/q_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x2048xi32>) -> tensor<1x128x2048xf32>
    %1772 = "onnx.Mul"(%y_scale_298, %651) {onnx_node_name = "/model/layers.25/attn/q_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1773 = "onnx.Mul"(%1771, %1772) {onnx_node_name = "/model/layers.25/attn/q_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x2048xf32>, tensor<f32>) -> tensor<1x128x2048xf32>
    %1774 = "onnx.MatMulInteger"(%y_297, %656, %y_zero_point_299, %655) {onnx_node_name = "/model/layers.25/attn/k_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1775 = "onnx.Cast"(%1774) {onnx_node_name = "/model/layers.25/attn/k_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1776 = "onnx.Mul"(%y_scale_298, %654) {onnx_node_name = "/model/layers.25/attn/k_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1777 = "onnx.Mul"(%1775, %1776) {onnx_node_name = "/model/layers.25/attn/k_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1778 = "onnx.MatMulInteger"(%y_297, %659, %y_zero_point_299, %658) {onnx_node_name = "/model/layers.25/attn/v_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1779 = "onnx.Cast"(%1778) {onnx_node_name = "/model/layers.25/attn/v_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1780 = "onnx.Mul"(%y_scale_298, %657) {onnx_node_name = "/model/layers.25/attn/v_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1781 = "onnx.Mul"(%1779, %1780) {onnx_node_name = "/model/layers.25/attn/v_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1782 = "onnx.Reshape"(%1773, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.25/attn/q_norm/Reshape_1"} : (tensor<1x128x2048xf32>, tensor<3xi64>) -> tensor<1x2048x128xf32>
    %1783 = "onnx.Reshape"(%1777, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.25/attn/k_norm/Reshape_1"} : (tensor<1x128x1024xf32>, tensor<3xi64>) -> tensor<1x1024x128xf32>
    %1784 = "onnx.Custom"(%1782, %111) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.25/attn/q_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x2048x128xf32>, tensor<128xf32>) -> tensor<1x2048x128xf32>
    %1785 = "onnx.Custom"(%1783, %112) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.25/attn/k_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x1024x128xf32>, tensor<128xf32>) -> tensor<1x1024x128xf32>
    %1786 = "onnx.Reshape"(%1784, %5) {allowzero = 0 : si64, onnx_node_name = "/model/layers.25/attn/q_norm/Reshape_2"} : (tensor<1x2048x128xf32>, tensor<3xi64>) -> tensor<1x128x2048xf32>
    %1787 = "onnx.Reshape"(%1785, %4) {allowzero = 0 : si64, onnx_node_name = "/model/layers.25/attn/k_norm/Reshape_2"} : (tensor<1x1024x128xf32>, tensor<3xi64>) -> tensor<1x128x1024xf32>
    %1788 = "onnx.Custom"(%1786, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.25/attn/q_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x2048xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x2048xf32>
    %1789 = "onnx.Custom"(%1787, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.25/attn/k_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x1024xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x1024xf32>
    %1790:3 = "onnx.Custom"(%1788, %1789, %1781, %arg53, %arg54, %731, %0, %3, %3) {do_rotary = 0 : si64, domain_name = "com.microsoft", function_name = "GroupQueryAttention", kv_num_heads = 8 : si64, num_heads = 16 : si64, onnx_node_name = "/model/layers.25/attn/GroupQueryAttention", rotary_interleaved = 0 : si64, scale = 0.0883883461 : f32, softcap = 0.000000e+00 : f32} : (tensor<1x128x2048xf32>, tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1x8x1x128xf32>, tensor<1x8x1x128xf32>, tensor<1x1xi32>, tensor<i32>, none, none) -> (tensor<1x128x2048xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>)
    %y_300, %y_scale_301, %y_zero_point_302 = "onnx.DynamicQuantizeLinear"(%1790#0) {onnx_node_name = "/model/layers.25/attn/GroupQueryAttention/output_0_QuantizeLinear"} : (tensor<1x128x2048xf32>) -> (tensor<1x128x2048xui8>, tensor<f32>, tensor<ui8>)
    %1791 = "onnx.MatMulInteger"(%y_300, %662, %y_zero_point_302, %661) {onnx_node_name = "/model/layers.25/attn/o_proj/MatMul_quant"} : (tensor<1x128x2048xui8>, tensor<2048x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1792 = "onnx.Cast"(%1791) {onnx_node_name = "/model/layers.25/attn/o_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1793 = "onnx.Mul"(%y_scale_301, %660) {onnx_node_name = "/model/layers.25/attn/o_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1794 = "onnx.Mul"(%1792, %1793) {onnx_node_name = "/model/layers.25/attn/o_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1795:4 = "onnx.Custom"(%1769#3, %1794, %113) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.25/post_attention_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_303, %y_scale_304, %y_zero_point_305 = "onnx.DynamicQuantizeLinear"(%1795#0) {onnx_node_name = "/model/layers.25/post_attention_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %1796 = "onnx.MatMulInteger"(%y_303, %665, %y_zero_point_305, %664) {onnx_node_name = "/model/layers.25/mlp/gate_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %1797 = "onnx.Cast"(%1796) {onnx_node_name = "/model/layers.25/mlp/gate_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %1798 = "onnx.Mul"(%y_scale_304, %663) {onnx_node_name = "/model/layers.25/mlp/gate_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1799 = "onnx.Mul"(%1797, %1798) {onnx_node_name = "/model/layers.25/mlp/gate_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %1800 = "onnx.MatMulInteger"(%y_303, %668, %y_zero_point_305, %667) {onnx_node_name = "/model/layers.25/mlp/up_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %1801 = "onnx.Cast"(%1800) {onnx_node_name = "/model/layers.25/mlp/up_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %1802 = "onnx.Mul"(%y_scale_304, %666) {onnx_node_name = "/model/layers.25/mlp/up_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1803 = "onnx.Mul"(%1801, %1802) {onnx_node_name = "/model/layers.25/mlp/up_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %1804 = "onnx.Sigmoid"(%1799) {onnx_node_name = "/model/layers.25/mlp/act_fn/Sigmoid"} : (tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %1805 = "onnx.Mul"(%1799, %1804) {onnx_node_name = "/model/layers.25/mlp/act_fn/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %1806 = "onnx.Mul"(%1805, %1803) {onnx_node_name = "/model/layers.25/mlp/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %y_306, %y_scale_307, %y_zero_point_308 = "onnx.DynamicQuantizeLinear"(%1806) {onnx_node_name = "/model/layers.25/mlp/Mul/output_0_QuantizeLinear"} : (tensor<1x128x3072xf32>) -> (tensor<1x128x3072xui8>, tensor<f32>, tensor<ui8>)
    %1807 = "onnx.MatMulInteger"(%y_306, %671, %y_zero_point_308, %670) {onnx_node_name = "/model/layers.25/mlp/down_proj/MatMul_quant"} : (tensor<1x128x3072xui8>, tensor<3072x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1808 = "onnx.Cast"(%1807) {onnx_node_name = "/model/layers.25/mlp/down_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1809 = "onnx.Mul"(%y_scale_307, %669) {onnx_node_name = "/model/layers.25/mlp/down_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1810 = "onnx.Mul"(%1808, %1809) {onnx_node_name = "/model/layers.25/mlp/down_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1811:4 = "onnx.Custom"(%1795#3, %1810, %114) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.26/input_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_309, %y_scale_310, %y_zero_point_311 = "onnx.DynamicQuantizeLinear"(%1811#0) {onnx_node_name = "/model/layers.26/input_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %1812 = "onnx.MatMulInteger"(%y_309, %674, %y_zero_point_311, %673) {onnx_node_name = "/model/layers.26/attn/q_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x2048xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x2048xi32>
    %1813 = "onnx.Cast"(%1812) {onnx_node_name = "/model/layers.26/attn/q_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x2048xi32>) -> tensor<1x128x2048xf32>
    %1814 = "onnx.Mul"(%y_scale_310, %672) {onnx_node_name = "/model/layers.26/attn/q_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1815 = "onnx.Mul"(%1813, %1814) {onnx_node_name = "/model/layers.26/attn/q_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x2048xf32>, tensor<f32>) -> tensor<1x128x2048xf32>
    %1816 = "onnx.MatMulInteger"(%y_309, %677, %y_zero_point_311, %676) {onnx_node_name = "/model/layers.26/attn/k_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1817 = "onnx.Cast"(%1816) {onnx_node_name = "/model/layers.26/attn/k_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1818 = "onnx.Mul"(%y_scale_310, %675) {onnx_node_name = "/model/layers.26/attn/k_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1819 = "onnx.Mul"(%1817, %1818) {onnx_node_name = "/model/layers.26/attn/k_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1820 = "onnx.MatMulInteger"(%y_309, %680, %y_zero_point_311, %679) {onnx_node_name = "/model/layers.26/attn/v_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1821 = "onnx.Cast"(%1820) {onnx_node_name = "/model/layers.26/attn/v_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1822 = "onnx.Mul"(%y_scale_310, %678) {onnx_node_name = "/model/layers.26/attn/v_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1823 = "onnx.Mul"(%1821, %1822) {onnx_node_name = "/model/layers.26/attn/v_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1824 = "onnx.Reshape"(%1815, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.26/attn/q_norm/Reshape_1"} : (tensor<1x128x2048xf32>, tensor<3xi64>) -> tensor<1x2048x128xf32>
    %1825 = "onnx.Reshape"(%1819, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.26/attn/k_norm/Reshape_1"} : (tensor<1x128x1024xf32>, tensor<3xi64>) -> tensor<1x1024x128xf32>
    %1826 = "onnx.Custom"(%1824, %115) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.26/attn/q_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x2048x128xf32>, tensor<128xf32>) -> tensor<1x2048x128xf32>
    %1827 = "onnx.Custom"(%1825, %116) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.26/attn/k_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x1024x128xf32>, tensor<128xf32>) -> tensor<1x1024x128xf32>
    %1828 = "onnx.Reshape"(%1826, %5) {allowzero = 0 : si64, onnx_node_name = "/model/layers.26/attn/q_norm/Reshape_2"} : (tensor<1x2048x128xf32>, tensor<3xi64>) -> tensor<1x128x2048xf32>
    %1829 = "onnx.Reshape"(%1827, %4) {allowzero = 0 : si64, onnx_node_name = "/model/layers.26/attn/k_norm/Reshape_2"} : (tensor<1x1024x128xf32>, tensor<3xi64>) -> tensor<1x128x1024xf32>
    %1830 = "onnx.Custom"(%1828, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.26/attn/q_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x2048xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x2048xf32>
    %1831 = "onnx.Custom"(%1829, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.26/attn/k_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x1024xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x1024xf32>
    %1832:3 = "onnx.Custom"(%1830, %1831, %1823, %arg55, %arg56, %731, %0, %3, %3) {do_rotary = 0 : si64, domain_name = "com.microsoft", function_name = "GroupQueryAttention", kv_num_heads = 8 : si64, num_heads = 16 : si64, onnx_node_name = "/model/layers.26/attn/GroupQueryAttention", rotary_interleaved = 0 : si64, scale = 0.0883883461 : f32, softcap = 0.000000e+00 : f32} : (tensor<1x128x2048xf32>, tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1x8x1x128xf32>, tensor<1x8x1x128xf32>, tensor<1x1xi32>, tensor<i32>, none, none) -> (tensor<1x128x2048xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>)
    %y_312, %y_scale_313, %y_zero_point_314 = "onnx.DynamicQuantizeLinear"(%1832#0) {onnx_node_name = "/model/layers.26/attn/GroupQueryAttention/output_0_QuantizeLinear"} : (tensor<1x128x2048xf32>) -> (tensor<1x128x2048xui8>, tensor<f32>, tensor<ui8>)
    %1833 = "onnx.MatMulInteger"(%y_312, %683, %y_zero_point_314, %682) {onnx_node_name = "/model/layers.26/attn/o_proj/MatMul_quant"} : (tensor<1x128x2048xui8>, tensor<2048x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1834 = "onnx.Cast"(%1833) {onnx_node_name = "/model/layers.26/attn/o_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1835 = "onnx.Mul"(%y_scale_313, %681) {onnx_node_name = "/model/layers.26/attn/o_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1836 = "onnx.Mul"(%1834, %1835) {onnx_node_name = "/model/layers.26/attn/o_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1837:4 = "onnx.Custom"(%1811#3, %1836, %117) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.26/post_attention_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_315, %y_scale_316, %y_zero_point_317 = "onnx.DynamicQuantizeLinear"(%1837#0) {onnx_node_name = "/model/layers.26/post_attention_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %1838 = "onnx.MatMulInteger"(%y_315, %686, %y_zero_point_317, %685) {onnx_node_name = "/model/layers.26/mlp/gate_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %1839 = "onnx.Cast"(%1838) {onnx_node_name = "/model/layers.26/mlp/gate_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %1840 = "onnx.Mul"(%y_scale_316, %684) {onnx_node_name = "/model/layers.26/mlp/gate_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1841 = "onnx.Mul"(%1839, %1840) {onnx_node_name = "/model/layers.26/mlp/gate_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %1842 = "onnx.MatMulInteger"(%y_315, %689, %y_zero_point_317, %688) {onnx_node_name = "/model/layers.26/mlp/up_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %1843 = "onnx.Cast"(%1842) {onnx_node_name = "/model/layers.26/mlp/up_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %1844 = "onnx.Mul"(%y_scale_316, %687) {onnx_node_name = "/model/layers.26/mlp/up_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1845 = "onnx.Mul"(%1843, %1844) {onnx_node_name = "/model/layers.26/mlp/up_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %1846 = "onnx.Sigmoid"(%1841) {onnx_node_name = "/model/layers.26/mlp/act_fn/Sigmoid"} : (tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %1847 = "onnx.Mul"(%1841, %1846) {onnx_node_name = "/model/layers.26/mlp/act_fn/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %1848 = "onnx.Mul"(%1847, %1845) {onnx_node_name = "/model/layers.26/mlp/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %y_318, %y_scale_319, %y_zero_point_320 = "onnx.DynamicQuantizeLinear"(%1848) {onnx_node_name = "/model/layers.26/mlp/Mul/output_0_QuantizeLinear"} : (tensor<1x128x3072xf32>) -> (tensor<1x128x3072xui8>, tensor<f32>, tensor<ui8>)
    %1849 = "onnx.MatMulInteger"(%y_318, %692, %y_zero_point_320, %691) {onnx_node_name = "/model/layers.26/mlp/down_proj/MatMul_quant"} : (tensor<1x128x3072xui8>, tensor<3072x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1850 = "onnx.Cast"(%1849) {onnx_node_name = "/model/layers.26/mlp/down_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1851 = "onnx.Mul"(%y_scale_319, %690) {onnx_node_name = "/model/layers.26/mlp/down_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1852 = "onnx.Mul"(%1850, %1851) {onnx_node_name = "/model/layers.26/mlp/down_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1853:4 = "onnx.Custom"(%1837#3, %1852, %118) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.27/input_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_321, %y_scale_322, %y_zero_point_323 = "onnx.DynamicQuantizeLinear"(%1853#0) {onnx_node_name = "/model/layers.27/input_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %1854 = "onnx.MatMulInteger"(%y_321, %695, %y_zero_point_323, %694) {onnx_node_name = "/model/layers.27/attn/q_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x2048xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x2048xi32>
    %1855 = "onnx.Cast"(%1854) {onnx_node_name = "/model/layers.27/attn/q_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x2048xi32>) -> tensor<1x128x2048xf32>
    %1856 = "onnx.Mul"(%y_scale_322, %693) {onnx_node_name = "/model/layers.27/attn/q_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1857 = "onnx.Mul"(%1855, %1856) {onnx_node_name = "/model/layers.27/attn/q_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x2048xf32>, tensor<f32>) -> tensor<1x128x2048xf32>
    %1858 = "onnx.MatMulInteger"(%y_321, %698, %y_zero_point_323, %697) {onnx_node_name = "/model/layers.27/attn/k_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1859 = "onnx.Cast"(%1858) {onnx_node_name = "/model/layers.27/attn/k_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1860 = "onnx.Mul"(%y_scale_322, %696) {onnx_node_name = "/model/layers.27/attn/k_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1861 = "onnx.Mul"(%1859, %1860) {onnx_node_name = "/model/layers.27/attn/k_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1862 = "onnx.MatMulInteger"(%y_321, %701, %y_zero_point_323, %700) {onnx_node_name = "/model/layers.27/attn/v_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1863 = "onnx.Cast"(%1862) {onnx_node_name = "/model/layers.27/attn/v_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1864 = "onnx.Mul"(%y_scale_322, %699) {onnx_node_name = "/model/layers.27/attn/v_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1865 = "onnx.Mul"(%1863, %1864) {onnx_node_name = "/model/layers.27/attn/v_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1866 = "onnx.Reshape"(%1857, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.27/attn/q_norm/Reshape_1"} : (tensor<1x128x2048xf32>, tensor<3xi64>) -> tensor<1x2048x128xf32>
    %1867 = "onnx.Reshape"(%1861, %6) {allowzero = 0 : si64, onnx_node_name = "/model/layers.27/attn/k_norm/Reshape_1"} : (tensor<1x128x1024xf32>, tensor<3xi64>) -> tensor<1x1024x128xf32>
    %1868 = "onnx.Custom"(%1866, %119) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.27/attn/q_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x2048x128xf32>, tensor<128xf32>) -> tensor<1x2048x128xf32>
    %1869 = "onnx.Custom"(%1867, %120) {axis = -1 : si64, domain_name = "", epsilon = 9.99999997E-7 : f32, function_name = "SimplifiedLayerNormalization", onnx_node_name = "/model/layers.27/attn/k_norm/SimplifiedLayerNormalization", stash_type = 1 : si64} : (tensor<1x1024x128xf32>, tensor<128xf32>) -> tensor<1x1024x128xf32>
    %1870 = "onnx.Reshape"(%1868, %5) {allowzero = 0 : si64, onnx_node_name = "/model/layers.27/attn/q_norm/Reshape_2"} : (tensor<1x2048x128xf32>, tensor<3xi64>) -> tensor<1x128x2048xf32>
    %1871 = "onnx.Reshape"(%1869, %4) {allowzero = 0 : si64, onnx_node_name = "/model/layers.27/attn/k_norm/Reshape_2"} : (tensor<1x1024x128xf32>, tensor<3xi64>) -> tensor<1x128x1024xf32>
    %1872 = "onnx.Custom"(%1870, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.27/attn/q_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x2048xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x2048xf32>
    %1873 = "onnx.Custom"(%1871, %arg2, %11, %12) {domain_name = "com.microsoft", function_name = "RotaryEmbedding", interleaved = 0 : si64, num_heads = 0 : si64, onnx_node_name = "/model/layers.27/attn/k_rotary/RotaryEmbedding", rotary_embedding_dim = 0 : si64} : (tensor<1x128x1024xf32>, tensor<1x128xi64>, tensor<40960x64xf32>, tensor<40960x64xf32>) -> tensor<1x128x1024xf32>
    %1874:3 = "onnx.Custom"(%1872, %1873, %1865, %arg57, %arg58, %731, %0, %3, %3) {do_rotary = 0 : si64, domain_name = "com.microsoft", function_name = "GroupQueryAttention", kv_num_heads = 8 : si64, num_heads = 16 : si64, onnx_node_name = "/model/layers.27/attn/GroupQueryAttention", rotary_interleaved = 0 : si64, scale = 0.0883883461 : f32, softcap = 0.000000e+00 : f32} : (tensor<1x128x2048xf32>, tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1x8x1x128xf32>, tensor<1x8x1x128xf32>, tensor<1x1xi32>, tensor<i32>, none, none) -> (tensor<1x128x2048xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>)
    %y_324, %y_scale_325, %y_zero_point_326 = "onnx.DynamicQuantizeLinear"(%1874#0) {onnx_node_name = "/model/layers.27/attn/GroupQueryAttention/output_0_QuantizeLinear"} : (tensor<1x128x2048xf32>) -> (tensor<1x128x2048xui8>, tensor<f32>, tensor<ui8>)
    %1875 = "onnx.MatMulInteger"(%y_324, %704, %y_zero_point_326, %703) {onnx_node_name = "/model/layers.27/attn/o_proj/MatMul_quant"} : (tensor<1x128x2048xui8>, tensor<2048x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1876 = "onnx.Cast"(%1875) {onnx_node_name = "/model/layers.27/attn/o_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1877 = "onnx.Mul"(%y_scale_325, %702) {onnx_node_name = "/model/layers.27/attn/o_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1878 = "onnx.Mul"(%1876, %1877) {onnx_node_name = "/model/layers.27/attn/o_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1879:4 = "onnx.Custom"(%1853#3, %1878, %121) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.27/post_attention_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> (tensor<1x128x1024xf32>, none, none, tensor<1x128x1024xf32>)
    %y_327, %y_scale_328, %y_zero_point_329 = "onnx.DynamicQuantizeLinear"(%1879#0) {onnx_node_name = "/model/layers.27/post_attention_layernorm/output_0_QuantizeLinear"} : (tensor<1x128x1024xf32>) -> (tensor<1x128x1024xui8>, tensor<f32>, tensor<ui8>)
    %1880 = "onnx.MatMulInteger"(%y_327, %707, %y_zero_point_329, %706) {onnx_node_name = "/model/layers.27/mlp/gate_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %1881 = "onnx.Cast"(%1880) {onnx_node_name = "/model/layers.27/mlp/gate_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %1882 = "onnx.Mul"(%y_scale_328, %705) {onnx_node_name = "/model/layers.27/mlp/gate_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1883 = "onnx.Mul"(%1881, %1882) {onnx_node_name = "/model/layers.27/mlp/gate_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %1884 = "onnx.MatMulInteger"(%y_327, %710, %y_zero_point_329, %709) {onnx_node_name = "/model/layers.27/mlp/up_proj/MatMul_quant"} : (tensor<1x128x1024xui8>, tensor<1024x3072xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x3072xi32>
    %1885 = "onnx.Cast"(%1884) {onnx_node_name = "/model/layers.27/mlp/up_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x3072xi32>) -> tensor<1x128x3072xf32>
    %1886 = "onnx.Mul"(%y_scale_328, %708) {onnx_node_name = "/model/layers.27/mlp/up_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1887 = "onnx.Mul"(%1885, %1886) {onnx_node_name = "/model/layers.27/mlp/up_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x3072xf32>, tensor<f32>) -> tensor<1x128x3072xf32>
    %1888 = "onnx.Sigmoid"(%1883) {onnx_node_name = "/model/layers.27/mlp/act_fn/Sigmoid"} : (tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %1889 = "onnx.Mul"(%1883, %1888) {onnx_node_name = "/model/layers.27/mlp/act_fn/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %1890 = "onnx.Mul"(%1889, %1887) {onnx_node_name = "/model/layers.27/mlp/Mul"} : (tensor<1x128x3072xf32>, tensor<1x128x3072xf32>) -> tensor<1x128x3072xf32>
    %y_330, %y_scale_331, %y_zero_point_332 = "onnx.DynamicQuantizeLinear"(%1890) {onnx_node_name = "/model/layers.27/mlp/Mul/output_0_QuantizeLinear"} : (tensor<1x128x3072xf32>) -> (tensor<1x128x3072xui8>, tensor<f32>, tensor<ui8>)
    %1891 = "onnx.MatMulInteger"(%y_330, %713, %y_zero_point_332, %712) {onnx_node_name = "/model/layers.27/mlp/down_proj/MatMul_quant"} : (tensor<1x128x3072xui8>, tensor<3072x1024xi8>, tensor<ui8>, tensor<i8>) -> tensor<1x128x1024xi32>
    %1892 = "onnx.Cast"(%1891) {onnx_node_name = "/model/layers.27/mlp/down_proj/MatMul/output_0_output_quantized_cast", saturate = 1 : si64, to = f32} : (tensor<1x128x1024xi32>) -> tensor<1x128x1024xf32>
    %1893 = "onnx.Mul"(%y_scale_331, %711) {onnx_node_name = "/model/layers.27/mlp/down_proj/MatMul_quant_scales_mul"} : (tensor<f32>, tensor<f32>) -> tensor<f32>
    %1894 = "onnx.Mul"(%1892, %1893) {onnx_node_name = "/model/layers.27/mlp/down_proj/MatMul_quant_output_scale_mul"} : (tensor<1x128x1024xf32>, tensor<f32>) -> tensor<1x128x1024xf32>
    %1895 = "onnx.Custom"(%1879#3, %1894, %122) {domain_name = "com.microsoft", epsilon = 9.99999997E-7 : f32, function_name = "SkipSimplifiedLayerNormalization", onnx_node_name = "/model/layers.28/final_norm_layernorm/SkipLayerNorm"} : (tensor<1x128x1024xf32>, tensor<1x128x1024xf32>, tensor<1024xf32>) -> tensor<1x128x1024xf32>
    %1896 = "onnx.DequantizeLinear"(%2, %123, %124) {axis = 1 : si64, onnx_node_name = "model.embed_tokens.weight_transposed_DequantizeLinear"} : (tensor<1024x151936xui8>, tensor<f32>, tensor<ui8>) -> tensor<1024x151936xf32>
    %1897 = "onnx.MatMul"(%1895, %1896) {onnx_node_name = "/lm_head/MatMul"} : (tensor<1x128x1024xf32>, tensor<1024x151936xf32>) -> tensor<1x128x151936xf32>
    return %1897, %740#1, %740#2, %782#1, %782#2, %824#1, %824#2, %866#1, %866#2, %908#1, %908#2, %950#1, %950#2, %992#1, %992#2, %1034#1, %1034#2, %1076#1, %1076#2, %1118#1, %1118#2, %1160#1, %1160#2, %1202#1, %1202#2, %1244#1, %1244#2, %1286#1, %1286#2, %1328#1, %1328#2, %1370#1, %1370#2, %1412#1, %1412#2, %1454#1, %1454#2, %1496#1, %1496#2, %1538#1, %1538#2, %1580#1, %1580#2, %1622#1, %1622#2, %1664#1, %1664#2, %1706#1, %1706#2, %1748#1, %1748#2, %1790#1, %1790#2, %1832#1, %1832#2, %1874#1, %1874#2 : tensor<1x128x151936xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>, tensor<1x8x128x128xf32>
  }
  "onnx.EntryPoint"() {func = @main_graph} : () -> ()
}
